diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 843edfd000be..d927f59c9182 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -542,6 +542,29 @@ config ARCH_PXA
 	help
 	  Support for Intel/Marvell's PXA2xx/PXA3xx processor line.
 
+config ARCH_MSM
+	bool "Qualcomm MSM (non-multiplatform)"
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select GENERIC_CLOCKEVENTS
+	select GENERIC_ALLOCATOR
+	select HAVE_CLK
+	select HAVE_CLK_PREPARE
+	select ARCH_HAS_OPP
+	select SOC_BUS
+	select MULTI_IRQ_HANDLER
+	select PM_OPP
+	select SPARSE_IRQ
+	select USE_OF
+	select THERMAL_WRITABLE_TRIPS
+
+	help
+	  Support for Qualcomm MSM/QSD based systems.  This runs on the
+	  apps processor of the MSM/QSD and depends on a shared memory
+	  interface to the modem processor which runs the baseband
+	  stack and controls some vital subsystems
+	  (clock and power control, etc).
+
 config ARCH_RPC
 	bool "RiscPC"
 	depends on MMU
@@ -768,6 +791,8 @@ source "arch/arm/mach-mediatek/Kconfig"
 
 source "arch/arm/mach-meson/Kconfig"
 
+source "arch/arm/mach-msm/Kconfig"
+
 source "arch/arm/mach-mmp/Kconfig"
 
 source "arch/arm/mach-moxart/Kconfig"
@@ -1470,7 +1495,7 @@ config ARM_PSCI
 config ARCH_NR_GPIO
 	int
 	default 2048 if ARCH_SOCFPGA
-	default 1024 if ARCH_BRCMSTB || ARCH_RENESAS || ARCH_TEGRA || \
+	default 1024 if ARCH_BRCMSTB || ARCH_RENESAS || ARCH_TEGRA || ARCH_MSM || \
 		ARCH_ZYNQ
 	default 512 if ARCH_EXYNOS || ARCH_KEYSTONE || SOC_OMAP5 || \
 		SOC_DRA7XX || ARCH_S3C24XX || ARCH_S3C64XX || ARCH_S5PV210
@@ -1848,6 +1873,21 @@ config DEPRECATED_PARAM_STRUCT
 	  This was deprecated in 2001 and announced to live on for 5 years.
 	  Some old boot loaders still use this way.
 
+config BUILD_ARM_APPENDED_DTB_IMAGE
+	bool "Build a concatenated zImage/dtb by default"
+	depends on OF
+	help
+	  Enabling this option will cause a concatenated zImage and DTB to
+	  be built by default (instead of a standalone zImage.)  The image
+	  will built in arch/arm/boot/zImage-dtb.<dtb name>
+
+config BUILD_ARM_APPENDED_DTB_IMAGE_NAME
+	string "Default dtb name"
+	depends on BUILD_ARM_APPENDED_DTB_IMAGE
+	help
+	  name of the dtb to append when building a concatenated
+	  zImage/dtb.
+
 # Compressed boot loader in ROM.  Yes, we really want to ask about
 # TEXT and BSS so we preserve their values in the config files.
 config ZBOOT_ROM_TEXT
diff --git a/arch/arm/Makefile b/arch/arm/Makefile
index fc26c3d7b9b6..77875726f002 100644
--- a/arch/arm/Makefile
+++ b/arch/arm/Makefile
@@ -189,6 +189,7 @@ machine-$(CONFIG_ARCH_MESON)		+= meson
 machine-$(CONFIG_ARCH_MMP)		+= mmp
 machine-$(CONFIG_ARCH_MPS2)		+= vexpress
 machine-$(CONFIG_ARCH_MOXART)		+= moxart
+machine-$(CONFIG_ARCH_MSM)		+= msm
 machine-$(CONFIG_ARCH_MV78XX0)		+= mv78xx0
 machine-$(CONFIG_ARCH_MVEBU)		+= mvebu
 machine-$(CONFIG_ARCH_MXC)		+= imx
@@ -307,6 +308,8 @@ libs-y				:= arch/arm/lib/ $(libs-y)
 boot := arch/arm/boot
 ifeq ($(CONFIG_XIP_KERNEL),y)
 KBUILD_IMAGE := $(boot)/xipImage
+else ifeq ($(CONFIG_BUILD_ARM_APPENDED_DTB_IMAGE),y)
+KBUILD_IMAGE := zImage-dtb.$(CONFIG_BUILD_ARM_APPENDED_DTB_IMAGE_NAME)
 else
 KBUILD_IMAGE := $(boot)/zImage
 endif
diff --git a/arch/arm/boot/dts/Makefile b/arch/arm/boot/dts/Makefile
index 37a3de760d40..77a633615ebe 100644
--- a/arch/arm/boot/dts/Makefile
+++ b/arch/arm/boot/dts/Makefile
@@ -1184,3 +1184,5 @@ dtb-$(CONFIG_ARCH_ASPEED) += \
 	aspeed-bmc-opp-zaius.dtb \
 	aspeed-bmc-portwell-neptune.dtb \
 	aspeed-bmc-quanta-q71l.dtb
+
+subdir- := qcom
diff --git a/arch/arm/include/asm/fixmap.h b/arch/arm/include/asm/fixmap.h
index 472c93db5dac..6801c9c94a17 100644
--- a/arch/arm/include/asm/fixmap.h
+++ b/arch/arm/include/asm/fixmap.h
@@ -11,6 +11,7 @@
 
 enum fixed_addresses {
 	FIX_EARLYCON_MEM_BASE,
+	FIX_SMP_MEM_BASE,
 	__end_of_permanent_fixed_addresses,
 
 	FIX_KMAP_BEGIN = __end_of_permanent_fixed_addresses,
diff --git a/arch/arm/include/asm/system_misc.h b/arch/arm/include/asm/system_misc.h
index 8e76db83c498..60ee40fd3881 100644
--- a/arch/arm/include/asm/system_misc.h
+++ b/arch/arm/include/asm/system_misc.h
@@ -43,6 +43,8 @@ static inline int handle_guest_sea(phys_addr_t addr, unsigned int esr)
 	return -1;
 }
 
+extern char* (*arch_read_hardware_id)(void);
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_ARM_SYSTEM_MISC_H */
diff --git a/arch/arm/include/asm/vfp.h b/arch/arm/include/asm/vfp.h
index 7157d2a30a49..6f960bcd0e0e 100644
--- a/arch/arm/include/asm/vfp.h
+++ b/arch/arm/include/asm/vfp.h
@@ -94,6 +94,8 @@
 
 #ifndef __ASSEMBLY__
 void vfp_disable(void);
+int vfp_pm_suspend(void);
+void vfp_pm_resume(void);
 #endif
 
 #endif /* __ASM_VFP_H */
diff --git a/arch/arm/tools/mach-types b/arch/arm/tools/mach-types
index 4eac94c1eb6f..69aee5e0cf59 100644
--- a/arch/arm/tools/mach-types
+++ b/arch/arm/tools/mach-types
@@ -548,8 +548,16 @@ smdk4412		MACH_SMDK4412		SMDK4412		3765
 marzen			MACH_MARZEN		MARZEN			3790
 empc_a500		MACH_EMPC_A500		EMPC_A500		3848
 u8520			MACH_U8520		U8520			3990
+msm8625_ffa		MACH_MSM8625_FFA	MSM8625_FFA		4166
+msm8625_evt		MACH_MSM8625_EVT	MSM8625_EVT		4193
 urt			MACH_URT		URT			4347
 keystone		MACH_KEYSTONE		KEYSTONE		4390
+msm8930dt		MACH_MSM8930DT		MSM8930DT		4425
+msm8625q_evbd		MACH_MSM8625Q_EVBD	MSM8625Q_EVBD		4447
+msm8625q_skud		MACH_MSM8625Q_SKUD	MSM8625Q_SKUD		4456
+msm8625_fih_sae		MACH_MSM8625_FIH_SAE	MSM8625_FIH_SAE		4462
+msm8930_qrd8930		MACH_MSM8930_QRD8930	MSM8930_QRD8930		4558
+msm8909		ARCH_MSM8909	MSM8909		4559
 ckb_rza1h		MACH_CKB_RZA1H		CKB_RZA1H		4780
 bcm2835			MACH_BCM2835		BCM2835			4828
 cm_3g			MACH_CM_3G		CM_3G			4943
diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 35d0f823e823..4c0e677ce3b7 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -454,7 +454,7 @@ void vfp_disable(void)
 }
 
 #ifdef CONFIG_CPU_PM
-static int vfp_pm_suspend(void)
+int vfp_pm_suspend(void)
 {
 	struct thread_info *ti = current_thread_info();
 	u32 fpexc = fmrx(FPEXC);
@@ -480,7 +480,7 @@ static int vfp_pm_suspend(void)
 	return 0;
 }
 
-static void vfp_pm_resume(void)
+void vfp_pm_resume(void)
 {
 	/* ensure we have access to the vfp */
 	vfp_enable(NULL);
diff --git a/drivers/Kconfig b/drivers/Kconfig
index 95b9ccc08165..992f02d75836 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -179,6 +179,8 @@ source "drivers/reset/Kconfig"
 
 source "drivers/fmc/Kconfig"
 
+source "drivers/coresight/Kconfig"
+
 source "drivers/phy/Kconfig"
 
 source "drivers/powercap/Kconfig"
diff --git a/drivers/clk/Kconfig b/drivers/clk/Kconfig
index 721572a8c429..5f0f4f2faaf3 100644
--- a/drivers/clk/Kconfig
+++ b/drivers/clk/Kconfig
@@ -293,5 +293,6 @@ source "drivers/clk/sunxi-ng/Kconfig"
 source "drivers/clk/tegra/Kconfig"
 source "drivers/clk/ti/Kconfig"
 source "drivers/clk/uniphier/Kconfig"
+source "drivers/clk/msm/Kconfig"
 
 endmenu
diff --git a/drivers/clk/Makefile b/drivers/clk/Makefile
index 0bb25dd009d1..eccaac27f0cc 100644
--- a/drivers/clk/Makefile
+++ b/drivers/clk/Makefile
@@ -107,3 +107,4 @@ obj-$(CONFIG_X86)			+= x86/
 endif
 obj-$(CONFIG_ARCH_ZX)			+= zte/
 obj-$(CONFIG_ARCH_ZYNQ)			+= zynq/
+obj-$(CONFIG_ARCH_MSM)          += msm/
diff --git a/drivers/cpuidle/Makefile b/drivers/cpuidle/Makefile
index 9d7176cee3d3..d11f00d5100c 100644
--- a/drivers/cpuidle/Makefile
+++ b/drivers/cpuidle/Makefile
@@ -29,3 +29,5 @@ obj-$(CONFIG_MIPS_CPS_CPUIDLE)		+= cpuidle-cps.o
 # POWERPC drivers
 obj-$(CONFIG_PSERIES_CPUIDLE)		+= cpuidle-pseries.o
 obj-$(CONFIG_POWERNV_CPUIDLE)		+= cpuidle-powernv.o
+
+obj-$(CONFIG_MSM_PM) += lpm-levels.o  lpm-levels-of.o lpm-workarounds.o
diff --git a/drivers/power/Kconfig b/drivers/power/Kconfig
index 63454b5cac27..47c9169f38ac 100644
--- a/drivers/power/Kconfig
+++ b/drivers/power/Kconfig
@@ -1,3 +1,4 @@
 source "drivers/power/avs/Kconfig"
+source "drivers/power/qcom/Kconfig"
 source "drivers/power/reset/Kconfig"
 source "drivers/power/supply/Kconfig"
diff --git a/drivers/power/Makefile b/drivers/power/Makefile
index ff35c712d824..8322bdb69740 100644
--- a/drivers/power/Makefile
+++ b/drivers/power/Makefile
@@ -1,3 +1,4 @@
 obj-$(CONFIG_POWER_AVS)		+= avs/
+obj-y						+= qcom/
 obj-$(CONFIG_POWER_RESET)	+= reset/
 obj-$(CONFIG_POWER_SUPPLY)	+= supply/
diff --git a/drivers/regulator/Kconfig b/drivers/regulator/Kconfig
index 5dbccf5f3037..8aac9ab3952e 100644
--- a/drivers/regulator/Kconfig
+++ b/drivers/regulator/Kconfig
@@ -992,5 +992,85 @@ config REGULATOR_WM8994
 	  This driver provides support for the voltage regulators on the
 	  WM8994 CODEC.
 
+config REGULATOR_CPR
+	bool "RBCPR regulator driver for APC"
+	depends on OF
+	help
+	  Compile in RBCPR (RapidBridge Core Power Reduction) driver to support
+	  corner vote for APC power rail. The driver takes PTE process voltage
+	  suggestions in efuse as initial settings. It converts corner vote
+	  to voltage value before writing to a voltage regulator API, such as
+	  that provided by spm-regulator driver.
+
+config REGULATOR_CPR2_GFX
+	bool "RBCPR regulator driver for GFX"
+	depends on OF
+	help
+	  This driver supports the CPR (core power reduction) controller for the
+	  graphics (GFX) rail. The GFX CPR2 controller monitors the graphics voltage
+	  requirements. This driver reads initial voltage values out of hardware
+	  fuses and CPR target quotient values out of device tree.
+
+config REGULATOR_CPR3
+	bool "CPR3 regulator core support"
+	help
+	  This driver supports Core Power Reduction (CPR) version 3 controllers
+	  which are used by some Qualcomm Technologies, Inc. (QTI) SoCs to
+	  manage important voltage regulators.  CPR3 controllers are capable of
+	  monitoring several ring oscillator sensing loops simultaneously.  The
+	  CPR3 controller informs software when the silicon conditions require
+	  the supply voltage to be increased or decreased.  On certain supply
+	  rails, the CPR3 controller is able to propagate the voltage increase
+	  or decrease requests all the way to the PMIC without software
+	  involvement.
+
+config REGULATOR_CPR3_HMSS
+	bool "CPR3 regulator for HMSS"
+	depends on OF
+	select REGULATOR_CPR3
+	help
+	  This driver supports Qualcomm Technologies, Inc. HMSS application
+	  processor specific features including memory array power mux (APM)
+	  switching, two CPR3 threads which monitor the two HMSS clusters that
+	  are both powered by a shared supply, and hardware closed-loop auto
+	  voltage stepping.  This driver reads both initial voltage and CPR
+	  target quotient values out of hardware fuses.
+
+config REGULATOR_CPR3_MMSS
+	bool "RBCPR3 regulator for MMSS"
+	depends on OF
+	select REGULATOR_CPR3
+	help
+	  This driver supports Qualcomm Technologies, Inc. MMSS graphics
+	  processor specific features.  The MMSS CPR3 controller only uses one
+	  thread to monitor the MMSS voltage requirements.  This driver reads
+	  initial voltage values out of hardware fuses and CPR target quotient
+	  values out of device tree.
+
+config REGULATOR_CPR4_APSS
+	bool "CPR4 regulator for APSS"
+	depends on OF
+	select REGULATOR_CPR3
+	help
+	  This driver supports Qualcomm Technologies, Inc. APSS application
+	  processor specific features including memory array power mux (APM)
+	  switching, one CPR4 thread which monitor the two APSS clusters that
+	  are both powered by a shared supply, hardware closed-loop auto
+	  voltage stepping, voltage adjustments based on online core count,
+	  voltage adjustments based on temperature readings, and voltage
+	  adjustments for performance boost mode. This driver reads both initial
+	  voltage and CPR target quotient values out of hardware fuses.
+
+config REGULATOR_CPRH_KBSS
+	bool "CPRH regulator for KBSS"
+	depends on OF
+	select REGULATOR_CPR3
+	help
+	  This driver supports Qualcomm Technologies, Inc. KBSS application
+	  processor specific features including CPR hardening (CPRh) and two
+	  CPRh controllers which monitor the two KBSS clusters each powered by
+	  independent voltage supplies. This driver reads both initial voltage
+	  and CPR target quotient values out of hardware fuses.
+
 endif
 
diff --git a/drivers/regulator/Makefile b/drivers/regulator/Makefile
index bd818ceb7c72..0a8ec5fd636f 100644
--- a/drivers/regulator/Makefile
+++ b/drivers/regulator/Makefile
@@ -126,5 +126,12 @@ obj-$(CONFIG_REGULATOR_WM831X) += wm831x-ldo.o
 obj-$(CONFIG_REGULATOR_WM8350) += wm8350-regulator.o
 obj-$(CONFIG_REGULATOR_WM8400) += wm8400-regulator.o
 obj-$(CONFIG_REGULATOR_WM8994) += wm8994-regulator.o
+obj-$(CONFIG_REGULATOR_CPR) += cpr-regulator.o
+obj-$(CONFIG_REGULATOR_CPR3) += cpr3-regulator.o cpr3-util.o
+obj-$(CONFIG_REGULATOR_CPR3_HMSS) += cpr3-hmss-regulator.o
+obj-$(CONFIG_REGULATOR_CPR3_MMSS) += cpr3-mmss-regulator.o
+obj-$(CONFIG_REGULATOR_CPR4_APSS) += cpr4-apss-regulator.o
+obj-$(CONFIG_REGULATOR_CPRH_KBSS) += cprh-kbss-regulator.o
+obj-$(CONFIG_REGULATOR_CPR2_GFX) += cpr2-gfx-regulator.o
 
 ccflags-$(CONFIG_REGULATOR_DEBUG) += -DDEBUG
diff --git a/drivers/soc/qcom/Kconfig b/drivers/soc/qcom/Kconfig
index 5856e792d09c..81cf78866232 100644
--- a/drivers/soc/qcom/Kconfig
+++ b/drivers/soc/qcom/Kconfig
@@ -34,7 +34,6 @@ config QCOM_GLINK_SSR
 config QCOM_GSBI
         tristate "QCOM General Serial Bus Interface"
         depends on ARCH_QCOM
-        select MFD_SYSCON
         help
           Say y here to enable GSBI support.  The GSBI provides control
           functions for connecting the underlying serial UART, SPI, and I2C
@@ -136,4 +135,767 @@ config QCOM_APR
           application processor and QDSP6. APR is
           used by audio driver to configure QDSP6
           ASM, ADM and AFE modules.
+
+if ARCH_MSM
+
+config CP_ACCESS64
+	depends on ARM64
+        tristate "CP 64-bit register access tool"
+        help
+          Provide support for AARCH64 CP register access using /sys
+          interface. Read and write to CP registers from userspace
+          through sysfs interface. A sys file (cp_rw) will be created under
+          /sys/devices/cpaccess/cpaccess0.
+
+          If unsure, say N.
+
+config MSM_INRUSH_CURRENT_MITIGATION
+        bool "Inrush-current mitigation Driver"
+        help
+         This driver helps in mitigating in-rush current on MSM
+         chipsets which has voltage droop issues due to sudden
+         huge load on a rail. This driver introduces an intermediate
+         load to mitigate the in-rush current.
+
+config MSM_QDSP6_APRV2
+        bool "Audio QDSP6 APRv2 support"
+        depends on MSM_SMD
+        help
+          Enable APRv2 IPC protocol support between
+          application processor and QDSP6. APR is
+          used by audio driver to configure QDSP6's
+          ASM, ADM and AFE.
+
+config MSM_GLADIATOR_ERP
+	tristate "GLADIATOR coherency interconnect error reporting driver"
+	help
+	  Support dumping debug information for the GLADIATOR
+	  cache interconnect in the error interrupt handler.
+	  Meant to be used for debug scenarios only.
+
+	  If unsure, say N.
+
+config PANIC_ON_GLADIATOR_ERROR
+	depends on MSM_GLADIATOR_ERP
+	bool "Panic on GLADIATOR error report"
+	help
+	  Panic upon detection of an Gladiator coherency interconnect error
+	  in order to support dumping debug information.
+	  Meant to be used for debug scenarios only.
+
+	  If unsure, say N.
+
+config MSM_QDSP6_APRV3
+	bool "Audio QDSP6 APRv3 support"
+	depends on MSM_SMD
+	help
+	  Enable APRv3 IPC protocol support between
+	  application processor and QDSP6. APR is
+	  used by audio driver to configure QDSP6v2's
+	  ASM, ADM and AFE.
+
+config MSM_QDSP6_APRV2_GLINK
+	bool "Audio QDSP6 APRv2 over Glink support"
+	depends on MSM_GLINK
+	help
+	  Enable APRv2 IPC protocol support over
+	  Glink between application processor and
+	  QDSP6. APR is used by audio driver to
+	  configure QDSP6's ASM, ADM and AFE.
+
+config MSM_QDSP6_APRV3_GLINK
+	bool "Audio QDSP6 APRv3 over Glink support"
+	depends on MSM_GLINK
+	help
+	  Enable APRv3 IPC protocol support over
+	  Glink between application processor and
+	  QDSP6. APR is used by audio driver to
+	  configure QDSP6v2's ASM, ADM and AFE.
+
+config MSM_ADSP_LOADER
+	tristate "ADSP loader support"
+	select SND_SOC_MSM_APRV2_INTF
+	depends on MSM_QDSP6_APRV2 || MSM_QDSP6_APRV3 || \
+		MSM_QDSP6_APRV2_GLINK || MSM_QDSP6_APRV3_GLINK
+	help
+	  Enable ADSP image loader.
+	  The ADSP loader brings ADSP out of reset
+	  for the platforms that use APRv2.
+	  Say M if you want to enable this module.
+
+config MSM_MEMORY_DUMP
+	bool "MSM Memory Dump Support"
+	help
+	  This enables memory dump feature. It allows various client
+	  subsystems to register respective dump regions. At the time
+	  of deadlocks or cpu hangs these dump regions are captured to
+	  give a snapshot of the system at the time of the crash.
+
+config MSM_MEMORY_DUMP_V2
+	bool "MSM Memory Dump V2 Support"
+	help
+	  This enables memory dump feature. It allows various client
+	  subsystems to register respective dump regions. At the time
+	  of deadlocks or cpu hangs these dump regions are captured to
+	  give a snapshot of the system at the time of the crash.
+
+config MSM_DEBUG_LAR_UNLOCK
+	bool "MSM Debug LAR Unlock Support"
+	depends on MSM_MEMORY_DUMP_V2
+	help
+	 This allows unlocking Core Debug lock to allow capture
+	 of upper 32 bits of program counter at the time of
+	 system crash. This is useful in getting correct crash
+	 location.
+
+config MSM_JTAG
+	bool "Debug and ETM trace support across power collapse"
+	help
+	  Enables support for debugging (specifically breakpoints) and ETM
+	  processor tracing across power collapse both for JTag and OS hosted
+	  software running on the target. Enabling this will ensure debug
+	  and ETM registers are saved and restored across power collapse.
+
+	  If unsure, say 'N' here to avoid potential power, performance and
+	  memory penalty.
+
+config MSM_JTAG_MM
+	bool "Debug and ETM trace support across power collapse using memory mapped access"
+	help
+	  Enables support for debugging (specifically breakpoints) and ETM
+	  processor tracing across power collapse both for JTag and OS hosted
+	  software running on the target. Enabling this will ensure debug
+	  and ETM registers are saved and restored across power collapse.
+
+	  Required on targets on which cp14 access to debug and ETM registers is
+	  not permitted and so memory mapped access is necessary.
+
+	  If unsure, say 'N' here to avoid potential power, performance and
+	  memory penalty.
+
+config MSM_JTAGV8
+	bool "Debug and ETM trace support across power collapse for ARMv8"
+	help
+	  Enables support for debugging (specifically breakpoints) and ETM
+	  processor tracing across power collapse both for JTag and OS hosted
+	  software running on ARMv8 target. Enabling this will ensure debug
+	  and ETM registers are saved and restored across power collapse.
+
+	  If unsure, say 'N' here to avoid potential power, performance and
+	  memory penalty.
+
+config MSM_BOOT_STATS
+	bool "Use MSM boot stats reporting"
+	help
+	 Use this to report msm boot stats such as bootloader throughput,
+	 display init, total boot time.
+	 This figures are reported in mpm sleep clock cycles and have a
+	 resolution of 31 bits as 1 bit is used as an overflow check.
+
+config MSM_CPUSS_DUMP
+	bool "CPU Subsystem Dumping support"
+	help
+	 Add support to dump various hardware entities such as the instruction
+	 and data tlb's as well as the unified tlb, which are a part of the
+	 cpu subsystem to an allocated buffer. This allows for analysis of the
+	 the entities if corruption is suspected.
+	 If unsure, say N
+
+config MSM_COMMON_LOG
+	bool "MSM Common Log Support"
+	help
+	 Use this to export symbols of some log address and variables
+	 that need to parse crash dump files to a memory dump table. This
+	 table can be used by post analysis tools to extract information
+	 from memory when device crashes.
+
+config MSM_DDR_HEALTH
+	bool "MSM DDR Health Driver"
+	default n
+	help
+	  This option enables a driver which allocates DDR buffer of requested
+	  size and sends it's locatoin to RPM sub system. RPM subsystem can
+	  make use of this buffer to monitor DDR health.
+
+config MSM_HYP_DEBUG
+	bool "MSM Hypervisor Debug Driver"
+	help
+	  This enables the Hypervisor Debug driver. It allows the mapping and
+	  and unmapping of user defined memory range from stage 2. It also
+	  supports mapping and unmapping PIL image load memory range from
+	  stage 2 depending upon the PIL image state.
+
+config MSM_WATCHDOG_V2
+	bool "MSM Watchdog Support"
+	help
+	  This enables the watchdog module. It causes kernel panic if the
+	  watchdog times out. It allows for detection of cpu hangs and
+	  deadlocks. It does not run during the bootup process, so it will
+	  not catch any early lockups.
+
+config MSM_FORCE_WDOG_BITE_ON_PANIC
+	bool "MSM force watchdog bite"
+	depends on MSM_WATCHDOG_V2
+	help
+	  This forces a watchdog bite when the device restarts due to a
+	  kernel panic. On certain MSM SoCs, this provides us
+	  additional debugging information.
+
+config MSM_CORE_HANG_DETECT
+	tristate "MSM Core Hang Detection Support"
+	help
+	  This enables the core hang detection module. It causes SoC
+	  reset on core hang detection and collects the core context
+	  for hang.
+
+config MSM_GLADIATOR_HANG_DETECT
+	tristate "MSM Gladiator Hang Detection Support"
+	help
+	  This enables the gladiator hang detection module.
+	  If the configured threshold is reached, it causes SoC reset on
+	  gladiator hang detection and collects the context for the
+	  gladiator hang.
+
+config MSM_CPU_PWR_CTL
+	bool "Cpu subsystem power control"
+	depends on SMP && (ARM || ARM64)
+	default y
+	help
+	Provides routines to power on cpu rails and l2 cache
+	controller during coldboot.
+
+config MSM_CACHE_M4M_ERP64
+       bool "Cache and M4M error report"
+       depends on ARCH_MSM8996
+       help
+         Say 'Y' here to enable reporting of cache and M4M errors to the kernel
+         log. The kernel log contains collected error syndrome and address
+         registers. These register dumps can be used as useful information
+         to find out possible hardware problems.
+
+config MSM_CACHE_M4M_ERP64_PANIC_ON_CE
+       bool "Panic on correctable cache/M4M errors"
+       depends on MSM_CACHE_M4M_ERP64
+       help
+         Say 'Y' here to cause kernel panic when correctable cache/M4M errors
+         are detected.  Enabling this is useful when you want to dump memory
+         and system state close to the time when the error occured.
+
+          If unsure, say N.
+
+config MSM_CACHE_M4M_ERP64_PANIC_ON_UE
+       bool "Panic on uncorrectable cache/M4M errors"
+       depends on MSM_CACHE_M4M_ERP64
+       help
+         Say 'Y' here to cause kernel panic when uncorrectable cache/M4M errors
+         are detected.
+
+config MSM_RPM_SMD
+	bool "RPM driver using SMD protocol"
+	help
+	  RPM is the dedicated hardware engine for managing shared SoC
+	  resources. This config adds driver support for using SMD as a
+	  transport layer communication with RPM hardware. It also selects
+	  the MSM_MPM config that programs the MPM module to monitor interrupts
+	  during sleep modes.
+
+config MSM_RPM_RBCPR_STATS_V2_LOG
+	tristate "MSM Resource Power Manager RPBCPR Stat Driver"
+	depends on DEBUG_FS
+	help
+	  This option enables v2 of the rpmrbcpr_stats driver which reads RPM
+	  memory for statistics pertaining to RPM's RBCPR(Rapid Bridge Core
+	  Power Reduction) driver. The drivers outputs the message via a
+	  debugfs node.
+
+config MSM_RPM_LOG
+	tristate "MSM Resource Power Manager Log Driver"
+	depends on DEBUG_FS
+	depends on MSM_RPM_SMD
+	default n
+	help
+          This option enables a driver which can read from a circular buffer
+          of messages produced by the RPM. These messages provide diagnostic
+          information about RPM operation. The driver outputs the messages
+          via a debugfs node.
+
+config MSM_RPM_STATS_LOG
+        tristate "MSM Resource Power Manager Stat Driver"
+        depends on DEBUG_FS
+        depends on MSM_RPM_SMD
+        default n
+          help
+          This option enables a driver which reads RPM messages from a shared
+          memory location. These messages provide statistical information about
+          the low power modes that RPM enters. The drivers outputs the message
+          via a debugfs node.
+
+config MSM_RUN_QUEUE_STATS
+	bool "Enable collection and exporting of MSM Run Queue stats to userspace"
+	help
+	 This option enables the driver to periodically collecting the statistics
+	 of kernel run queue information and calculate the load of the system.
+	 This information is exported to usespace via sysfs entries and userspace
+	 algorithms uses info and decide when to turn on/off the cpu cores.
+
+config MSM_SCM
+       bool "Secure Channel Manager (SCM) support"
+       default n
+
+menuconfig MSM_SCM_XPU
+	bool "MSM XPU configuration driver"
+	depends on MSM_SCM
+
+if MSM_SCM_XPU
+
+choice
+	prompt "XPU Violation Behavior"
+	default MSM_XPU_ERR_FATAL
+
+config MSM_XPU_ERR_FATAL
+	bool "Configure XPU violations as fatal errors"
+	help
+	 Select if XPU violations have to be configured as fatal errors.
+
+config MSM_XPU_ERR_NONFATAL
+	bool "Configure XPU violations as non-fatal errors"
+	help
+	 Select if XPU violations have to be configured as non-fatal errors.
+
+endchoice
+
+endif
+
+config MSM_SCM_ERRATA
+	depends on DEBUG_FS
+	depends on MSM_SCM
+	bool "Support for enabling/disabling errata workarounds via debugfs"
+	help
+	  Exposes a debugfs interface intended for advanced system debugging
+	  where it may be desirable to enable or disable certain hardware
+	  errata workarounds at runtime.
+
+	  If unsure, say N.
+
+config MSM_PFE_WA
+	depends on HW_PERF_EVENTS
+	bool "Enable a H/W PFE WA"
+	help
+	  Sometimes the PFTLB entries get stuck in the invalid state and new
+	  prefetches get dropped. For a workaround, count L1 prefeches dropped
+	  due to PFTLB miss and reset H/W PFE when a overflow happens.
+
+	  If unsure, say N.
+
+config MSM_MPM_OF
+       bool "Modem Power Manager"
+       depends on OF
+       help
+        MPM is a dedicated hardware resource responsible for entering and
+        waking up from a system wide low power mode. The MPM driver tracks
+        the wakeup interrupts and configures the MPM to monitor the wakeup
+        interrupts when going to a system wide sleep mode. This config option
+        enables the MPM driver that supports initialization from a device
+        tree
+
+config MSM_SMEM
+	depends on REMOTE_SPINLOCK_MSM
+	bool "MSM Shared Memory (SMEM)"
+	help
+	  Support for the shared memory interface between the various
+	  processors in the System on a Chip (SoC) which allows basic
+	  inter-processor communication.
+
+config MSM_SMD
+	depends on MSM_SMEM
+	bool "MSM Shared Memory Driver (SMD)"
+	help
+	  Support for the shared memory interprocessor communication protocol
+	  which provides virual point to point serial channels between processes
+	  on the apps processor and processes on other processors in the SoC.
+	  Also includes support for the Shared Memory State Machine (SMSM)
+	  protocol which provides a mechanism to publish single bit state
+	  information to one or more processors in the SoC.
+
+config MSM_SMD_DEBUG
+	depends on MSM_SMD
+	bool "MSM SMD debug support"
+	help
+	  Support for debugging SMD and SMSM communication between apps and
+	  other processors in the SoC. Debug support primarily consists of
+	  logs consisting of information such as what interrupts were processed,
+	  what channels caused interrupt activity, and when internal state
+	  change events occur.
+
+config MSM_GLINK
+	bool "Generic Link (G-Link)"
+	help
+	  G-Link is a generic link transport that replaces SMD.  It is used
+	  within a System-on-Chip (SoC) for communication between both internal
+	  processors and external peripherals.  The actual physical transport
+	  is handled by transport plug-ins that can be individually enabled and
+	  configured separately.
+
+config MSM_GLINK_LOOPBACK_SERVER
+	bool "Generic Link (G-Link) Loopback Server"
+	help
+	  G-Link Loopback Server that enable loopback test framework to test
+	  and validate the G-Link protocol stack. It support both local and
+	  remote clients to configure the loopback server and echo back the
+	  data received from the clients.
+
+config MSM_GLINK_SMD_XPRT
+	depends on MSM_SMD
+	depends on MSM_GLINK
+	bool "Generic Link (G-Link) SMD Transport"
+	help
+	  G-Link SMD Transport is a G-Link Transport plug-in.  It allows G-Link
+	  communication to remote entities through a SMD physical transport
+	  channel.  The remote side is assumed to be pure SMD.  The nature of
+	  SMD limits this G-Link transport to only connecting with entities
+	  internal to the System-on-Chip.
+
+config MSM_GLINK_SMEM_NATIVE_XPRT
+	depends on MSM_SMEM
+	depends on MSM_GLINK
+	bool "Generic Link (G-Link) SMEM Native Transport"
+	help
+	  G-Link SMEM Native Transport is a G-Link Transport plug-in.  It allows
+	  G-Link communication to remote entities through a shared memory
+	  physical transport.  The nature of shared memory limits this G-Link
+	  transport to only connecting with entities internal to the
+	  System-on-Chip.
+
+config MSM_SPCOM
+	depends on MSM_GLINK
+	bool "Secure Processor Communication over GLINK"
+	help
+	  spcom driver allows loading Secure Processor Applications and
+	  sending messages to Secure Processor Applications.
+	  spcom provides interface to both user space app and kernel driver.
+	  It is using glink as the transport layer, which provides multiple
+	  logical channels over signle physical channel.
+	  The physical layer is based on shared memory and interrupts.
+	  spcom provides clients/server API, although currently only one client
+	  or server is allowed per logical channel.
+
+config MSM_SMEM_LOGGING
+	depends on MSM_SMEM
+	bool "MSM Shared Memory Logger"
+	help
+	  Enable the shared memory logging to log the events between
+	  the various processors in the system. This option exposes
+	  the shared memory logger at /dev/smem_log and a debugfs node
+	  named smem_log.
+
+config MSM_SMP2P
+	bool "SMSM Point-to-Point (SMP2P)"
+	depends on MSM_SMEM
+	help
+	  Provide point-to-point remote signaling support.
+	  SMP2P enables transferring 32-bit values between
+	  the local and a remote system using shared
+	  memory and interrupts. A client can open multiple
+	  32-bit values by specifying a unique string and
+	  remote processor ID.
+
+config MSM_SMP2P_TEST
+	bool "SMSM Point-to-Point Test"
+	depends on MSM_SMP2P
+	help
+	  Enables loopback and unit testing support for
+	  SMP2P. Loopback support is used by other
+	  processors to do unit testing. Unit tests
+	  are used to verify the local and remote
+	  implementations.
+
+config MSM_SPM
+	bool "Driver support for SPM Version 2"
+	help
+	  Enables the support for Version 2 of the SPM driver. SPM hardware is
+	  used to manage the processor power during sleep. The driver allows
+	  configuring SPM to allow different low power modes for both core and
+	  L2.
+
+config MSM_L2_SPM
+	bool "SPM support for L2 cache"
+	help
+	  Enable SPM driver support for L2 cache. Some MSM chipsets allow
+	  control of L2 cache low power mode with a Subsystem Power manager.
+	  Enabling this driver allows configuring L2 SPM for low power modes
+	  on supported chipsets
+
+config MSM_QMI_INTERFACE
+	depends on IPC_ROUTER
+	depends on QMI_ENCDEC
+	bool "MSM QMI Interface Library"
+	help
+	  Library to send and receive QMI messages over IPC Router.
+	  This library provides interface functions to the kernel drivers
+	  to perform QMI message marshaling and transport them over IPC
+	  Router.
+
+config MSM_DCC
+	bool "MSM Data Capture and Compare enigne support"
+	help
+	  This option enables driver for Data Capture and Compare engine. DCC
+	  driver provides interface to configure DCC block and read back
+	  captured data from DCC's internal SRAM.
+
+config MSM_HVC
+	bool "MSM Hypervisor Call Support"
+	help
+	  This enables the Hypervisor Call module. It provides apis to call
+	  into the hypervisor thereby allowing access to services exposed by
+	  the hypervisor. It is primarily intended to be used for Silicon
+	  Partner/Manufacturer function identifier subrange but supports other
+	  service call subranges as well.
+
+config MSM_IPC_ROUTER_SMD_XPRT
+	depends on MSM_SMD
+	depends on IPC_ROUTER
+	bool "MSM SMD XPRT Layer"
+	help
+	  SMD Transport Layer that enables IPC Router communication within
+	  a System-on-Chip(SoC). When the SMD channels become available,
+	  this layer registers a transport with IPC Router and enable
+	  message exchange.
+
+config MSM_EVENT_TIMER
+	bool "Event timer"
+	help
+	  This option enables a modules that manages a list of event timers that
+	  need to be monitored by the PM. The enables the PM code to monitor
+	  events that require the core to be awake and ready to handle the
+	  event.
+
+config MSM_IPC_ROUTER_HSIC_XPRT
+	depends on USB_QCOM_IPC_BRIDGE
+	depends on IPC_ROUTER
+	bool "MSM HSIC XPRT Layer"
+	help
+	  HSIC Transport Layer that enables off-chip communication of
+	  IPC Router. When the HSIC endpoint becomes available, this layer
+	  registers the transport with IPC Router and enable message
+	  exchange.
+
+config MSM_SYSMON_GLINK_COMM
+	bool "MSM System Monitor communication support using GLINK transport"
+	depends on MSM_GLINK && MSM_SUBSYSTEM_RESTART
+	help
+	  This option adds support for MSM System Monitor APIs using the GLINK
+	  transport layer. The APIs provided may be used for notifying
+	  subsystems within the SoC about other subsystems' power-up/down
+	  state-changes.
+
+config MSM_IPC_ROUTER_GLINK_XPRT
+	depends on MSM_GLINK
+	depends on IPC_ROUTER
+	bool "MSM GLINK XPRT Layer"
+	help
+	  GLINK Transport Layer that enables IPC Router communication within
+	  a System-on-Chip(SoC). When the GLINK channels become available,
+	  this layer registers a transport with IPC Router and enable
+	  message exchange.
+
+config MSM_SYSTEM_HEALTH_MONITOR
+	bool "System Health Monitor"
+	depends on MSM_QMI_INTERFACE && MSM_SUBSYSTEM_RESTART
+	help
+	  System Health Monitor (SHM) passively monitors the health of the
+	  peripherals connected to the application processor. Software
+	  components in the application processor that experience
+	  communication failure can request the SHM to perform a system-wide
+	  health check. If any failures are detected during the health-check,
+	  then a subsystem restart will be triggered for the failed subsystem.
+
+config MSM_GLINK_PKT
+	bool "Enable device interface for GLINK packet channels"
+	depends on MSM_GLINK
+	help
+	  G-link packet driver provides the interface for the userspace
+	  clients to communicate over G-Link via deivce nodes.
+	  This enable the usersapce clients to read and write to
+	  some glink packets channel.
+
+config MSM_TZ_SMMU
+	bool "Helper functions for SMMU configuration through TZ"
+	depends on ARCH_MSMTHULIUM || ARCH_MSMTITANIUM
+	help
+	  Say 'Y' here for targets that need to call into TZ to configure
+	  SMMUs for any reason (for example, for errata workarounds or
+	  configuration of SMMU virtualization).
+
+	  If unsure, say N.
+
+config MSM_SUBSYSTEM_RESTART
+	bool "MSM Subsystem Restart"
+	help
+	  This option enables the MSM subsystem restart framework.
+
+	  The MSM subsystem restart framework provides support to boot,
+	  shutdown, and restart subsystems with a reference counted API.
+	  It also notifies userspace of transitions between these states via
+	  sysfs.
+
+config MSM_SYSMON_COMM
+	bool "MSM System Monitor communication support"
+	depends on MSM_SMD && MSM_SUBSYSTEM_RESTART
+	help
+	  This option adds support for MSM System Monitor library, which
+	  provides an API that may be used for notifying subsystems within
+	  the SoC about other subsystems' power-up/down state-changes.
+
+config MSM_PIL
+	bool "Peripheral image loading"
+	select FW_LOADER
+	default n
+	help
+	  Some peripherals need to be loaded into memory before they can be
+	  brought out of reset.
+
+	  Say yes to support these devices.
+
+config MSM_PIL_SSR_GENERIC
+	tristate "MSM Subsystem Boot Support"
+	depends on MSM_PIL && MSM_SUBSYSTEM_RESTART
+	help
+	  Support for booting and shutting down MSM Subsystem processors.
+	  This driver also monitors the SMSM status bits and the watchdog
+	  interrupt for the subsystem and restarts it on a watchdog bite
+	  or a fatal error. Subsystems include LPASS, Venus, VPU, WCNSS and
+	  BCSS.
+
+config MSM_PIL_MSS_QDSP6V5
+	tristate "MSS QDSP6v5 (Hexagon) Boot Support"
+	depends on MSM_PIL && MSM_SUBSYSTEM_RESTART
+	help
+	  Support for booting and shutting down QDSP6v5 (Hexagon) processors
+	  in modem subsystems. If you would like to make or receive phone
+	  calls then say Y here.
+
+	  If unsure, say N.
+
+config MSM_SHARED_HEAP_ACCESS
+	bool "Shared Heap access"
+	help
+	 Enable support to provide access to clients to certain HLOS regions
+	 which are protected by the secure environment.
+
+config TRACER_PKT
+	bool "Tracer Packet"
+	help
+	  Tracer Packet helps in profiling the performance of inter-
+	  processor communication protocols. The profiling information
+	  can be logged into the tracer packet itself.
+
+config MSM_SECURE_BUFFER
+	bool "Helper functions for securing buffers through TZ"
+	help
+	 Say 'Y' here for targets that need to call into TZ to secure
+	 memory buffers. This ensures that only the correct clients can
+	 use this memory and no unauthorized access is made to the
+	 buffer
+
+config ICNSS
+	tristate "Platform driver for Q6 integrated connectivity"
+	---help---
+	  This module adds support for Q6 integrated WLAN connectivity
+	  subsystem. This module is responsible for communicating WLAN on/off
+	  control messages to FW over QMI channel. It is also responsible for
+	  handling WLAN PD restart notifications.
+
+config MSM_BAM_DMUX
+	bool "BAM Data Mux Driver"
+	depends on SPS
+	help
+	  Support Muxed Data Channels over BAM interface.
+	  BAM has a limited number of pipes.  This driver
+	  provides a means to support more logical channels
+	  via muxing than BAM could without muxing.
+
+config MSM_PERFORMANCE
+	tristate "Core control driver to support userspace hotplug requests"
+	help
+	  This driver is used to provide CPU hotplug support to userspace.
+	  It ensures that no more than a user specified number of CPUs stay
+	  online at any given point in time.
+
+config MSM_PERFORMANCE_HOTPLUG_ON
+	bool "Hotplug functionality through msm_performance turned on"
+	depends on MSM_PERFORMANCE
+	default y
+	help
+	  If some other core-control driver is present turn off the core-control
+	  capability of msm_performance driver. Setting this flag to false will
+	  compile out the nodes needed for core-control functionality through
+	  msm_performance.
+
+config MSM_SERVICE_LOCATOR
+	bool "Service Locator"
+	depends on MSM_QMI_INTERFACE
+	help
+	  The Service Locator provides a library to retrieve location
+	  information given a service identifier. Location here translates
+	  to what process domain exports the service, and which subsystem
+	  that process domain will execute in.
+
+config MSM_SERVICE_NOTIFIER
+	bool "Service Notifier"
+	depends on MSM_SERVICE_LOCATOR && MSM_SUBSYSTEM_RESTART
+	help
+	  The Service Notifier provides a library for a kernel client to
+	  register for state change notifications regarding a remote service.
+	  A remote service here refers to a process providing certain services
+	  like audio, the identifier for which is provided by the service
+	  locator.
+
+config MSM_QBT1000
+	bool "QBT1000 Ultrasonic Fingerprint Sensor"
+	help
+	  This driver is used to enable clocks and marshal buffers for trusted
+	  applications running in the trusted execution environment.
+
+config MSM_PACMAN
+	tristate "Enable the Peripheral Access Control Manager (PACMan)"
+	help
+	  Add support for the Peripheral Access Control Manager (PACMan). This
+	  driver allows reconfiguration of the Bus Access Manager Low Speed
+	  peripheral (BLSP) ownership.
+
+config MSM_KERNEL_PROTECT
+	bool "Protect kernel text by removing write permissions in stage-2"
+        depends on !FUNCTION_TRACER
+        help
+          On hypervisor-enabled targets, this option will make a call into
+          the hypervisor to request that the kernel text be remapped
+          without write permissions.  This protects against malicious
+          devices rewriting kernel code.
+
+          Note that this will BREAK any runtime patching of the kernel text
+          (i.e. anything that uses apply_alternatives,
+          aarch64_insn_patch_text_nosync, etc. including the various CPU
+          errata workarounds in arch/arm64/kernel/cpu_errata.c).
+
+config MSM_KERNEL_PROTECT_TEST
+	bool "Bootup test of kernel protection (INTENTIONAL CRASH)"
+        depends on MSM_KERNEL_PROTECT
+        help
+          Attempts to write to the kernel text after making the kernel text
+          read-only.  This test is FATAL whether it passes or fails!
+          Success is signaled by a stage-2 fault.
+
+config MSM_REMOTEQDSS
+	bool "Allow debug tools to enable events on other processors"
+	depends on MSM_SCM && DEBUG_FS
+	help
+	  Other onchip processors/execution environments may support debug
+	  events. Provide a sysfs interface for debug tools to dynamically
+	  enable/disable these events. Interface located in
+	  /sys/class/remoteqdss.
+
+source "drivers/soc/qcom/memshare/Kconfig"
+
+endif # ARCH_MSM
+
 endmenu
diff --git a/drivers/soc/qcom/Makefile b/drivers/soc/qcom/Makefile
index 19dcf957cb3a..55aa6b3d5448 100644
--- a/drivers/soc/qcom/Makefile
+++ b/drivers/soc/qcom/Makefile
@@ -1,17 +1,116 @@
 # SPDX-License-Identifier: GPL-2.0
-obj-$(CONFIG_QCOM_GENI_SE) +=	qcom-geni-se.o
+obj-$(CONFIG_QCOM_GENI_SE) +=  qcom-geni-se.o
 obj-$(CONFIG_QCOM_COMMAND_DB) += cmd-db.o
-obj-$(CONFIG_QCOM_GLINK_SSR) +=	glink_ssr.o
+obj-$(CONFIG_QCOM_GLINK_SSR) +=        glink_ssr.o
+
+KASAN_SANITIZE_scm.o := n
+
+CFLAGS_scm.o :=$(call as-instr,.arch_extension sec,-DREQUIRES_SEC=1)
+
+ccflags-$(CONFIG_MSM_QBT1000) += -Idrivers/misc/
+
+obj-$(CONFIG_MSM_INRUSH_CURRENT_MITIGATION) += inrush-current-mitigation.o
+obj-$(CONFIG_MSM_GLADIATOR_ERP) += gladiator_erp.o
+obj-$(CONFIG_ARM64) += idle-v8.o cpu_ops.o
+obj-$(CONFIG_MSM_BOOT_STATS) += boot_stats.o
+obj-$(CONFIG_MSM_RUN_QUEUE_STATS) += msm_rq_stats.o
+obj-$(CONFIG_CPU_V7) += idle-v7.o
 obj-$(CONFIG_QCOM_GSBI)	+=	qcom_gsbi.o
-obj-$(CONFIG_QCOM_MDT_LOADER)	+= mdt_loader.o
-obj-$(CONFIG_QCOM_PM)	+=	spm.o
-obj-$(CONFIG_QCOM_QMI_HELPERS)	+= qmi_helpers.o
-qmi_helpers-y	+= qmi_encdec.o qmi_interface.o
-obj-$(CONFIG_QCOM_RMTFS_MEM)	+= rmtfs_mem.o
-obj-$(CONFIG_QCOM_SMD_RPM)	+= smd-rpm.o
-obj-$(CONFIG_QCOM_SMEM) +=	smem.o
+obj-$(CONFIG_QCOM_MDT_LOADER)  += mdt_loader.o
+obj-$(CONFIG_QCOM_PM)  +=      spm.o
+obj-$(CONFIG_QCOM_QMI_HELPERS) += qmi_helpers.o
+qmi_helpers-y  += qmi_encdec.o qmi_interface.o
+obj-$(CONFIG_QCOM_RMTFS_MEM)   += rmtfs_mem.o
+obj-$(CONFIG_QCOM_SMD_RPM)     += smd-rpm.o
+obj-$(CONFIG_QCOM_SMEM) +=     smem.o
 obj-$(CONFIG_QCOM_SMEM_STATE) += smem_state.o
-obj-$(CONFIG_QCOM_SMP2P)	+= smp2p.o
-obj-$(CONFIG_QCOM_SMSM)	+= smsm.o
+obj-$(CONFIG_QCOM_SMP2P)       += smp2p.o
+obj-$(CONFIG_QCOM_SMSM)        += smsm.o
 obj-$(CONFIG_QCOM_WCNSS_CTRL) += wcnss_ctrl.o
 obj-$(CONFIG_QCOM_APR) += apr.o
+obj-$(CONFIG_MSM_CPUSS_DUMP) += cpuss_dump.o
+obj-$(CONFIG_MSM_MEMORY_DUMP) += memory_dump.o
+obj-$(CONFIG_MSM_MEMORY_DUMP_V2) += memory_dump_v2.o
+obj-$(CONFIG_MSM_DDR_HEALTH) += ddr-health.o
+obj-$(CONFIG_MSM_DCC) += dcc.o
+obj-$(CONFIG_MSM_WATCHDOG_V2) += watchdog_v2.o
+obj-$(CONFIG_MSM_CORE_HANG_DETECT) += core_hang_detect.o
+obj-$(CONFIG_MSM_GLADIATOR_HANG_DETECT) += gladiator_hang_detect.o
+obj-$(CONFIG_MSM_COMMON_LOG) += common_log.o
+obj-$(CONFIG_MSM_HVC) += hvc.o
+obj-$(CONFIG_MSM_CPU_PWR_CTL) += cpu_pwr_ctl.o
+obj-$(CONFIG_MSM_CACHE_M4M_ERP64) += cache_m4m_erp64.o
+obj-$(CONFIG_SOC_BUS)  +=      socinfo.o
+obj-$(CONFIG_MSM_HYP_DEBUG) += hyp-debug.o
+obj-$(CONFIG_ARCH_MSM8996) += kryo-l2-accessors.o
+obj-$(CONFIG_MSM_RPM_SMD) +=	rpm-smd.o
+obj-$(CONFIG_MSM_EVENT_TIMER) += event_timer.o
+ifdef CONFIG_DEBUG_FS
+obj-$(CONFIG_MSM_RPM_SMD) +=	rpm-smd-debug.o
+endif
+obj-$(CONFIG_MSM_SPM) += spm.o spm_devices.o
+obj-$(CONFIG_SOC_BUS)	+=	socinfo.o
+obj-$(CONFIG_MSM_SCM)  +=      scm.o scm-boot.o
+obj-$(CONFIG_MSM_MPM_OF) += mpm-of.o
+obj-$(CONFIG_MSM_SMEM)  +=smem.o smem_debug.o
+obj-$(CONFIG_MSM_SMD)   += smd.o smd_debug.o smd_private.o smd_init_dt.o smsm_debug.o
+obj-$(CONFIG_MSM_GLINK) += glink.o glink_debugfs.o glink_ssr.o
+obj-$(CONFIG_MSM_GLINK_LOOPBACK_SERVER) += glink_loopback_server.o
+obj-$(CONFIG_MSM_GLINK_SMD_XPRT) += glink_smd_xprt.o
+obj-$(CONFIG_MSM_GLINK_SMEM_NATIVE_XPRT) += glink_smem_native_xprt.o
+obj-$(CONFIG_MSM_SPCOM) += spcom.o
+obj-$(CONFIG_MSM_SMEM_LOGGING) += smem_log.o
+obj-$(CONFIG_MSM_SMP2P) += smp2p.o smp2p_debug.o smp2p_sleepstate.o
+obj-$(CONFIG_MSM_SMP2P_TEST) += smp2p_loopback.o smp2p_test.o smp2p_spinlock_test.o
+obj-$(CONFIG_MSM_QMI_INTERFACE) += qmi_interface.o
+obj-$(CONFIG_MSM_IPC_ROUTER_SMD_XPRT) += ipc_router_smd_xprt.o
+obj-$(CONFIG_MSM_IPC_ROUTER_HSIC_XPRT) += ipc_router_hsic_xprt.o
+obj-$(CONFIG_MSM_IPC_ROUTER_MHI_XPRT) += ipc_router_mhi_xprt.o
+obj-$(CONFIG_MSM_IPC_ROUTER_GLINK_XPRT) += ipc_router_glink_xprt.o
+obj-$(CONFIG_MSM_SYSMON_GLINK_COMM) += sysmon-glink.o sysmon-qmi.o
+obj-$(CONFIG_MSM_SHARED_HEAP_ACCESS) += shared_memory.o
+obj-y			+=	qdsp6v2/
+obj-$(CONFIG_MSM_RPM_RBCPR_STATS_V2_LOG) += rpm_rbcpr_stats_v2.o
+obj-$(CONFIG_MEM_SHARE_QMI_SERVICE)		+= memshare/
+obj-$(CONFIG_CP_ACCESS64) += cpaccess64.o
+obj-$(CONFIG_MSM_RPM_STATS_LOG) += rpm_stats.o rpm_master_stat.o rpm_rail_stats.o
+ifdef CONFIG_ARCH_MSM8996
+obj-$(CONFIG_HW_PERF_EVENTS) += perf_event_kryo.o perf_event_l2.o
+else
+obj-$(CONFIG_HW_PERF_EVENTS) += perf_event_l2.o
+endif
+obj-$(CONFIG_MSM_RPM_LOG) += rpm_log.o
+obj-$(CONFIG_MSM_JTAG) += jtag-fuse.o jtag.o
+obj-$(CONFIG_MSM_JTAG_MM) +=  jtag-fuse.o jtag-mm.o
+obj-$(CONFIG_MSM_JTAGV8) += jtag-fuse.o jtagv8.o jtagv8-etm.o
+obj-$(CONFIG_MSM_SYSTEM_HEALTH_MONITOR) += system_health_monitor_v01.o
+obj-$(CONFIG_MSM_SYSTEM_HEALTH_MONITOR) += system_health_monitor.o
+obj-$(CONFIG_MSM_GLINK_PKT) += msm_glink_pkt.o
+obj-$(CONFIG_MSM_TZ_SMMU) += msm_tz_smmu.o
+obj-$(CONFIG_MSM_PIL)   +=      peripheral-loader.o
+obj-$(CONFIG_MSM_PIL_SSR_GENERIC) += subsys-pil-tz.o
+obj-$(CONFIG_MSM_PIL_MSS_QDSP6V5) += pil-q6v5.o pil-msa.o pil-q6v5-mss.o
+obj-$(CONFIG_MSM_SCM_ERRATA) += scm-errata.o
+obj-$(CONFIG_MSM_PFE_WA) += pfe-wa.o
+obj-$(CONFIG_MSM_PERFORMANCE) += msm_performance.o
+obj-$(CONFIG_ARCH_MSM8996) += msm_cpu_voltage.o
+
+ifdef CONFIG_MSM_SUBSYSTEM_RESTART
+	obj-y += subsystem_notif.o
+	obj-y += subsystem_restart.o
+	obj-y += ramdump.o
+endif
+
+obj-$(CONFIG_MSM_SERVICE_NOTIFIER) += service-notifier.o
+obj-$(CONFIG_MSM_SYSMON_COMM) += sysmon.o sysmon-qmi.o
+obj-$(CONFIG_MSM_SECURE_BUFFER) += secure_buffer.o
+obj-$(CONFIG_TRACER_PKT) += tracer_pkt.o
+obj-$(CONFIG_ICNSS) += icnss.o wlan_firmware_service_v01.o
+obj-$(CONFIG_MSM_BAM_DMUX) += bam_dmux.o
+obj-$(CONFIG_MSM_SERVICE_LOCATOR) += service-locator.o
+obj-$(CONFIG_MSM_PACMAN)        += msm_pacman.o
+obj-$(CONFIG_MSM_QBT1000) += qbt1000.o
+obj-$(CONFIG_MSM_SCM_XPU) += scm-xpu.o
+obj-$(CONFIG_MSM_KERNEL_PROTECT) += kernel_protect.o
+obj-$(CONFIG_MSM_RTB) += msm_rtb-hotplug.o
+obj-$(CONFIG_MSM_REMOTEQDSS) += remoteqdss.o
diff --git a/drivers/soc/qcom/glink_ssr.c b/drivers/soc/qcom/glink_ssr.c
index 19c7399eddb5..4952e12ffe3c 100644
--- a/drivers/soc/qcom/glink_ssr.c
+++ b/drivers/soc/qcom/glink_ssr.c
@@ -1,6 +1,4 @@
-/*
- * Copyright (c) 2014-2017, The Linux Foundation. All rights reserved.
- * Copyright (c) 2017, Linaro Ltd.
+/* Copyright (c) 2014-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -11,154 +9,969 @@
  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  * GNU General Public License for more details.
  */
-
-#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/ipc_logging.h>
+#include <linux/list.h>
 #include <linux/module.h>
 #include <linux/notifier.h>
-#include <linux/rpmsg.h>
-#include <linux/remoteproc/qcom_rproc.h>
-
-/**
- * struct do_cleanup_msg - The data structure for an SSR do_cleanup message
- * version:     The G-Link SSR protocol version
- * command:     The G-Link SSR command - do_cleanup
- * seq_num:     Sequence number
- * name_len:    Length of the name of the subsystem being restarted
- * name:        G-Link edge name of the subsystem being restarted
- */
-struct do_cleanup_msg {
-	__le32 version;
-	__le32 command;
-	__le32 seq_num;
-	__le32 name_len;
-	char name[32];
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/suspend.h>
+#include <linux/random.h>
+#include <soc/qcom/glink.h>
+#include <soc/qcom/subsystem_notif.h>
+#include <soc/qcom/subsystem_restart.h>
+#include "glink_private.h"
+
+#define GLINK_SSR_REPLY_TIMEOUT	HZ
+#define GLINK_SSR_INTENT_REQ_TIMEOUT_MS 500
+#define GLINK_SSR_EVENT_INIT ~0
+#define NUM_LOG_PAGES 3
+
+#define GLINK_SSR_LOG(x...) do { \
+	if (glink_ssr_log_ctx) \
+		ipc_log_string(glink_ssr_log_ctx, x); \
+} while (0)
+
+#define GLINK_SSR_ERR(x...) do { \
+	pr_err(x); \
+	GLINK_SSR_LOG(x); \
+} while (0)
+
+static void *glink_ssr_log_ctx;
+
+/* Global restart counter */
+static uint32_t sequence_number;
+
+/* Flag indicating if responses were received for all SSR notifications */
+static bool notifications_successful;
+
+/* Completion for setting notifications_successful */
+struct completion notifications_successful_complete;
+
+/**
+ * struct restart_notifier_block - restart notifier wrapper structure
+ * subsystem:	the name of the subsystem as recognized by the SSR framework
+ * nb:		notifier block structure used by the SSR framework
+ */
+struct restart_notifier_block {
+	const char *subsystem;
+	struct notifier_block nb;
 };
 
 /**
- * struct cleanup_done_msg - The data structure for an SSR cleanup_done message
- * version:     The G-Link SSR protocol version
- * response:    The G-Link SSR response to a do_cleanup command, cleanup_done
- * seq_num:     Sequence number
+ * struct configure_and_open_ch_work - Work structure for used for opening
+ *				glink_ssr channels
+ * edge:	The G-Link edge obtained from the link state callback
+ * transport:	The G-Link transport obtained from the link state callback
+ * link_state:	The link state obtained from the link state callback
+ * ss_info:	Subsystem information structure containing the info for this
+ *		callback
+ * work:	Work structure
  */
-struct cleanup_done_msg {
-	__le32 version;
-	__le32 response;
-	__le32 seq_num;
+struct configure_and_open_ch_work {
+	char edge[GLINK_NAME_SIZE];
+	char transport[GLINK_NAME_SIZE];
+	enum glink_link_state link_state;
+	struct subsys_info *ss_info;
+	struct work_struct work;
 };
 
 /**
- * G-Link SSR protocol commands
+ * struct close_ch_work - Work structure for used for closing glink_ssr channels
+ * edge:	The G-Link edge name for the channel being closed
+ * handle:	G-Link channel handle to be closed
+ * work:	Work structure
  */
-#define GLINK_SSR_DO_CLEANUP	0
-#define GLINK_SSR_CLEANUP_DONE	1
+struct close_ch_work {
+	char edge[GLINK_NAME_SIZE];
+	void *handle;
+	struct work_struct work;
+};
 
-struct glink_ssr {
-	struct device *dev;
-	struct rpmsg_endpoint *ept;
+static int glink_ssr_restart_notifier_cb(struct notifier_block *this,
+				  unsigned long code,
+				  void *data);
+static void delete_ss_info_notify_list(struct subsys_info *ss_info);
+static int configure_and_open_channel(struct subsys_info *ss_info);
+static struct workqueue_struct *glink_ssr_wq;
 
-	struct notifier_block nb;
+static LIST_HEAD(subsystem_list);
+static atomic_t responses_remaining = ATOMIC_INIT(0);
+static wait_queue_head_t waitqueue;
 
-	u32 seq_num;
-	struct completion completion;
-};
+static void link_state_cb_worker(struct work_struct *work)
+{
+	unsigned long flags;
+	struct configure_and_open_ch_work *ch_open_work =
+		container_of(work, struct configure_and_open_ch_work, work);
+	struct subsys_info *ss_info = ch_open_work->ss_info;
+
+	GLINK_SSR_LOG("<SSR> %s: LINK STATE[%d] %s:%s\n", __func__,
+			ch_open_work->link_state, ch_open_work->edge,
+			ch_open_work->transport);
 
-static int qcom_glink_ssr_callback(struct rpmsg_device *rpdev,
-				   void *data, int len, void *priv, u32 addr)
+	if (ss_info && ch_open_work->link_state == GLINK_LINK_STATE_UP) {
+		spin_lock_irqsave(&ss_info->link_up_lock, flags);
+		if (!ss_info->link_up) {
+			ss_info->link_up = true;
+			spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+			if (!configure_and_open_channel(ss_info)) {
+				glink_unregister_link_state_cb(
+						ss_info->link_state_handle);
+				ss_info->link_state_handle = NULL;
+			}
+			kfree(ch_open_work);
+			return;
+		}
+		spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+	} else {
+		if (ss_info) {
+			spin_lock_irqsave(&ss_info->link_up_lock, flags);
+			ss_info->link_up = false;
+			spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+			ss_info->handle = NULL;
+		} else {
+			GLINK_SSR_ERR("<SSR> %s: ss_info is NULL\n", __func__);
+		}
+	}
+
+	kfree(ch_open_work);
+}
+
+/**
+ * glink_lbsrv_link_state_cb() - Callback to receive link state updates
+ * @cb_info:	Information containing link & its state.
+ * @priv:	Private data passed during the link state registration.
+ *
+ * This function is called by the G-Link core to notify the glink_ssr module
+ * regarding the link state updates. This function is registered with the
+ * G-Link core by the loopback server during glink_register_link_state_cb().
+ */
+static void glink_ssr_link_state_cb(struct glink_link_state_cb_info *cb_info,
+				      void *priv)
 {
-	struct cleanup_done_msg *msg = data;
-	struct glink_ssr *ssr = dev_get_drvdata(&rpdev->dev);
+	struct subsys_info *ss_info;
+	struct configure_and_open_ch_work *open_ch_work;
 
-	if (len < sizeof(*msg)) {
-		dev_err(ssr->dev, "message too short\n");
-		return -EINVAL;
+	if (!cb_info) {
+		GLINK_SSR_ERR("<SSR> %s: Missing cb_data\n", __func__);
+		return;
 	}
 
-	if (le32_to_cpu(msg->version) != 0)
-		return -EINVAL;
+	ss_info = get_info_for_edge(cb_info->edge);
 
-	if (le32_to_cpu(msg->response) != GLINK_SSR_CLEANUP_DONE)
-		return 0;
+	open_ch_work = kmalloc(sizeof(*open_ch_work), GFP_KERNEL);
+	if (!open_ch_work) {
+		GLINK_SSR_ERR("<SSR> %s: Could not allocate open_ch_work\n",
+				__func__);
+		return;
+	}
 
-	if (le32_to_cpu(msg->seq_num) != ssr->seq_num) {
-		dev_err(ssr->dev, "invalid sequence number of response\n");
-		return -EINVAL;
+	strlcpy(open_ch_work->edge, cb_info->edge, GLINK_NAME_SIZE);
+	strlcpy(open_ch_work->transport, cb_info->transport, GLINK_NAME_SIZE);
+	open_ch_work->link_state = cb_info->link_state;
+	open_ch_work->ss_info = ss_info;
+
+	INIT_WORK(&open_ch_work->work, link_state_cb_worker);
+	queue_work(glink_ssr_wq, &open_ch_work->work);
+}
+
+/**
+ * glink_ssr_notify_rx() - RX Notification callback
+ * @handle:	G-Link channel handle
+ * @priv:	Private callback data
+ * @pkt_priv:	Private packet data
+ * @ptr:	Pointer to the data received
+ * @size:	Size of the data received
+ *
+ * This function is a notification callback from the G-Link core that data
+ * has been received from the remote side. This data is validate to make
+ * sure it is a cleanup_done message and is processed accordingly if it is.
+ */
+void glink_ssr_notify_rx(void *handle, const void *priv, const void *pkt_priv,
+		const void *ptr, size_t size)
+{
+	struct ssr_notify_data *cb_data = (struct ssr_notify_data *)priv;
+	struct cleanup_done_msg *resp = (struct cleanup_done_msg *)ptr;
+
+	if (unlikely(!cb_data))
+		goto missing_cb_data;
+	if (unlikely(!cb_data->do_cleanup_data))
+		goto missing_do_cleanup_data;
+	if (unlikely(!resp))
+		goto missing_response;
+	if (unlikely(resp->version != cb_data->do_cleanup_data->version))
+		goto version_mismatch;
+	if (unlikely(resp->seq_num != cb_data->do_cleanup_data->seq_num))
+		goto invalid_seq_number;
+	if (unlikely(resp->response != GLINK_SSR_CLEANUP_DONE))
+		goto wrong_response;
+
+	cb_data->responded = true;
+	atomic_dec(&responses_remaining);
+
+	GLINK_SSR_LOG(
+		"<SSR> %s: Response from %s resp[%d] version[%d] seq_num[%d] restarted[%s]\n",
+			__func__, cb_data->edge, resp->response,
+			resp->version, resp->seq_num,
+			cb_data->do_cleanup_data->name);
+
+	kfree(cb_data->do_cleanup_data);
+	cb_data->do_cleanup_data = NULL;
+	wake_up(&waitqueue);
+	return;
+
+missing_cb_data:
+	panic("%s: Missing cb_data!\n", __func__);
+	return;
+missing_do_cleanup_data:
+	panic("%s: Missing do_cleanup data!\n", __func__);
+	return;
+missing_response:
+	GLINK_SSR_ERR("<SSR> %s: Missing response data\n", __func__);
+	return;
+version_mismatch:
+	GLINK_SSR_ERR("<SSR> %s: Version mismatch. %s[%d], %s[%d]\n", __func__,
+			"do_cleanup version", cb_data->do_cleanup_data->version,
+			"cleanup_done version", resp->version);
+	return;
+invalid_seq_number:
+	GLINK_SSR_ERR("<SSR> %s: Invalid seq. number. %s[%d], %s[%d]\n",
+			__func__, "do_cleanup seq num",
+			cb_data->do_cleanup_data->seq_num,
+			"cleanup_done seq_num", resp->seq_num);
+	return;
+wrong_response:
+	GLINK_SSR_ERR("<SSR> %s: Not a cleaup_done message. %s[%d]\n", __func__,
+			"cleanup_done response", resp->response);
+	return;
+}
+
+/**
+ * glink_ssr_notify_tx_done() - Transmit finished notification callback
+ * @handle:	G-Link channel handle
+ * @priv:	Private callback data
+ * @pkt_priv:	Private packet data
+ * @ptr:	Pointer to the data received
+ *
+ * This function is a notification callback from the G-Link core that data
+ * we sent has finished transmitting.
+ */
+void glink_ssr_notify_tx_done(void *handle, const void *priv,
+		const void *pkt_priv, const void *ptr)
+{
+	struct ssr_notify_data *cb_data = (struct ssr_notify_data *)priv;
+
+	if (unlikely(!cb_data)) {
+		panic("%s: cb_data is NULL!\n", __func__);
+		return;
 	}
 
-	complete(&ssr->completion);
+	GLINK_SSR_LOG("<SSR> %s: Notified %s of restart\n",
+		__func__, cb_data->edge);
 
-	return 0;
+	cb_data->tx_done = true;
 }
 
-static int qcom_glink_ssr_notify(struct notifier_block *nb, unsigned long event,
-				 void *data)
+void close_ch_worker(struct work_struct *work)
 {
-	struct glink_ssr *ssr = container_of(nb, struct glink_ssr, nb);
-	struct do_cleanup_msg msg;
-	char *ssr_name = data;
-	int ret;
+	unsigned long flags;
+	void *link_state_handle;
+	struct subsys_info *ss_info;
+	struct close_ch_work *close_work =
+		container_of(work, struct close_ch_work, work);
+
+	glink_close(close_work->handle);
 
-	ssr->seq_num++;
-	reinit_completion(&ssr->completion);
+	ss_info = get_info_for_edge(close_work->edge);
+	BUG_ON(!ss_info);
 
-	memset(&msg, 0, sizeof(msg));
-	msg.command = cpu_to_le32(GLINK_SSR_DO_CLEANUP);
-	msg.seq_num = cpu_to_le32(ssr->seq_num);
-	msg.name_len = cpu_to_le32(strlen(ssr_name));
-	strlcpy(msg.name, ssr_name, sizeof(msg.name));
+	spin_lock_irqsave(&ss_info->link_up_lock, flags);
+	ss_info->link_up = false;
+	spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 
-	ret = rpmsg_send(ssr->ept, &msg, sizeof(msg));
-	if (ret < 0)
-		dev_err(ssr->dev, "failed to send cleanup message\n");
+	BUG_ON(ss_info->link_state_handle != NULL);
+	link_state_handle = glink_register_link_state_cb(ss_info->link_info,
+			NULL);
 
-	ret = wait_for_completion_timeout(&ssr->completion, HZ);
-	if (!ret)
-		dev_err(ssr->dev, "timeout waiting for cleanup done message\n");
+	if (IS_ERR_OR_NULL(link_state_handle))
+		GLINK_SSR_ERR("<SSR> %s: %s, ret[%d]\n", __func__,
+				"Couldn't register link state cb",
+				(int)PTR_ERR(link_state_handle));
+	else
+		ss_info->link_state_handle = link_state_handle;
 
+	BUG_ON(!ss_info->cb_data);
+	kfree(ss_info->cb_data);
+	kfree(close_work);
+}
+
+/**
+ * glink_ssr_notify_state() - Channel state notification callback
+ * @handle:	G-Link channel handle
+ * @priv:	Private callback data
+ * @event:	The state that has been transitioned to
+ *
+ * This function is a notification callback from the G-Link core that the
+ * channel state has changed.
+ */
+void glink_ssr_notify_state(void *handle, const void *priv, unsigned event)
+{
+	struct ssr_notify_data *cb_data = (struct ssr_notify_data *)priv;
+	struct close_ch_work *close_work;
+
+	if (!cb_data) {
+		GLINK_SSR_ERR("<SSR> %s: Could not allocate data for cb_data\n",
+				__func__);
+	} else {
+		GLINK_SSR_LOG("<SSR> %s: event[%d]\n",
+				__func__, event);
+		cb_data->event = event;
+		if (event == GLINK_REMOTE_DISCONNECTED) {
+			close_work =
+				kmalloc(sizeof(struct close_ch_work),
+						GFP_KERNEL);
+			if (!close_work) {
+				GLINK_SSR_ERR(
+					"<SSR> %s: Could not allocate %s\n",
+						__func__, "close work");
+				return;
+			}
+
+			strlcpy(close_work->edge, cb_data->edge,
+					sizeof(close_work->edge));
+			close_work->handle = handle;
+			INIT_WORK(&close_work->work, close_ch_worker);
+			queue_work(glink_ssr_wq, &close_work->work);
+		}
+	}
+}
+
+/**
+ * glink_ssr_notify_rx_intent_req() - RX intent request notification callback
+ * @handle:	G-Link channel handle
+ * @priv:	Private callback data
+ * @req_size:	The size of the requested intent
+ *
+ * This function is a notification callback from the G-Link core of the remote
+ * side's request for an RX intent to be queued.
+ *
+ * Return: Boolean indicating whether or not the request was successfully
+ *         received
+ */
+bool glink_ssr_notify_rx_intent_req(void *handle, const void *priv,
+		size_t req_size)
+{
+	struct ssr_notify_data *cb_data = (struct ssr_notify_data *)priv;
+
+	if (!cb_data) {
+		GLINK_SSR_ERR("<SSR> %s: Could not allocate data for cb_data\n",
+				__func__);
+		return false;
+	} else {
+		GLINK_SSR_LOG("<SSR> %s: rx_intent_req of size %zu\n",
+				__func__, req_size);
+		return true;
+	}
+}
+
+/**
+ * glink_ssr_restart_notifier_cb() - SSR restart notifier callback function
+ * @this:	Notifier block used by the SSR framework
+ * @code:	The SSR code for which stage of restart is occurring
+ * @data:	Structure containing private data - not used here.
+ *
+ * This function is a callback for the SSR framework. From here we initiate
+ * our handling of SSR.
+ *
+ * Return: Status of SSR handling
+ */
+static int glink_ssr_restart_notifier_cb(struct notifier_block *this,
+				  unsigned long code,
+				  void *data)
+{
+	int ret = 0;
+	struct subsys_info *ss_info = NULL;
+	struct restart_notifier_block *notifier =
+		container_of(this, struct restart_notifier_block, nb);
+
+	if (code == SUBSYS_AFTER_SHUTDOWN) {
+		GLINK_SSR_LOG("<SSR> %s: %s: subsystem restart for %s\n",
+				__func__, "SUBSYS_AFTER_SHUTDOWN",
+				notifier->subsystem);
+		ss_info = get_info_for_subsystem(notifier->subsystem);
+		if (ss_info == NULL) {
+			GLINK_SSR_ERR("<SSR> %s: ss_info is NULL\n", __func__);
+			return -EINVAL;
+		}
+
+		glink_ssr(ss_info->edge);
+		ret = notify_for_subsystem(ss_info);
+
+		if (ret) {
+			GLINK_SSR_ERR("<SSR>: %s: %s, ret[%d]\n", __func__,
+					"Subsystem notification failed", ret);
+			return ret;
+		}
+	}
 	return NOTIFY_DONE;
 }
 
-static int qcom_glink_ssr_probe(struct rpmsg_device *rpdev)
+/**
+ * notify for subsystem() - Notify other subsystems that a subsystem is being
+ *                          restarted
+ * @ss_info:	Subsystem info structure for the subsystem being restarted
+ *
+ * This function sends notifications to affected subsystems that the subsystem
+ * in ss_info is being restarted, and waits for the cleanup done response from
+ * all of those subsystems. It also initiates any local cleanup that is
+ * necessary.
+ *
+ * Return: 0 on success, standard error codes otherwise
+ */
+int notify_for_subsystem(struct subsys_info *ss_info)
+{
+	struct subsys_info *ss_info_channel;
+	struct subsys_info_leaf *ss_leaf_entry;
+	struct do_cleanup_msg *do_cleanup_data;
+	void *handle;
+	int wait_ret;
+	int ret;
+	unsigned long flags;
+
+	if (!ss_info) {
+		GLINK_SSR_ERR("<SSR> %s: ss_info structure invalid\n",
+				__func__);
+		return -EINVAL;
+	}
+
+	/*
+	 * No locking is needed here because ss_info->notify_list_len is
+	 * only modified during setup.
+	 */
+	atomic_set(&responses_remaining, ss_info->notify_list_len);
+	init_waitqueue_head(&waitqueue);
+	notifications_successful = true;
+
+	list_for_each_entry(ss_leaf_entry, &ss_info->notify_list,
+			notify_list_node) {
+		GLINK_SSR_LOG(
+			"<SSR> %s: Notifying: %s:%s of %s restart, seq_num[%d]\n",
+				__func__, ss_leaf_entry->edge,
+				ss_leaf_entry->xprt, ss_info->edge,
+				sequence_number);
+
+		ss_info_channel =
+			get_info_for_subsystem(ss_leaf_entry->ssr_name);
+		if (ss_info_channel == NULL) {
+			GLINK_SSR_ERR(
+				"<SSR> %s: unable to find subsystem name\n",
+					__func__);
+			return -ENODEV;
+		}
+		handle = ss_info_channel->handle;
+		ss_leaf_entry->cb_data = ss_info_channel->cb_data;
+
+		spin_lock_irqsave(&ss_info->link_up_lock, flags);
+		if (IS_ERR_OR_NULL(ss_info_channel->handle) ||
+				!ss_info_channel->cb_data ||
+				!ss_info_channel->link_up ||
+				ss_info_channel->cb_data->event
+						!= GLINK_CONNECTED) {
+
+			GLINK_SSR_LOG(
+				"<SSR> %s: %s:%s %s[%d], %s[%p], %s[%d]\n",
+				__func__, ss_leaf_entry->edge, "Not connected",
+				"resp. remaining",
+				atomic_read(&responses_remaining), "handle",
+				ss_info_channel->handle, "link_up",
+				ss_info_channel->link_up);
+
+			spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+			atomic_dec(&responses_remaining);
+			continue;
+		}
+		spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+
+		do_cleanup_data = kmalloc(sizeof(struct do_cleanup_msg),
+				GFP_KERNEL);
+		if (!do_cleanup_data) {
+			GLINK_SSR_ERR(
+				"%s %s: Could not allocate do_cleanup_msg\n",
+				"<SSR>", __func__);
+			return -ENOMEM;
+		}
+
+		do_cleanup_data->version = 0;
+		do_cleanup_data->command = GLINK_SSR_DO_CLEANUP;
+		do_cleanup_data->seq_num = sequence_number;
+		do_cleanup_data->name_len = strlen(ss_info->edge);
+		strlcpy(do_cleanup_data->name, ss_info->edge,
+				do_cleanup_data->name_len + 1);
+		ss_leaf_entry->cb_data->do_cleanup_data = do_cleanup_data;
+
+		ret = glink_queue_rx_intent(handle,
+				(void *)ss_leaf_entry->cb_data,
+				sizeof(struct cleanup_done_msg));
+		if (ret) {
+			GLINK_SSR_ERR(
+				"%s %s: %s, ret[%d], resp. remaining[%d]\n",
+				"<SSR>", __func__,
+				"queue_rx_intent failed", ret,
+				atomic_read(&responses_remaining));
+			kfree(do_cleanup_data);
+			ss_leaf_entry->cb_data->do_cleanup_data = NULL;
+
+			if (strcmp(ss_leaf_entry->ssr_name, "rpm")) {
+				subsystem_restart(ss_leaf_entry->ssr_name);
+				ss_leaf_entry->restarted = true;
+			} else {
+				panic("%s: Could not queue intent for RPM!\n",
+						__func__);
+			}
+			atomic_dec(&responses_remaining);
+			continue;
+		}
+
+		if (strcmp(ss_leaf_entry->ssr_name, "rpm"))
+			ret = glink_tx(handle, ss_leaf_entry->cb_data,
+					do_cleanup_data,
+					sizeof(*do_cleanup_data),
+					GLINK_TX_REQ_INTENT);
+		else
+			ret = glink_tx(handle, ss_leaf_entry->cb_data,
+					do_cleanup_data,
+					sizeof(*do_cleanup_data),
+					GLINK_TX_SINGLE_THREADED);
+
+		if (ret) {
+			GLINK_SSR_ERR("<SSR> %s: tx failed, ret[%d], %s[%d]\n",
+					__func__, ret, "resp. remaining",
+					atomic_read(&responses_remaining));
+			kfree(do_cleanup_data);
+			ss_leaf_entry->cb_data->do_cleanup_data = NULL;
+
+			if (strcmp(ss_leaf_entry->ssr_name, "rpm")) {
+				subsystem_restart(ss_leaf_entry->ssr_name);
+				ss_leaf_entry->restarted = true;
+			} else {
+				panic("%s: glink_tx() to RPM failed!\n",
+						__func__);
+			}
+			atomic_dec(&responses_remaining);
+			continue;
+		}
+
+		sequence_number++;
+	}
+
+	wait_ret = wait_event_timeout(waitqueue,
+			atomic_read(&responses_remaining) == 0,
+			GLINK_SSR_REPLY_TIMEOUT);
+
+	list_for_each_entry(ss_leaf_entry, &ss_info->notify_list,
+			notify_list_node) {
+		if (!wait_ret && !IS_ERR_OR_NULL(ss_leaf_entry->cb_data)
+				&& !ss_leaf_entry->cb_data->responded) {
+			GLINK_SSR_ERR("%s %s: Subsystem %s %s\n",
+				"<SSR>", __func__, ss_leaf_entry->edge,
+				"failed to respond. Restarting.");
+
+			notifications_successful = false;
+
+			/* Check for RPM, as it can't be restarted */
+			if (!strcmp(ss_leaf_entry->ssr_name, "rpm"))
+				panic("%s: RPM failed to respond!\n", __func__);
+			else if (!ss_leaf_entry->restarted)
+				subsystem_restart(ss_leaf_entry->ssr_name);
+		}
+		ss_leaf_entry->restarted = false;
+
+		if (!IS_ERR_OR_NULL(ss_leaf_entry->cb_data))
+			ss_leaf_entry->cb_data->responded = false;
+	}
+	complete(&notifications_successful_complete);
+	return 0;
+}
+EXPORT_SYMBOL(notify_for_subsystem);
+
+/**
+ * configure_and_open_channel() - configure and open a G-Link channel for
+ *                                the given subsystem
+ * @ss_info:	The subsys_info structure where the channel will be stored
+ *
+ * Return: 0 on success, standard error codes otherwise
+ */
+static int configure_and_open_channel(struct subsys_info *ss_info)
 {
-	struct glink_ssr *ssr;
+	struct glink_open_config open_cfg;
+	struct ssr_notify_data *cb_data = NULL;
+	void *handle = NULL;
+
+	if (!ss_info) {
+		GLINK_SSR_ERR("<SSR> %s: ss_info structure invalid\n",
+				__func__);
+		return -EINVAL;
+	}
 
-	ssr = devm_kzalloc(&rpdev->dev, sizeof(*ssr), GFP_KERNEL);
-	if (!ssr)
+	cb_data = kmalloc(sizeof(struct ssr_notify_data), GFP_KERNEL);
+	if (!cb_data) {
+		GLINK_SSR_ERR("<SSR> %s: Could not allocate cb_data\n",
+				__func__);
 		return -ENOMEM;
+	}
+	cb_data->responded = false;
+	cb_data->event = GLINK_SSR_EVENT_INIT;
+	cb_data->edge = ss_info->edge;
+	ss_info->cb_data = cb_data;
 
-	init_completion(&ssr->completion);
+	memset(&open_cfg, 0, sizeof(struct glink_open_config));
 
-	ssr->dev = &rpdev->dev;
-	ssr->ept = rpdev->ept;
-	ssr->nb.notifier_call = qcom_glink_ssr_notify;
+	if (ss_info->xprt) {
+		open_cfg.transport = ss_info->xprt;
+	} else {
+		open_cfg.transport = NULL;
+		open_cfg.options = GLINK_OPT_INITIAL_XPORT;
+	}
+	open_cfg.edge = ss_info->edge;
+	open_cfg.name = "glink_ssr";
+	open_cfg.notify_rx = glink_ssr_notify_rx;
+	open_cfg.notify_tx_done = glink_ssr_notify_tx_done;
+	open_cfg.notify_state = glink_ssr_notify_state;
+	open_cfg.notify_rx_intent_req = glink_ssr_notify_rx_intent_req;
+	open_cfg.priv = ss_info->cb_data;
+	open_cfg.rx_intent_req_timeout_ms = GLINK_SSR_INTENT_REQ_TIMEOUT_MS;
 
-	dev_set_drvdata(&rpdev->dev, ssr);
+	handle = glink_open(&open_cfg);
+	if (IS_ERR_OR_NULL(handle)) {
+		GLINK_SSR_ERR(
+			"<SSR> %s:%s %s: unable to open channel, ret[%d]\n",
+				 open_cfg.edge, open_cfg.name, __func__,
+				 (int)PTR_ERR(handle));
+		kfree(cb_data);
+		cb_data = NULL;
+		ss_info->cb_data = NULL;
+		return PTR_ERR(handle);
+	}
+	ss_info->handle = handle;
+	return 0;
+}
 
-	return qcom_register_ssr_notifier(&ssr->nb);
+/**
+ * get_info_for_subsystem() - Retrieve information about a subsystem from the
+ *                            global subsystem_info_list
+ * @subsystem:	The name of the subsystem recognized by the SSR
+ *		framework
+ *
+ * Return: subsys_info structure containing info for the requested subsystem;
+ *         NULL if no structure can be found for the requested subsystem
+ */
+struct subsys_info *get_info_for_subsystem(const char *subsystem)
+{
+	struct subsys_info *ss_info_entry;
+
+	list_for_each_entry(ss_info_entry, &subsystem_list,
+			subsystem_list_node) {
+		if (!strcmp(subsystem, ss_info_entry->ssr_name))
+			return ss_info_entry;
+	}
+
+	return NULL;
 }
+EXPORT_SYMBOL(get_info_for_subsystem);
 
-static void qcom_glink_ssr_remove(struct rpmsg_device *rpdev)
+/**
+ * get_info_for_edge() - Retrieve information about a subsystem from the
+ *                       global subsystem_info_list
+ * @edge:	The name of the edge recognized by G-Link
+ *
+ * Return: subsys_info structure containing info for the requested subsystem;
+ *         NULL if no structure can be found for the requested subsystem
+ */
+struct subsys_info *get_info_for_edge(const char *edge)
 {
-	struct glink_ssr *ssr = dev_get_drvdata(&rpdev->dev);
+	struct subsys_info *ss_info_entry;
 
-	qcom_unregister_ssr_notifier(&ssr->nb);
+	list_for_each_entry(ss_info_entry, &subsystem_list,
+			subsystem_list_node) {
+		if (!strcmp(edge, ss_info_entry->edge))
+			return ss_info_entry;
+	}
+
+	return NULL;
 }
+EXPORT_SYMBOL(get_info_for_edge);
 
-static const struct rpmsg_device_id qcom_glink_ssr_match[] = {
-	{ "glink_ssr" },
-	{}
+/**
+ * glink_ssr_get_seq_num() - Get the current SSR sequence number
+ *
+ * Return: The current SSR sequence number
+ */
+uint32_t glink_ssr_get_seq_num(void)
+{
+	return sequence_number;
+}
+EXPORT_SYMBOL(glink_ssr_get_seq_num);
+
+/**
+ * delete_ss_info_notify_list() - Delete the notify list for a subsystem
+ * @ss_info:	The subsystem info structure
+ */
+static void delete_ss_info_notify_list(struct subsys_info *ss_info)
+{
+	struct subsys_info_leaf *leaf, *temp;
+
+	list_for_each_entry_safe(leaf, temp, &ss_info->notify_list,
+			notify_list_node) {
+		list_del(&leaf->notify_list_node);
+		kfree(leaf);
+	}
+}
+
+/**
+ * glink_ssr_wait_cleanup_done() - Get the value of the
+ *                                 notifications_successful flag.
+ * @timeout_multiplier: timeout multiplier for waiting on all processors
+ *
+ * Return: True if cleanup_done received from all processors, false otherwise
+ */
+bool glink_ssr_wait_cleanup_done(unsigned ssr_timeout_multiplier)
+{
+	int wait_ret =
+		wait_for_completion_timeout(&notifications_successful_complete,
+			ssr_timeout_multiplier * GLINK_SSR_REPLY_TIMEOUT);
+	reinit_completion(&notifications_successful_complete);
+
+	if (!notifications_successful || !wait_ret)
+		return false;
+	else
+		return true;
+}
+EXPORT_SYMBOL(glink_ssr_wait_cleanup_done);
+
+/**
+ * glink_ssr_probe() - G-Link SSR platform device probe function
+ * @pdev:	Pointer to the platform device structure
+ *
+ * This function parses DT for information on which subsystems should be
+ * notified when each subsystem undergoes SSR. The global subsystem information
+ * list is built from this information. In addition, SSR notifier callback
+ * functions are registered here for the necessary subsystems.
+ *
+ * Return: 0 on success, standard error codes otherwise
+ */
+static int glink_ssr_probe(struct platform_device *pdev)
+{
+	struct device_node *node;
+	struct device_node *phandle_node;
+	struct restart_notifier_block *nb;
+	struct subsys_info *ss_info;
+	struct subsys_info_leaf *ss_info_leaf;
+	struct glink_link_info *link_info;
+	char *key;
+	const char *edge;
+	const char *subsys_name;
+	const char *xprt;
+	void *handle;
+	void *link_state_handle;
+	int phandle_index = 0;
+	int ret = 0;
+
+	if (!pdev) {
+		GLINK_SSR_ERR("<SSR> %s: pdev is NULL\n", __func__);
+		ret = -EINVAL;
+		goto pdev_null_or_ss_info_alloc_failed;
+	}
+
+	node = pdev->dev.of_node;
+
+	ss_info = kmalloc(sizeof(*ss_info), GFP_KERNEL);
+	if (!ss_info) {
+		GLINK_SSR_ERR("<SSR> %s: %s\n", __func__,
+			"Could not allocate subsystem info structure\n");
+		ret = -ENOMEM;
+		goto pdev_null_or_ss_info_alloc_failed;
+	}
+	INIT_LIST_HEAD(&ss_info->notify_list);
+
+	link_info = kmalloc(sizeof(struct glink_link_info),
+			GFP_KERNEL);
+	if (!link_info) {
+		GLINK_SSR_ERR("<SSR> %s: %s\n", __func__,
+			"Could not allocate link info structure\n");
+		ret = -ENOMEM;
+		goto link_info_alloc_failed;
+	}
+	ss_info->link_info = link_info;
+
+	key = "label";
+	subsys_name = of_get_property(node, key, NULL);
+	if (!subsys_name) {
+		GLINK_SSR_ERR("<SSR> %s: missing key %s\n", __func__, key);
+		ret = -ENODEV;
+		goto label_or_edge_missing;
+	}
+
+	key = "qcom,edge";
+	edge = of_get_property(node, key, NULL);
+	if (!edge) {
+		GLINK_SSR_ERR("<SSR> %s: missing key %s\n", __func__, key);
+		ret = -ENODEV;
+		goto label_or_edge_missing;
+	}
+
+	key = "qcom,xprt";
+	xprt = of_get_property(node, key, NULL);
+	if (!xprt)
+		GLINK_SSR_LOG(
+			"%s %s: no transport present for subys/edge %s/%s\n",
+			"<SSR>", __func__, subsys_name, edge);
+
+	ss_info->ssr_name = subsys_name;
+	ss_info->edge = edge;
+	ss_info->xprt = xprt;
+	ss_info->notify_list_len = 0;
+	ss_info->link_info->transport = xprt;
+	ss_info->link_info->edge = edge;
+	ss_info->link_info->glink_link_state_notif_cb = glink_ssr_link_state_cb;
+	ss_info->link_up = false;
+	ss_info->handle = NULL;
+	ss_info->link_state_handle = NULL;
+	ss_info->cb_data = NULL;
+	spin_lock_init(&ss_info->link_up_lock);
+
+	nb = kmalloc(sizeof(struct restart_notifier_block), GFP_KERNEL);
+	if (!nb) {
+		GLINK_SSR_ERR("<SSR> %s: Could not allocate notifier block\n",
+				__func__);
+		ret = -ENOMEM;
+		goto label_or_edge_missing;
+	}
+
+	nb->subsystem = subsys_name;
+	nb->nb.notifier_call = glink_ssr_restart_notifier_cb;
+
+	handle = subsys_notif_register_notifier(nb->subsystem, &nb->nb);
+	if (IS_ERR_OR_NULL(handle)) {
+		GLINK_SSR_ERR("<SSR> %s: Could not register SSR notifier cb\n",
+				__func__);
+		ret = -EINVAL;
+		goto nb_registration_fail;
+	}
+
+	key = "qcom,notify-edges";
+	while (true) {
+		phandle_node = of_parse_phandle(node, key, phandle_index++);
+		if (!phandle_node && phandle_index == 0) {
+			GLINK_SSR_ERR(
+				"<SSR> %s: qcom,notify-edges is not present",
+				__func__);
+			ret = -ENODEV;
+			goto notify_edges_not_present;
+		}
+
+		if (!phandle_node)
+			break;
+
+		ss_info_leaf = kmalloc(sizeof(struct subsys_info_leaf),
+				GFP_KERNEL);
+		if (!ss_info_leaf) {
+			GLINK_SSR_ERR(
+				"<SSR> %s: Could not allocate subsys_info_leaf\n",
+				__func__);
+			ret = -ENOMEM;
+			goto notify_edges_not_present;
+		}
+
+		subsys_name = of_get_property(phandle_node, "label", NULL);
+		edge = of_get_property(phandle_node, "qcom,edge", NULL);
+		xprt = of_get_property(phandle_node, "qcom,xprt", NULL);
+
+		of_node_put(phandle_node);
+
+		if (!subsys_name || !edge) {
+			GLINK_SSR_ERR(
+				"%s, %s: Found DT node with invalid data!\n",
+				"<SSR>", __func__);
+			ret = -EINVAL;
+			goto invalid_dt_node;
+		}
+
+		ss_info_leaf->ssr_name = subsys_name;
+		ss_info_leaf->edge = edge;
+		ss_info_leaf->xprt = xprt;
+		ss_info_leaf->restarted = false;
+		list_add_tail(&ss_info_leaf->notify_list_node,
+				&ss_info->notify_list);
+		ss_info->notify_list_len++;
+	}
+
+	list_add_tail(&ss_info->subsystem_list_node, &subsystem_list);
+
+	link_state_handle = glink_register_link_state_cb(ss_info->link_info,
+			NULL);
+	if (IS_ERR_OR_NULL(link_state_handle)) {
+		GLINK_SSR_ERR("<SSR> %s: Could not register link state cb\n",
+				__func__);
+		ret = PTR_ERR(link_state_handle);
+		goto link_state_register_fail;
+	}
+	ss_info->link_state_handle = link_state_handle;
+
+	return 0;
+
+link_state_register_fail:
+	list_del(&ss_info->subsystem_list_node);
+invalid_dt_node:
+	kfree(ss_info_leaf);
+notify_edges_not_present:
+	subsys_notif_unregister_notifier(handle, &nb->nb);
+	delete_ss_info_notify_list(ss_info);
+nb_registration_fail:
+	kfree(nb);
+label_or_edge_missing:
+	kfree(link_info);
+link_info_alloc_failed:
+	kfree(ss_info);
+pdev_null_or_ss_info_alloc_failed:
+	return ret;
+}
+
+static struct of_device_id match_table[] = {
+	{ .compatible = "qcom,glink_ssr" },
+	{},
 };
 
-static struct rpmsg_driver qcom_glink_ssr_driver = {
-	.probe = qcom_glink_ssr_probe,
-	.remove = qcom_glink_ssr_remove,
-	.callback = qcom_glink_ssr_callback,
-	.id_table = qcom_glink_ssr_match,
-	.drv = {
-		.name = "qcom_glink_ssr",
+static struct platform_driver glink_ssr_driver = {
+	.probe = glink_ssr_probe,
+	.driver = {
+		.name = "msm_glink_ssr",
+		.owner = THIS_MODULE,
+		.of_match_table = match_table,
 	},
 };
-module_rpmsg_driver(qcom_glink_ssr_driver);
 
-MODULE_ALIAS("rpmsg:glink_ssr");
-MODULE_DESCRIPTION("Qualcomm GLINK SSR notifier");
+static int glink_ssr_init(void)
+{
+	int ret;
+
+	glink_ssr_log_ctx =
+		ipc_log_context_create(NUM_LOG_PAGES, "glink_ssr", 0);
+	glink_ssr_wq = create_singlethread_workqueue("glink_ssr_wq");
+	ret = platform_driver_register(&glink_ssr_driver);
+	if (ret)
+		GLINK_SSR_ERR("<SSR> %s: %s ret: %d\n", __func__,
+				"glink_ssr driver registration failed", ret);
+
+	notifications_successful = false;
+	init_completion(&notifications_successful_complete);
+	return 0;
+}
+
+module_init(glink_ssr_init);
+
+MODULE_DESCRIPTION("MSM Generic Link (G-Link) SSR Module");
 MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/qcom/qcom_gsbi.c b/drivers/soc/qcom/qcom_gsbi.c
index 09c669e70d63..7e1f120f2b32 100644
--- a/drivers/soc/qcom/qcom_gsbi.c
+++ b/drivers/soc/qcom/qcom_gsbi.c
@@ -18,129 +18,22 @@
 #include <linux/of.h>
 #include <linux/of_platform.h>
 #include <linux/platform_device.h>
-#include <linux/regmap.h>
-#include <linux/mfd/syscon.h>
-#include <dt-bindings/soc/qcom,gsbi.h>
 
 #define GSBI_CTRL_REG		0x0000
 #define GSBI_PROTOCOL_SHIFT	4
-#define MAX_GSBI		12
-
-#define TCSR_ADM_CRCI_BASE	0x70
-
-struct crci_config {
-	u32 num_rows;
-	const u32 (*array)[MAX_GSBI];
-};
-
-static const u32 crci_ipq8064[][MAX_GSBI] = {
-	{
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000c00, 0x003000, 0x00c000,
-		0x030000, 0x0c0000, 0x300000, 0xc00000
-	},
-	{
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000c00, 0x003000, 0x00c000,
-		0x030000, 0x0c0000, 0x300000, 0xc00000
-	},
-};
-
-static const struct crci_config config_ipq8064 = {
-	.num_rows = ARRAY_SIZE(crci_ipq8064),
-	.array = crci_ipq8064,
-};
-
-static const unsigned int crci_apq8064[][MAX_GSBI] = {
-	{
-		0x001800, 0x006000, 0x000030, 0x0000c0,
-		0x000300, 0x000400, 0x000000, 0x000000,
-		0x000000, 0x000000, 0x000000, 0x000000
-	},
-	{
-		0x000000, 0x000000, 0x000000, 0x000000,
-		0x000000, 0x000020, 0x0000c0, 0x000000,
-		0x000000, 0x000000, 0x000000, 0x000000
-	},
-};
-
-static const struct crci_config config_apq8064 = {
-	.num_rows = ARRAY_SIZE(crci_apq8064),
-	.array = crci_apq8064,
-};
-
-static const unsigned int crci_msm8960[][MAX_GSBI] = {
-	{
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000400, 0x000000, 0x000000,
-		0x000000, 0x000000, 0x000000, 0x000000
-	},
-	{
-		0x000000, 0x000000, 0x000000, 0x000000,
-		0x000000, 0x000020, 0x0000c0, 0x000300,
-		0x001800, 0x006000, 0x000000, 0x000000
-	},
-};
-
-static const struct crci_config config_msm8960 = {
-	.num_rows = ARRAY_SIZE(crci_msm8960),
-	.array = crci_msm8960,
-};
-
-static const unsigned int crci_msm8660[][MAX_GSBI] = {
-	{	/* ADM 0 - B */
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000c00, 0x003000, 0x00c000,
-		0x030000, 0x0c0000, 0x300000, 0xc00000
-	},
-	{	/* ADM 0 - B */
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000c00, 0x003000, 0x00c000,
-		0x030000, 0x0c0000, 0x300000, 0xc00000
-	},
-	{	/* ADM 1 - A */
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000c00, 0x003000, 0x00c000,
-		0x030000, 0x0c0000, 0x300000, 0xc00000
-	},
-	{	/* ADM 1 - B */
-		0x000003, 0x00000c, 0x000030, 0x0000c0,
-		0x000300, 0x000c00, 0x003000, 0x00c000,
-		0x030000, 0x0c0000, 0x300000, 0xc00000
-	},
-};
-
-static const struct crci_config config_msm8660 = {
-	.num_rows = ARRAY_SIZE(crci_msm8660),
-	.array = crci_msm8660,
-};
 
 struct gsbi_info {
 	struct clk *hclk;
 	u32 mode;
 	u32 crci;
-	struct regmap *tcsr;
-};
-
-static const struct of_device_id tcsr_dt_match[] = {
-	{ .compatible = "qcom,tcsr-ipq8064", .data = &config_ipq8064},
-	{ .compatible = "qcom,tcsr-apq8064", .data = &config_apq8064},
-	{ .compatible = "qcom,tcsr-msm8960", .data = &config_msm8960},
-	{ .compatible = "qcom,tcsr-msm8660", .data = &config_msm8660},
-	{ },
 };
 
 static int gsbi_probe(struct platform_device *pdev)
 {
 	struct device_node *node = pdev->dev.of_node;
-	struct device_node *tcsr_node;
-	const struct of_device_id *match;
 	struct resource *res;
 	void __iomem *base;
 	struct gsbi_info *gsbi;
-	int i;
-	u32 mask, gsbi_num;
-	const struct crci_config *config = NULL;
 
 	gsbi = devm_kzalloc(&pdev->dev, sizeof(*gsbi), GFP_KERNEL);
 
@@ -152,32 +45,6 @@ static int gsbi_probe(struct platform_device *pdev)
 	if (IS_ERR(base))
 		return PTR_ERR(base);
 
-	/* get the tcsr node and setup the config and regmap */
-	gsbi->tcsr = syscon_regmap_lookup_by_phandle(node, "syscon-tcsr");
-
-	if (!IS_ERR(gsbi->tcsr)) {
-		tcsr_node = of_parse_phandle(node, "syscon-tcsr", 0);
-		if (tcsr_node) {
-			match = of_match_node(tcsr_dt_match, tcsr_node);
-			if (match)
-				config = match->data;
-			else
-				dev_warn(&pdev->dev, "no matching TCSR\n");
-
-			of_node_put(tcsr_node);
-		}
-	}
-
-	if (of_property_read_u32(node, "cell-index", &gsbi_num)) {
-		dev_err(&pdev->dev, "missing cell-index\n");
-		return -EINVAL;
-	}
-
-	if (gsbi_num < 1 || gsbi_num > MAX_GSBI) {
-		dev_err(&pdev->dev, "invalid cell-index\n");
-		return -EINVAL;
-	}
-
 	if (of_property_read_u32(node, "qcom,mode", &gsbi->mode)) {
 		dev_err(&pdev->dev, "missing mode configuration\n");
 		return -EINVAL;
@@ -197,25 +64,6 @@ static int gsbi_probe(struct platform_device *pdev)
 	writel_relaxed((gsbi->mode << GSBI_PROTOCOL_SHIFT) | gsbi->crci,
 				base + GSBI_CTRL_REG);
 
-	/*
-	 * modify tcsr to reflect mode and ADM CRCI mux
-	 * Each gsbi contains a pair of bits, one for RX and one for TX
-	 * SPI mode requires both bits cleared, otherwise they are set
-	 */
-	if (config) {
-		for (i = 0; i < config->num_rows; i++) {
-			mask = config->array[i][gsbi_num - 1];
-
-			if (gsbi->mode == GSBI_PROT_SPI)
-				regmap_update_bits(gsbi->tcsr,
-					TCSR_ADM_CRCI_BASE + 4 * i, mask, 0);
-			else
-				regmap_update_bits(gsbi->tcsr,
-					TCSR_ADM_CRCI_BASE + 4 * i, mask, mask);
-
-		}
-	}
-
 	/* make sure the gsbi control write is not reordered */
 	wmb();
 
@@ -243,6 +91,7 @@ MODULE_DEVICE_TABLE(of, gsbi_dt_match);
 static struct platform_driver gsbi_driver = {
 	.driver = {
 		.name		= "gsbi",
+		.owner		= THIS_MODULE,
 		.of_match_table	= gsbi_dt_match,
 	},
 	.probe = gsbi_probe,
diff --git a/drivers/soc/qcom/qmi_interface.c b/drivers/soc/qcom/qmi_interface.c
index 938ca41c56cd..d6853e4bea72 100644
--- a/drivers/soc/qcom/qmi_interface.c
+++ b/drivers/soc/qcom/qmi_interface.c
@@ -1,848 +1,2254 @@
-// SPDX-License-Identifier: GPL-2.0
-/*
- * Copyright (C) 2017 Linaro Ltd.
+/* Copyright (c) 2012-2015, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
  */
-#include <linux/kernel.h>
+
+#include <linux/slab.h>
+#include <linux/uaccess.h>
 #include <linux/module.h>
-#include <linux/device.h>
-#include <linux/qrtr.h>
-#include <linux/net.h>
-#include <linux/completion.h>
-#include <linux/idr.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/io.h>
 #include <linux/string.h>
-#include <net/sock.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/list.h>
+#include <linux/socket.h>
+#include <linux/gfp.h>
+#include <linux/qmi_encdec.h>
 #include <linux/workqueue.h>
-#include <linux/soc/qcom/qmi.h>
-
-static struct socket *qmi_sock_create(struct qmi_handle *qmi,
-				      struct sockaddr_qrtr *sq);
+#include <linux/mutex.h>
+#include <linux/hashtable.h>
+#include <linux/ipc_router.h>
+#include <linux/ipc_logging.h>
+
+#include <soc/qcom/msm_qmi_interface.h>
+
+#include "qmi_interface_priv.h"
+
+#define BUILD_INSTANCE_ID(vers, ins) (((vers) & 0xFF) | (((ins) & 0xFF) << 8))
+#define LOOKUP_MASK 0xFFFFFFFF
+#define MAX_WQ_NAME_LEN 20
+#define QMI_REQ_RESP_LOG_PAGES 3
+#define QMI_IND_LOG_PAGES 2
+#define QMI_REQ_RESP_LOG(buf...) \
+do { \
+	if (qmi_req_resp_log_ctx) { \
+		ipc_log_string(qmi_req_resp_log_ctx, buf); \
+	} \
+} while (0) \
+
+#define QMI_IND_LOG(buf...) \
+do { \
+	if (qmi_ind_log_ctx) { \
+		ipc_log_string(qmi_ind_log_ctx, buf); \
+	} \
+} while (0) \
+
+static LIST_HEAD(svc_event_nb_list);
+static DEFINE_MUTEX(svc_event_nb_list_lock);
+
+struct qmi_notify_event_work {
+	unsigned event;
+	void *oob_data;
+	size_t oob_data_len;
+	void *priv;
+	struct work_struct work;
+};
+static void qmi_notify_event_worker(struct work_struct *work);
+
+#define HANDLE_HASH_TBL_SZ 1
+static DEFINE_HASHTABLE(handle_hash_tbl, HANDLE_HASH_TBL_SZ);
+static DEFINE_MUTEX(handle_hash_tbl_lock);
+
+struct elem_info qmi_response_type_v01_ei[] = {
+	{
+		.data_type	= QMI_SIGNED_2_BYTE_ENUM,
+		.elem_len	= 1,
+		.elem_size	= sizeof(uint16_t),
+		.is_array	= NO_ARRAY,
+		.tlv_type	= QMI_COMMON_TLV_TYPE,
+		.offset		= offsetof(struct qmi_response_type_v01,
+					   result),
+		.ei_array	= NULL,
+	},
+	{
+		.data_type      = QMI_SIGNED_2_BYTE_ENUM,
+		.elem_len       = 1,
+		.elem_size      = sizeof(uint16_t),
+		.is_array       = NO_ARRAY,
+		.tlv_type       = QMI_COMMON_TLV_TYPE,
+		.offset         = offsetof(struct qmi_response_type_v01,
+					   error),
+		.ei_array       = NULL,
+	},
+	{
+		.data_type	= QMI_EOTI,
+		.elem_len	= 0,
+		.elem_size	= 0,
+		.is_array	= NO_ARRAY,
+		.tlv_type	= QMI_COMMON_TLV_TYPE,
+		.offset		= 0,
+		.ei_array	= NULL,
+	},
+};
+
+struct elem_info qmi_error_resp_type_v01_ei[] = {
+	{
+		.data_type = QMI_STRUCT,
+		.elem_len  = 1,
+		.elem_size = sizeof(struct qmi_response_type_v01),
+		.is_array  = NO_ARRAY,
+		.tlv_type  = 0x02,
+		.offset    = 0,
+		.ei_array  = qmi_response_type_v01_ei,
+	},
+	{
+		.data_type = QMI_EOTI,
+		.elem_len  = 0,
+		.elem_size = 0,
+		.is_array  = NO_ARRAY,
+		.tlv_type  = 0x00,
+		.offset    = 0,
+		.ei_array  = NULL,
+	},
+};
+
+struct msg_desc err_resp_desc = {
+	.max_msg_len = 7,
+	.msg_id = 0,
+	.ei_array = qmi_error_resp_type_v01_ei,
+};
+
+static DEFINE_MUTEX(qmi_svc_event_notifier_lock);
+static struct msm_ipc_port *qmi_svc_event_notifier_port;
+static struct workqueue_struct *qmi_svc_event_notifier_wq;
+static void qmi_svc_event_notifier_init(void);
+static void qmi_svc_event_worker(struct work_struct *work);
+static struct svc_event_nb *find_svc_event_nb(uint32_t service_id,
+					      uint32_t instance_id);
+DECLARE_WORK(qmi_svc_event_work, qmi_svc_event_worker);
+static void svc_resume_tx_worker(struct work_struct *work);
+static void clean_txn_info(struct qmi_handle *handle);
+static void *qmi_req_resp_log_ctx;
+static void *qmi_ind_log_ctx;
 
 /**
- * qmi_recv_new_server() - handler of NEW_SERVER control message
- * @qmi:	qmi handle
- * @service:	service id of the new server
- * @instance:	instance id of the new server
- * @node:	node of the new server
- * @port:	port of the new server
+ * qmi_log() - Pass log data to IPC logging framework
+ * @handle:	The pointer to the qmi_handle
+ * @cntl_flg:	Indicates the type(request/response/indications) of the message
+ * @txn_id:	Transaction ID of the message.
+ * @msg_id:	Message ID of the incoming/outgoing message.
+ * @msg_len:	Total size of the message.
  *
- * Calls the new_server callback to inform the client about a newly registered
- * server matching the currently registered service lookup.
+ * This function builds the data the would be passed on to the IPC logging
+ * framework. The data that would be passed corresponds to the information
+ * that is exchanged between the IPC Router and kernel modules during
+ * request/response/indication transactions.
  */
-static void qmi_recv_new_server(struct qmi_handle *qmi,
-				unsigned int service, unsigned int instance,
-				unsigned int node, unsigned int port)
+
+static void qmi_log(struct qmi_handle *handle,
+			unsigned char cntl_flag, uint16_t txn_id,
+			uint16_t msg_id, uint16_t msg_len)
 {
-	struct qmi_ops *ops = &qmi->ops;
-	struct qmi_service *svc;
-	int ret;
+	uint32_t service_id = 0;
+	const char *ops_type = NULL;
+
+	if (handle->handle_type == QMI_CLIENT_HANDLE) {
+		service_id = handle->dest_service_id;
+		if (cntl_flag == QMI_REQUEST_CONTROL_FLAG)
+			ops_type = "TX";
+		else if (cntl_flag == QMI_INDICATION_CONTROL_FLAG ||
+			cntl_flag == QMI_RESPONSE_CONTROL_FLAG)
+			ops_type = "RX";
+	} else if (handle->handle_type == QMI_SERVICE_HANDLE) {
+		service_id = handle->svc_ops_options->service_id;
+		if (cntl_flag == QMI_REQUEST_CONTROL_FLAG)
+			ops_type = "RX";
+		else if (cntl_flag == QMI_INDICATION_CONTROL_FLAG ||
+			cntl_flag == QMI_RESPONSE_CONTROL_FLAG)
+			ops_type = "TX";
+	}
 
-	if (!ops->new_server)
-		return;
+	/*
+	 * IPC Logging format is as below:-
+	 * <Type of module>(CLNT or  SERV)	:
+	 * <Opertaion Type> (Transmit/ RECV)	:
+	 * <Control Flag> (Req/Resp/Ind)	:
+	 * <Transaction ID>			:
+	 * <Message ID>				:
+	 * <Message Length>			:
+	 * <Service ID>				:
+	 */
+	if (qmi_req_resp_log_ctx &&
+		((cntl_flag == QMI_REQUEST_CONTROL_FLAG) ||
+		(cntl_flag == QMI_RESPONSE_CONTROL_FLAG))) {
+		QMI_REQ_RESP_LOG("%s %s CF:%x TI:%x MI:%x ML:%x SvcId: %x",
+		(handle->handle_type == QMI_CLIENT_HANDLE ? "QCCI" : "QCSI"),
+		ops_type, cntl_flag, txn_id, msg_id, msg_len, service_id);
+	} else if (qmi_ind_log_ctx &&
+		(cntl_flag == QMI_INDICATION_CONTROL_FLAG)) {
+		QMI_IND_LOG("%s %s CF:%x TI:%x MI:%x ML:%x SvcId: %x",
+		(handle->handle_type == QMI_CLIENT_HANDLE ? "QCCI" : "QCSI"),
+		ops_type, cntl_flag, txn_id, msg_id, msg_len, service_id);
+	}
+}
 
-	/* Ignore EOF marker */
-	if (!node && !port)
-		return;
+/**
+ * add_req_handle() - Create and Add a request handle to the connection
+ * @conn_h: Connection handle over which the request has arrived.
+ * @msg_id: Message ID of the request.
+ * @txn_id: Transaction ID of the request.
+ *
+ * @return: Pointer to request handle on success, NULL on error.
+ *
+ * This function creates a request handle to track the request that arrives
+ * on a connection. This function then adds it to the connection's request
+ * handle list.
+ */
+static struct req_handle *add_req_handle(struct qmi_svc_clnt_conn *conn_h,
+					 uint16_t msg_id, uint16_t txn_id)
+{
+	struct req_handle *req_h;
 
-	svc = kzalloc(sizeof(*svc), GFP_KERNEL);
-	if (!svc)
-		return;
+	req_h = kmalloc(sizeof(struct req_handle), GFP_KERNEL);
+	if (!req_h) {
+		pr_err("%s: Error allocating req_h\n", __func__);
+		return NULL;
+	}
 
-	svc->service = service;
-	svc->version = instance & 0xff;
-	svc->instance = instance >> 8;
-	svc->node = node;
-	svc->port = port;
+	req_h->conn_h = conn_h;
+	req_h->msg_id = msg_id;
+	req_h->txn_id = txn_id;
+	list_add_tail(&req_h->list, &conn_h->req_handle_list);
+	return req_h;
+}
 
-	ret = ops->new_server(qmi, svc);
-	if (ret < 0)
-		kfree(svc);
-	else
-		list_add(&svc->list_node, &qmi->lookup_results);
+/**
+ * verify_req_handle() - Verify the validity of a request handle
+ * @conn_h: Connection handle over which the request has arrived.
+ * @req_h: Request handle to be verified.
+ *
+ * @return: true on success, false on failure.
+ *
+ * This function is used to check if the request handle is present in
+ * the connection handle.
+ */
+static bool verify_req_handle(struct qmi_svc_clnt_conn *conn_h,
+			      struct req_handle *req_h)
+{
+	struct req_handle *temp_req_h;
+
+	list_for_each_entry(temp_req_h, &conn_h->req_handle_list, list) {
+		if (temp_req_h == req_h)
+			return true;
+	}
+	return false;
 }
 
 /**
- * qmi_recv_del_server() - handler of DEL_SERVER control message
- * @qmi:	qmi handle
- * @node:	node of the dying server, a value of -1 matches all nodes
- * @port:	port of the dying server, a value of -1 matches all ports
+ * rmv_req_handle() - Remove and destroy the request handle
+ * @req_h: Request handle to be removed and destroyed.
  *
- * Calls the del_server callback for each previously seen server, allowing the
- * client to react to the disappearing server.
+ * @return: 0.
  */
-static void qmi_recv_del_server(struct qmi_handle *qmi,
-				unsigned int node, unsigned int port)
+static int rmv_req_handle(struct req_handle *req_h)
 {
-	struct qmi_ops *ops = &qmi->ops;
-	struct qmi_service *svc;
-	struct qmi_service *tmp;
+	list_del(&req_h->list);
+	kfree(req_h);
+	return 0;
+}
 
-	list_for_each_entry_safe(svc, tmp, &qmi->lookup_results, list_node) {
-		if (node != -1 && svc->node != node)
-			continue;
-		if (port != -1 && svc->port != port)
-			continue;
+/**
+ * add_svc_clnt_conn() - Create and add a connection handle to a service
+ * @handle: QMI handle in which the service is hosted.
+ * @clnt_addr: Address of the client connecting with the service.
+ * @clnt_addr_len: Length of the client address.
+ *
+ * @return: Pointer to connection handle on success, NULL on error.
+ *
+ * This function is used to create a connection handle that binds the service
+ * with a client. This function is called on a service's QMI handle when a
+ * client sends its first message to the service.
+ *
+ * This function must be called with handle->handle_lock locked.
+ */
+static struct qmi_svc_clnt_conn *add_svc_clnt_conn(
+	struct qmi_handle *handle, void *clnt_addr, size_t clnt_addr_len)
+{
+	struct qmi_svc_clnt_conn *conn_h;
 
-		if (ops->del_server)
-			ops->del_server(qmi, svc);
+	conn_h = kmalloc(sizeof(struct qmi_svc_clnt_conn), GFP_KERNEL);
+	if (!conn_h) {
+		pr_err("%s: Error allocating conn_h\n", __func__);
+		return NULL;
+	}
 
-		list_del(&svc->list_node);
-		kfree(svc);
+	conn_h->clnt_addr = kmalloc(clnt_addr_len, GFP_KERNEL);
+	if (!conn_h->clnt_addr) {
+		pr_err("%s: Error allocating clnt_addr\n", __func__);
+		return NULL;
 	}
+
+	INIT_LIST_HEAD(&conn_h->list);
+	conn_h->svc_handle = handle;
+	memcpy(conn_h->clnt_addr, clnt_addr, clnt_addr_len);
+	conn_h->clnt_addr_len = clnt_addr_len;
+	INIT_LIST_HEAD(&conn_h->req_handle_list);
+	INIT_DELAYED_WORK(&conn_h->resume_tx_work, svc_resume_tx_worker);
+	INIT_LIST_HEAD(&conn_h->pending_txn_list);
+	mutex_init(&conn_h->pending_txn_lock);
+	list_add_tail(&conn_h->list, &handle->conn_list);
+	return conn_h;
 }
 
 /**
- * qmi_recv_bye() - handler of BYE control message
- * @qmi:	qmi handle
- * @node:	id of the dying node
+ * find_svc_clnt_conn() - Find the existence of a client<->service connection
+ * @handle: Service's QMI handle.
+ * @clnt_addr: Address of the client to be present in the connection.
+ * @clnt_addr_len: Length of the client address.
+ *
+ * @return: Pointer to connection handle if the matching connection is found,
+ *          NULL if the connection is not found.
+ *
+ * This function is used to find the existence of a client<->service connection
+ * handle in a service's QMI handle. This function tries to match the client
+ * address in the existing connections.
  *
- * Signals the client that all previously registered services on this node are
- * now gone and then calls the bye callback to allow the client client further
- * cleaning up resources associated with this remote.
+ * This function must be called with handle->handle_lock locked.
  */
-static void qmi_recv_bye(struct qmi_handle *qmi,
-			 unsigned int node)
+static struct qmi_svc_clnt_conn *find_svc_clnt_conn(
+	struct qmi_handle *handle, void *clnt_addr, size_t clnt_addr_len)
 {
-	struct qmi_ops *ops = &qmi->ops;
+	struct qmi_svc_clnt_conn *conn_h;
+
+	list_for_each_entry(conn_h, &handle->conn_list, list) {
+		if (!memcmp(conn_h->clnt_addr, clnt_addr, clnt_addr_len))
+			return conn_h;
+	}
+	return NULL;
+}
 
-	qmi_recv_del_server(qmi, node, -1);
+/**
+ * verify_svc_clnt_conn() - Verify the existence of a connection handle
+ * @handle: Service's QMI handle.
+ * @conn_h: Connection handle to be verified.
+ *
+ * @return: true on success, false on failure.
+ *
+ * This function is used to verify the existence of a connection in the
+ * connection list maintained by the service.
+ *
+ * This function must be called with handle->handle_lock locked.
+ */
+static bool verify_svc_clnt_conn(struct qmi_handle *handle,
+				 struct qmi_svc_clnt_conn *conn_h)
+{
+	struct qmi_svc_clnt_conn *temp_conn_h;
 
-	if (ops->bye)
-		ops->bye(qmi, node);
+	list_for_each_entry(temp_conn_h, &handle->conn_list, list) {
+		if (temp_conn_h == conn_h)
+			return true;
+	}
+	return false;
 }
 
 /**
- * qmi_recv_del_client() - handler of DEL_CLIENT control message
- * @qmi:	qmi handle
- * @node:	node of the dying client
- * @port:	port of the dying client
+ * rmv_svc_clnt_conn() - Remove the connection handle info from the service
+ * @conn_h: Connection handle to be removed.
+ *
+ * This function removes a connection handle from a service's QMI handle.
  *
- * Signals the client about a dying client, by calling the del_client callback.
+ * This function must be called with handle->handle_lock locked.
  */
-static void qmi_recv_del_client(struct qmi_handle *qmi,
-				unsigned int node, unsigned int port)
+static void rmv_svc_clnt_conn(struct qmi_svc_clnt_conn *conn_h)
 {
-	struct qmi_ops *ops = &qmi->ops;
+	struct req_handle *req_h, *temp_req_h;
+	struct qmi_txn *txn_h, *temp_txn_h;
+
+	list_del(&conn_h->list);
+	list_for_each_entry_safe(req_h, temp_req_h,
+				 &conn_h->req_handle_list, list)
+		rmv_req_handle(req_h);
+
+	mutex_lock(&conn_h->pending_txn_lock);
+	list_for_each_entry_safe(txn_h, temp_txn_h,
+				 &conn_h->pending_txn_list, list) {
+		list_del(&txn_h->list);
+		kfree(txn_h->enc_data);
+		kfree(txn_h);
+	}
+	mutex_unlock(&conn_h->pending_txn_lock);
+	flush_delayed_work(&conn_h->resume_tx_work);
+	kfree(conn_h->clnt_addr);
+	kfree(conn_h);
+}
 
-	if (ops->del_client)
-		ops->del_client(qmi, node, port);
+/**
+ * qmi_event_notify() - Notification function to QMI client/service interface
+ * @event: Type of event that gets notified.
+ * @oob_data: Any out-of-band data associated with event.
+ * @oob_data_len: Length of the out-of-band data, if any.
+ * @priv: Private data.
+ *
+ * This function is called by the underlying transport to notify the QMI
+ * interface regarding any incoming event. This function is registered by
+ * QMI interface when it opens a port/handle with the underlying transport.
+ */
+static void qmi_event_notify(unsigned event, void *oob_data,
+			     size_t oob_data_len, void *priv)
+{
+	struct qmi_notify_event_work *notify_work;
+	struct qmi_handle *handle;
+	uint32_t key = 0;
+
+	notify_work = kmalloc(sizeof(struct qmi_notify_event_work),
+			      GFP_KERNEL);
+	if (!notify_work) {
+		pr_err("%s: Couldn't notify %d event to %p\n",
+			__func__, event, priv);
+		return;
+	}
+	notify_work->event = event;
+	if (oob_data) {
+		notify_work->oob_data = kmalloc(oob_data_len, GFP_KERNEL);
+		if (!notify_work->oob_data) {
+			pr_err("%s: Couldn't allocate oob_data @ %d to %p\n",
+				__func__, event, priv);
+			kfree(notify_work);
+			return;
+		}
+		memcpy(notify_work->oob_data, oob_data, oob_data_len);
+	} else {
+		notify_work->oob_data = NULL;
+	}
+	notify_work->oob_data_len = oob_data_len;
+	notify_work->priv = priv;
+	INIT_WORK(&notify_work->work, qmi_notify_event_worker);
+
+	mutex_lock(&handle_hash_tbl_lock);
+	hash_for_each_possible(handle_hash_tbl, handle, handle_hash, key) {
+		if (handle == (struct qmi_handle *)priv) {
+			queue_work(handle->handle_wq,
+				   &notify_work->work);
+			mutex_unlock(&handle_hash_tbl_lock);
+			return;
+		}
+	}
+	mutex_unlock(&handle_hash_tbl_lock);
+	kfree(notify_work->oob_data);
+	kfree(notify_work);
 }
 
-static void qmi_recv_ctrl_pkt(struct qmi_handle *qmi,
-			      const void *buf, size_t len)
+static void qmi_notify_event_worker(struct work_struct *work)
 {
-	const struct qrtr_ctrl_pkt *pkt = buf;
+	struct qmi_notify_event_work *notify_work =
+		container_of(work, struct qmi_notify_event_work, work);
+	struct qmi_handle *handle = (struct qmi_handle *)notify_work->priv;
+	unsigned long flags;
+
+	if (!handle)
+		return;
 
-	if (len < sizeof(struct qrtr_ctrl_pkt)) {
-		pr_debug("ignoring short control packet\n");
+	mutex_lock(&handle->handle_lock);
+	if (handle->handle_reset) {
+		mutex_unlock(&handle->handle_lock);
+		kfree(notify_work->oob_data);
+		kfree(notify_work);
 		return;
 	}
 
-	switch (le32_to_cpu(pkt->cmd)) {
-	case QRTR_TYPE_BYE:
-		qmi_recv_bye(qmi, le32_to_cpu(pkt->client.node));
+	switch (notify_work->event) {
+	case IPC_ROUTER_CTRL_CMD_DATA:
+		spin_lock_irqsave(&handle->notify_lock, flags);
+		handle->notify(handle, QMI_RECV_MSG, handle->notify_priv);
+		spin_unlock_irqrestore(&handle->notify_lock, flags);
 		break;
-	case QRTR_TYPE_NEW_SERVER:
-		qmi_recv_new_server(qmi,
-				    le32_to_cpu(pkt->server.service),
-				    le32_to_cpu(pkt->server.instance),
-				    le32_to_cpu(pkt->server.node),
-				    le32_to_cpu(pkt->server.port));
+
+	case IPC_ROUTER_CTRL_CMD_RESUME_TX:
+		if (handle->handle_type == QMI_CLIENT_HANDLE) {
+			queue_delayed_work(handle->handle_wq,
+					   &handle->resume_tx_work,
+					   msecs_to_jiffies(0));
+		} else if (handle->handle_type == QMI_SERVICE_HANDLE) {
+			struct msm_ipc_addr rtx_addr = {0};
+			struct qmi_svc_clnt_conn *conn_h;
+			union rr_control_msg *msg;
+
+			msg = (union rr_control_msg *)notify_work->oob_data;
+			rtx_addr.addrtype = MSM_IPC_ADDR_ID;
+			rtx_addr.addr.port_addr.node_id = msg->cli.node_id;
+			rtx_addr.addr.port_addr.port_id = msg->cli.port_id;
+			conn_h = find_svc_clnt_conn(handle, &rtx_addr,
+						    sizeof(rtx_addr));
+			if (conn_h)
+				queue_delayed_work(handle->handle_wq,
+						   &conn_h->resume_tx_work,
+						   msecs_to_jiffies(0));
+		}
 		break;
-	case QRTR_TYPE_DEL_SERVER:
-		qmi_recv_del_server(qmi,
-				    le32_to_cpu(pkt->server.node),
-				    le32_to_cpu(pkt->server.port));
+
+	case IPC_ROUTER_CTRL_CMD_NEW_SERVER:
+	case IPC_ROUTER_CTRL_CMD_REMOVE_SERVER:
+	case IPC_ROUTER_CTRL_CMD_REMOVE_CLIENT:
+		queue_delayed_work(handle->handle_wq,
+				   &handle->ctl_work, msecs_to_jiffies(0));
 		break;
-	case QRTR_TYPE_DEL_CLIENT:
-		qmi_recv_del_client(qmi,
-				    le32_to_cpu(pkt->client.node),
-				    le32_to_cpu(pkt->client.port));
+	default:
 		break;
 	}
+	mutex_unlock(&handle->handle_lock);
+	kfree(notify_work->oob_data);
+	kfree(notify_work);
 }
 
-static void qmi_send_new_lookup(struct qmi_handle *qmi, struct qmi_service *svc)
+/**
+ * clnt_resume_tx_worker() - Handle the Resume_Tx event
+ * @work : Pointer to the work strcuture.
+ *
+ * This function handles the resume_tx event for any QMI client that
+ * exists in the kernel space. This function parses the pending_txn_list of
+ * the handle and attempts a send for each transaction in that list.
+ */
+static void clnt_resume_tx_worker(struct work_struct *work)
 {
-	struct qrtr_ctrl_pkt pkt;
-	struct sockaddr_qrtr sq;
-	struct msghdr msg = { };
-	struct kvec iv = { &pkt, sizeof(pkt) };
+	struct delayed_work *rtx_work = to_delayed_work(work);
+	struct qmi_handle *handle =
+		container_of(rtx_work, struct qmi_handle, resume_tx_work);
+	struct qmi_txn *pend_txn, *temp_txn;
 	int ret;
+	uint16_t msg_id;
 
-	memset(&pkt, 0, sizeof(pkt));
-	pkt.cmd = cpu_to_le32(QRTR_TYPE_NEW_LOOKUP);
-	pkt.server.service = cpu_to_le32(svc->service);
-	pkt.server.instance = cpu_to_le32(svc->version | svc->instance << 8);
+	mutex_lock(&handle->handle_lock);
+	if (handle->handle_reset)
+		goto out_clnt_handle_rtx;
 
-	sq.sq_family = qmi->sq.sq_family;
-	sq.sq_node = qmi->sq.sq_node;
-	sq.sq_port = QRTR_PORT_CTRL;
+	list_for_each_entry_safe(pend_txn, temp_txn,
+				&handle->pending_txn_list, list) {
+		ret = msm_ipc_router_send_msg(
+				(struct msm_ipc_port *)handle->src_port,
+				(struct msm_ipc_addr *)handle->dest_info,
+				pend_txn->enc_data, pend_txn->enc_data_len);
 
-	msg.msg_name = &sq;
-	msg.msg_namelen = sizeof(sq);
-
-	mutex_lock(&qmi->sock_lock);
-	if (qmi->sock) {
-		ret = kernel_sendmsg(qmi->sock, &msg, &iv, 1, sizeof(pkt));
-		if (ret < 0)
-			pr_err("failed to send lookup registration: %d\n", ret);
+		if (ret == -EAGAIN)
+			break;
+		msg_id = ((struct qmi_header *)pend_txn->enc_data)->msg_id;
+		kfree(pend_txn->enc_data);
+		if (ret < 0) {
+			pr_err("%s: Sending transaction %d from port %d failed",
+				__func__, pend_txn->txn_id,
+				((struct msm_ipc_port *)handle->src_port)->
+							this_port.port_id);
+			if (pend_txn->type == QMI_ASYNC_TXN) {
+				pend_txn->resp_cb(pend_txn->handle,
+						msg_id, pend_txn->resp,
+						pend_txn->resp_cb_data,
+						ret);
+				list_del(&pend_txn->list);
+				kfree(pend_txn);
+			} else if (pend_txn->type == QMI_SYNC_TXN) {
+				pend_txn->send_stat = ret;
+				wake_up(&pend_txn->wait_q);
+			}
+		} else {
+			list_del(&pend_txn->list);
+			list_add_tail(&pend_txn->list, &handle->txn_list);
+		}
 	}
-	mutex_unlock(&qmi->sock_lock);
+out_clnt_handle_rtx:
+	mutex_unlock(&handle->handle_lock);
 }
 
 /**
- * qmi_add_lookup() - register a new lookup with the name service
- * @qmi:	qmi handle
- * @service:	service id of the request
- * @instance:	instance id of the request
- * @version:	version number of the request
+ * svc_resume_tx_worker() - Handle the Resume_Tx event
+ * @work : Pointer to the work strcuture.
  *
- * Registering a lookup query with the name server will cause the name server
- * to send NEW_SERVER and DEL_SERVER control messages to this socket as
- * matching services are registered.
- *
- * Return: 0 on success, negative errno on failure.
+ * This function handles the resume_tx event for any QMI service that
+ * exists in the kernel space. This function parses the pending_txn_list of
+ * the connection handle and attempts a send for each transaction in that list.
  */
-int qmi_add_lookup(struct qmi_handle *qmi, unsigned int service,
-		   unsigned int version, unsigned int instance)
+static void svc_resume_tx_worker(struct work_struct *work)
 {
-	struct qmi_service *svc;
+	struct delayed_work *rtx_work = to_delayed_work(work);
+	struct qmi_svc_clnt_conn *conn_h =
+		container_of(rtx_work, struct qmi_svc_clnt_conn,
+			     resume_tx_work);
+	struct qmi_handle *handle = (struct qmi_handle *)conn_h->svc_handle;
+	struct qmi_txn *pend_txn, *temp_txn;
+	int ret;
 
-	svc = kzalloc(sizeof(*svc), GFP_KERNEL);
-	if (!svc)
-		return -ENOMEM;
+	mutex_lock(&conn_h->pending_txn_lock);
+	if (handle->handle_reset)
+		goto out_svc_handle_rtx;
 
-	svc->service = service;
-	svc->version = version;
-	svc->instance = instance;
+	list_for_each_entry_safe(pend_txn, temp_txn,
+				&conn_h->pending_txn_list, list) {
+		ret = msm_ipc_router_send_msg(
+				(struct msm_ipc_port *)handle->src_port,
+				(struct msm_ipc_addr *)conn_h->clnt_addr,
+				pend_txn->enc_data, pend_txn->enc_data_len);
 
-	list_add(&svc->list_node, &qmi->lookups);
+		if (ret == -EAGAIN)
+			break;
+		if (ret < 0)
+			pr_err("%s: Sending transaction %d from port %d failed",
+				__func__, pend_txn->txn_id,
+				((struct msm_ipc_port *)handle->src_port)->
+							this_port.port_id);
+		list_del(&pend_txn->list);
+		kfree(pend_txn->enc_data);
+		kfree(pend_txn);
+	}
+out_svc_handle_rtx:
+	mutex_unlock(&conn_h->pending_txn_lock);
+}
 
-	qmi_send_new_lookup(qmi, svc);
+/**
+ * handle_rmv_server() - Handle the server exit event
+ * @handle: Client handle on which the server exit event is received.
+ * @ctl_msg: Information about the server that is exiting.
+ *
+ * @return: 0 on success, standard Linux error codes on failure.
+ *
+ * This function must be called with handle->handle_lock locked.
+ */
+static int handle_rmv_server(struct qmi_handle *handle,
+			     union rr_control_msg *ctl_msg)
+{
+	struct msm_ipc_addr *svc_addr;
+	unsigned long flags;
+
+	if (unlikely(!handle->dest_info))
+		return 0;
+
+	svc_addr = (struct msm_ipc_addr *)(handle->dest_info);
+	if (svc_addr->addr.port_addr.node_id == ctl_msg->srv.node_id &&
+	    svc_addr->addr.port_addr.port_id == ctl_msg->srv.port_id) {
+		/* Wakeup any threads waiting for the response */
+		handle->handle_reset = 1;
+		clean_txn_info(handle);
+
+		spin_lock_irqsave(&handle->notify_lock, flags);
+		handle->notify(handle, QMI_SERVER_EXIT, handle->notify_priv);
+		spin_unlock_irqrestore(&handle->notify_lock, flags);
+	}
+	return 0;
+}
 
+/**
+ * handle_rmv_client() - Handle the client exit event
+ * @handle: Service handle on which the client exit event is received.
+ * @ctl_msg: Information about the client that is exiting.
+ *
+ * @return: 0 on success, standard Linux error codes on failure.
+ *
+ * This function must be called with handle->handle_lock locked.
+ */
+static int handle_rmv_client(struct qmi_handle *handle,
+			     union rr_control_msg *ctl_msg)
+{
+	struct qmi_svc_clnt_conn *conn_h;
+	struct msm_ipc_addr clnt_addr = {0};
+	unsigned long flags;
+
+	clnt_addr.addrtype = MSM_IPC_ADDR_ID;
+	clnt_addr.addr.port_addr.node_id = ctl_msg->cli.node_id;
+	clnt_addr.addr.port_addr.port_id = ctl_msg->cli.port_id;
+	conn_h = find_svc_clnt_conn(handle, &clnt_addr, sizeof(clnt_addr));
+	if (conn_h) {
+		spin_lock_irqsave(&handle->notify_lock, flags);
+		handle->svc_ops_options->disconnect_cb(handle, conn_h);
+		spin_unlock_irqrestore(&handle->notify_lock, flags);
+		rmv_svc_clnt_conn(conn_h);
+	}
 	return 0;
 }
-EXPORT_SYMBOL(qmi_add_lookup);
 
-static void qmi_send_new_server(struct qmi_handle *qmi, struct qmi_service *svc)
+/**
+ * handle_ctl_msg: Worker function to handle the control events
+ * @work: Work item to map the QMI handle.
+ *
+ * This function is a worker function to handle the incoming control
+ * events like REMOVE_SERVER/REMOVE_CLIENT. The work item is unique
+ * to a handle and the workker function handles the control events on
+ * a specific handle.
+ */
+static void handle_ctl_msg(struct work_struct *work)
 {
-	struct qrtr_ctrl_pkt pkt;
-	struct sockaddr_qrtr sq;
-	struct msghdr msg = { };
-	struct kvec iv = { &pkt, sizeof(pkt) };
-	int ret;
+	struct delayed_work *ctl_work = to_delayed_work(work);
+	struct qmi_handle *handle =
+		container_of(ctl_work, struct qmi_handle, ctl_work);
+	unsigned int ctl_msg_len;
+	union rr_control_msg *ctl_msg = NULL;
+	struct msm_ipc_addr src_addr;
+	int rc;
+
+	mutex_lock(&handle->handle_lock);
+	while (1) {
+		if (handle->handle_reset)
+			break;
 
-	memset(&pkt, 0, sizeof(pkt));
-	pkt.cmd = cpu_to_le32(QRTR_TYPE_NEW_SERVER);
-	pkt.server.service = cpu_to_le32(svc->service);
-	pkt.server.instance = cpu_to_le32(svc->version | svc->instance << 8);
-	pkt.server.node = cpu_to_le32(qmi->sq.sq_node);
-	pkt.server.port = cpu_to_le32(qmi->sq.sq_port);
+		/* Read the messages */
+		rc = msm_ipc_router_read_msg(
+			(struct msm_ipc_port *)(handle->ctl_port),
+			&src_addr, (unsigned char **)&ctl_msg, &ctl_msg_len);
+		if (rc == -ENOMSG)
+			break;
+		if (rc < 0) {
+			pr_err("%s: Read failed %d\n", __func__, rc);
+			break;
+		}
+		if (ctl_msg->cmd == IPC_ROUTER_CTRL_CMD_REMOVE_SERVER &&
+		    handle->handle_type == QMI_CLIENT_HANDLE)
+			handle_rmv_server(handle, ctl_msg);
+		else if (ctl_msg->cmd == IPC_ROUTER_CTRL_CMD_REMOVE_CLIENT &&
+			 handle->handle_type == QMI_SERVICE_HANDLE)
+			handle_rmv_client(handle, ctl_msg);
+		kfree(ctl_msg);
+	}
+	mutex_unlock(&handle->handle_lock);
+	return;
+}
 
-	sq.sq_family = qmi->sq.sq_family;
-	sq.sq_node = qmi->sq.sq_node;
-	sq.sq_port = QRTR_PORT_CTRL;
+struct qmi_handle *qmi_handle_create(
+	void (*notify)(struct qmi_handle *handle,
+		       enum qmi_event_type event, void *notify_priv),
+	void *notify_priv)
+{
+	struct qmi_handle *temp_handle;
+	struct msm_ipc_port *port_ptr, *ctl_port_ptr;
+	static uint32_t handle_count;
+	char wq_name[MAX_WQ_NAME_LEN];
+
+	temp_handle = kzalloc(sizeof(struct qmi_handle), GFP_KERNEL);
+	if (!temp_handle) {
+		pr_err("%s: Failure allocating client handle\n", __func__);
+		return NULL;
+	}
+	mutex_lock(&handle_hash_tbl_lock);
+	handle_count++;
+	scnprintf(wq_name, MAX_WQ_NAME_LEN, "qmi_hndl%08x", handle_count);
+	hash_add(handle_hash_tbl, &temp_handle->handle_hash, 0);
+	temp_handle->handle_wq = create_singlethread_workqueue(wq_name);
+	mutex_unlock(&handle_hash_tbl_lock);
+	if (!temp_handle->handle_wq) {
+		pr_err("%s: Couldn't create workqueue for handle\n", __func__);
+		goto handle_create_err1;
+	}
 
-	msg.msg_name = &sq;
-	msg.msg_namelen = sizeof(sq);
+	/* Initialize common elements */
+	temp_handle->handle_type = QMI_CLIENT_HANDLE;
+	temp_handle->next_txn_id = 1;
+	mutex_init(&temp_handle->handle_lock);
+	spin_lock_init(&temp_handle->notify_lock);
+	temp_handle->notify = notify;
+	temp_handle->notify_priv = notify_priv;
+	init_waitqueue_head(&temp_handle->reset_waitq);
+	INIT_DELAYED_WORK(&temp_handle->resume_tx_work, clnt_resume_tx_worker);
+	INIT_DELAYED_WORK(&temp_handle->ctl_work, handle_ctl_msg);
+
+	/* Initialize client specific elements */
+	INIT_LIST_HEAD(&temp_handle->txn_list);
+	INIT_LIST_HEAD(&temp_handle->pending_txn_list);
+
+	/* Initialize service specific elements */
+	INIT_LIST_HEAD(&temp_handle->conn_list);
+
+	port_ptr = msm_ipc_router_create_port(qmi_event_notify,
+					      (void *)temp_handle);
+	if (!port_ptr) {
+		pr_err("%s: IPC router port creation failed\n", __func__);
+		goto handle_create_err2;
+	}
 
-	mutex_lock(&qmi->sock_lock);
-	if (qmi->sock) {
-		ret = kernel_sendmsg(qmi->sock, &msg, &iv, 1, sizeof(pkt));
-		if (ret < 0)
-			pr_err("send service registration failed: %d\n", ret);
+	ctl_port_ptr = msm_ipc_router_create_port(qmi_event_notify,
+						  (void *)temp_handle);
+	if (!ctl_port_ptr) {
+		pr_err("%s: IPC router ctl port creation failed\n", __func__);
+		goto handle_create_err3;
 	}
-	mutex_unlock(&qmi->sock_lock);
+	msm_ipc_router_bind_control_port(ctl_port_ptr);
+
+	temp_handle->src_port = port_ptr;
+	temp_handle->ctl_port = ctl_port_ptr;
+	return temp_handle;
+
+handle_create_err3:
+	msm_ipc_router_close_port(port_ptr);
+handle_create_err2:
+	destroy_workqueue(temp_handle->handle_wq);
+handle_create_err1:
+	mutex_lock(&handle_hash_tbl_lock);
+	hash_del(&temp_handle->handle_hash);
+	mutex_unlock(&handle_hash_tbl_lock);
+	kfree(temp_handle);
+	return NULL;
 }
+EXPORT_SYMBOL(qmi_handle_create);
 
-/**
- * qmi_add_server() - register a service with the name service
- * @qmi:	qmi handle
- * @service:	type of the service
- * @instance:	instance of the service
- * @version:	version of the service
- *
- * Register a new service with the name service. This allows clients to find
- * and start sending messages to the client associated with @qmi.
- *
- * Return: 0 on success, negative errno on failure.
- */
-int qmi_add_server(struct qmi_handle *qmi, unsigned int service,
-		   unsigned int version, unsigned int instance)
+static void clean_txn_info(struct qmi_handle *handle)
+{
+	struct qmi_txn *txn_handle, *temp_txn_handle, *pend_txn;
+
+	list_for_each_entry_safe(pend_txn, temp_txn_handle,
+				&handle->pending_txn_list, list) {
+		if (pend_txn->type == QMI_ASYNC_TXN) {
+			list_del(&pend_txn->list);
+			pend_txn->resp_cb(pend_txn->handle,
+					((struct qmi_header *)
+					pend_txn->enc_data)->msg_id,
+					pend_txn->resp, pend_txn->resp_cb_data,
+					-ENETRESET);
+			kfree(pend_txn->enc_data);
+			kfree(pend_txn);
+		} else if (pend_txn->type == QMI_SYNC_TXN) {
+			kfree(pend_txn->enc_data);
+			wake_up(&pend_txn->wait_q);
+		}
+	}
+	list_for_each_entry_safe(txn_handle, temp_txn_handle,
+				 &handle->txn_list, list) {
+		if (txn_handle->type == QMI_ASYNC_TXN) {
+			list_del(&txn_handle->list);
+			kfree(txn_handle);
+		} else if (txn_handle->type == QMI_SYNC_TXN) {
+			wake_up(&txn_handle->wait_q);
+		}
+	}
+}
+
+int qmi_handle_destroy(struct qmi_handle *handle)
+{
+	DEFINE_WAIT(wait);
+
+	if (!handle)
+		return -EINVAL;
+
+	mutex_lock(&handle_hash_tbl_lock);
+	hash_del(&handle->handle_hash);
+	mutex_unlock(&handle_hash_tbl_lock);
+
+	mutex_lock(&handle->handle_lock);
+	handle->handle_reset = 1;
+	clean_txn_info(handle);
+	msm_ipc_router_close_port((struct msm_ipc_port *)(handle->ctl_port));
+	msm_ipc_router_close_port((struct msm_ipc_port *)(handle->src_port));
+	mutex_unlock(&handle->handle_lock);
+	flush_workqueue(handle->handle_wq);
+	destroy_workqueue(handle->handle_wq);
+
+	mutex_lock(&handle->handle_lock);
+	while (!list_empty(&handle->txn_list) ||
+		    !list_empty(&handle->pending_txn_list)) {
+		prepare_to_wait(&handle->reset_waitq, &wait,
+				TASK_UNINTERRUPTIBLE);
+		mutex_unlock(&handle->handle_lock);
+		schedule();
+		mutex_lock(&handle->handle_lock);
+		finish_wait(&handle->reset_waitq, &wait);
+	}
+	mutex_unlock(&handle->handle_lock);
+	kfree(handle->dest_info);
+	kfree(handle);
+	return 0;
+}
+EXPORT_SYMBOL(qmi_handle_destroy);
+
+int qmi_register_ind_cb(struct qmi_handle *handle,
+	void (*ind_cb)(struct qmi_handle *handle,
+		       unsigned int msg_id, void *msg,
+		       unsigned int msg_len, void *ind_cb_priv),
+	void *ind_cb_priv)
+{
+	if (!handle)
+		return -EINVAL;
+
+	mutex_lock(&handle->handle_lock);
+	if (handle->handle_reset) {
+		mutex_unlock(&handle->handle_lock);
+		return -ENETRESET;
+	}
+
+	handle->ind_cb = ind_cb;
+	handle->ind_cb_priv = ind_cb_priv;
+	mutex_unlock(&handle->handle_lock);
+	return 0;
+}
+EXPORT_SYMBOL(qmi_register_ind_cb);
+
+static int qmi_encode_and_send_req(struct qmi_txn **ret_txn_handle,
+	struct qmi_handle *handle, enum txn_type type,
+	struct msg_desc *req_desc, void *req, unsigned int req_len,
+	struct msg_desc *resp_desc, void *resp, unsigned int resp_len,
+	void (*resp_cb)(struct qmi_handle *handle,
+			unsigned int msg_id, void *msg,
+			void *resp_cb_data, int stat),
+	void *resp_cb_data)
 {
-	struct qmi_service *svc;
+	struct qmi_txn *txn_handle;
+	int rc, encoded_req_len;
+	void *encoded_req;
+
+	if (!handle || !handle->dest_info ||
+	    !req_desc || !resp_desc || !resp)
+		return -EINVAL;
 
-	svc = kzalloc(sizeof(*svc), GFP_KERNEL);
-	if (!svc)
+	if ((!req && req_len) || (!req_len && req))
+		return -EINVAL;
+
+	mutex_lock(&handle->handle_lock);
+	if (handle->handle_reset) {
+		mutex_unlock(&handle->handle_lock);
+		return -ENETRESET;
+	}
+
+	/* Allocate Transaction Info */
+	txn_handle = kzalloc(sizeof(struct qmi_txn), GFP_KERNEL);
+	if (!txn_handle) {
+		pr_err("%s: Failed to allocate txn handle\n", __func__);
+		mutex_unlock(&handle->handle_lock);
 		return -ENOMEM;
+	}
+	txn_handle->type = type;
+	INIT_LIST_HEAD(&txn_handle->list);
+	init_waitqueue_head(&txn_handle->wait_q);
+
+	/* Cache the parameters passed & mark it as sync*/
+	txn_handle->handle = handle;
+	txn_handle->resp_desc = resp_desc;
+	txn_handle->resp = resp;
+	txn_handle->resp_len = resp_len;
+	txn_handle->resp_received = 0;
+	txn_handle->resp_cb = resp_cb;
+	txn_handle->resp_cb_data = resp_cb_data;
+	txn_handle->enc_data = NULL;
+	txn_handle->enc_data_len = 0;
+
+	/* Encode the request msg */
+	encoded_req_len = req_desc->max_msg_len + QMI_HEADER_SIZE;
+	encoded_req = kmalloc(encoded_req_len, GFP_KERNEL);
+	if (!encoded_req) {
+		pr_err("%s: Failed to allocate req_msg_buf\n", __func__);
+		rc = -ENOMEM;
+		goto encode_and_send_req_err1;
+	}
+	rc = qmi_kernel_encode(req_desc,
+		(void *)(encoded_req + QMI_HEADER_SIZE),
+		req_desc->max_msg_len, req);
+	if (rc < 0) {
+		pr_err("%s: Encode Failure %d\n", __func__, rc);
+		goto encode_and_send_req_err2;
+	}
+	encoded_req_len = rc;
 
-	svc->service = service;
-	svc->version = version;
-	svc->instance = instance;
+	/* Encode the header & Add to the txn_list */
+	if (!handle->next_txn_id)
+		handle->next_txn_id++;
+	txn_handle->txn_id = handle->next_txn_id++;
+	encode_qmi_header(encoded_req, QMI_REQUEST_CONTROL_FLAG,
+			  txn_handle->txn_id, req_desc->msg_id,
+			  encoded_req_len);
+	encoded_req_len += QMI_HEADER_SIZE;
 
-	list_add(&svc->list_node, &qmi->services);
+	/*
+	 * Check if this port has transactions queued to its pending list
+	 * and if there are any pending transactions then add the current
+	 * transaction to the pending list rather than sending it. This avoids
+	 * out-of-order message transfers.
+	 */
+	if (!list_empty(&handle->pending_txn_list)) {
+		rc = -EAGAIN;
+		goto append_pend_txn;
+	}
 
-	qmi_send_new_server(qmi, svc);
+	list_add_tail(&txn_handle->list, &handle->txn_list);
+	qmi_log(handle, QMI_REQUEST_CONTROL_FLAG, txn_handle->txn_id,
+			req_desc->msg_id, encoded_req_len);
+	/* Send the request */
+	rc = msm_ipc_router_send_msg((struct msm_ipc_port *)(handle->src_port),
+		(struct msm_ipc_addr *)handle->dest_info,
+		encoded_req, encoded_req_len);
+append_pend_txn:
+	if (rc == -EAGAIN) {
+		txn_handle->enc_data = encoded_req;
+		txn_handle->enc_data_len = encoded_req_len;
+		if (list_empty(&handle->pending_txn_list))
+			list_del(&txn_handle->list);
+		list_add_tail(&txn_handle->list, &handle->pending_txn_list);
+		if (ret_txn_handle)
+			*ret_txn_handle = txn_handle;
+		mutex_unlock(&handle->handle_lock);
+		return 0;
+	}
+	if (rc < 0) {
+		pr_err("%s: send_msg failed %d\n", __func__, rc);
+		goto encode_and_send_req_err3;
+	}
+	mutex_unlock(&handle->handle_lock);
 
+	kfree(encoded_req);
+	if (ret_txn_handle)
+		*ret_txn_handle = txn_handle;
 	return 0;
+
+encode_and_send_req_err3:
+	list_del(&txn_handle->list);
+encode_and_send_req_err2:
+	kfree(encoded_req);
+encode_and_send_req_err1:
+	kfree(txn_handle);
+	mutex_unlock(&handle->handle_lock);
+	return rc;
 }
-EXPORT_SYMBOL(qmi_add_server);
+
+int qmi_send_req_wait(struct qmi_handle *handle,
+		      struct msg_desc *req_desc,
+		      void *req, unsigned int req_len,
+		      struct msg_desc *resp_desc,
+		      void *resp, unsigned int resp_len,
+		      unsigned long timeout_ms)
+{
+	struct qmi_txn *txn_handle = NULL;
+	int rc;
+
+	/* Encode and send the request */
+	rc = qmi_encode_and_send_req(&txn_handle, handle, QMI_SYNC_TXN,
+				     req_desc, req, req_len,
+				     resp_desc, resp, resp_len,
+				     NULL, NULL);
+	if (rc < 0) {
+		pr_err("%s: Error encode & send req: %d\n", __func__, rc);
+		return rc;
+	}
+
+	/* Wait for the response */
+	if (!timeout_ms) {
+		wait_event(txn_handle->wait_q,
+			   (txn_handle->resp_received ||
+			    handle->handle_reset ||
+			   (txn_handle->send_stat < 0)));
+	} else {
+		rc = wait_event_timeout(txn_handle->wait_q,
+				(txn_handle->resp_received ||
+				handle->handle_reset ||
+				(txn_handle->send_stat < 0)),
+				msecs_to_jiffies(timeout_ms));
+		if (rc == 0)
+			rc = -ETIMEDOUT;
+	}
+
+	mutex_lock(&handle->handle_lock);
+	if (!txn_handle->resp_received) {
+		pr_err("%s: Response Wait Error %d\n", __func__, rc);
+		if (handle->handle_reset)
+			rc = -ENETRESET;
+		if (rc >= 0)
+			rc = -EFAULT;
+		if (txn_handle->send_stat < 0)
+			rc = txn_handle->send_stat;
+		goto send_req_wait_err;
+	}
+	rc = 0;
+
+send_req_wait_err:
+	list_del(&txn_handle->list);
+	kfree(txn_handle);
+	wake_up(&handle->reset_waitq);
+	mutex_unlock(&handle->handle_lock);
+	return rc;
+}
+EXPORT_SYMBOL(qmi_send_req_wait);
+
+int qmi_send_req_nowait(struct qmi_handle *handle,
+			struct msg_desc *req_desc,
+			void *req, unsigned int req_len,
+			struct msg_desc *resp_desc,
+			void *resp, unsigned int resp_len,
+			void (*resp_cb)(struct qmi_handle *handle,
+					unsigned int msg_id, void *msg,
+					void *resp_cb_data, int stat),
+			void *resp_cb_data)
+{
+	return qmi_encode_and_send_req(NULL, handle, QMI_ASYNC_TXN,
+				       req_desc, req, req_len,
+				       resp_desc, resp, resp_len,
+				       resp_cb, resp_cb_data);
+}
+EXPORT_SYMBOL(qmi_send_req_nowait);
 
 /**
- * qmi_txn_init() - allocate transaction id within the given QMI handle
- * @qmi:	QMI handle
- * @txn:	transaction context
- * @ei:		description of how to decode a matching response (optional)
- * @c_struct:	pointer to the object to decode the response into (optional)
+ * qmi_encode_and_send_resp() - Encode and send QMI response
+ * @handle: QMI service handle sending the response.
+ * @conn_h: Connection handle to which the response is sent.
+ * @req_h: Request handle for which the response is sent.
+ * @resp_desc: Message Descriptor describing the response structure.
+ * @resp: Response structure.
+ * @resp_len: Length of the response structure.
  *
- * This allocates a transaction id within the QMI handle. If @ei and @c_struct
- * are specified any responses to this transaction will be decoded as described
- * by @ei into @c_struct.
+ * @return: 0 on success, standard Linux error codes on failure.
  *
- * A client calling qmi_txn_init() must call either qmi_txn_wait() or
- * qmi_txn_cancel() to free up the allocated resources.
+ * This function encodes and sends a response message from a service to
+ * a client identified from the connection handle. The request for which
+ * the response is sent is identified from the connection handle.
  *
- * Return: Transaction id on success, negative errno on failure.
+ * This function must be called with handle->handle_lock locked.
  */
-int qmi_txn_init(struct qmi_handle *qmi, struct qmi_txn *txn,
-		 struct qmi_elem_info *ei, void *c_struct)
+static int qmi_encode_and_send_resp(struct qmi_handle *handle,
+	struct qmi_svc_clnt_conn *conn_h, struct req_handle *req_h,
+	struct msg_desc *resp_desc, void *resp, unsigned int resp_len)
 {
-	int ret;
+	struct qmi_txn *txn_handle;
+	uint16_t cntl_flag;
+	int rc;
+	int encoded_resp_len;
+	void *encoded_resp;
+
+	if (handle->handle_reset) {
+		rc = -ENETRESET;
+		goto encode_and_send_resp_err0;
+	}
 
-	memset(txn, 0, sizeof(*txn));
+	if (handle->handle_type != QMI_SERVICE_HANDLE ||
+	    !verify_svc_clnt_conn(handle, conn_h) ||
+	    (req_h && !verify_req_handle(conn_h, req_h))) {
+		rc = -EINVAL;
+		goto encode_and_send_resp_err0;
+	}
 
-	mutex_init(&txn->lock);
-	init_completion(&txn->completion);
-	txn->qmi = qmi;
-	txn->ei = ei;
-	txn->dest = c_struct;
+	/* Allocate Transaction Info */
+	txn_handle = kzalloc(sizeof(struct qmi_txn), GFP_KERNEL);
+	if (!txn_handle) {
+		pr_err("%s: Failed to allocate txn handle\n", __func__);
+		rc = -ENOMEM;
+		goto encode_and_send_resp_err0;
+	}
+	INIT_LIST_HEAD(&txn_handle->list);
+	init_waitqueue_head(&txn_handle->wait_q);
+	txn_handle->handle = handle;
+	txn_handle->enc_data = NULL;
+	txn_handle->enc_data_len = 0;
+
+	/* Encode the response msg */
+	encoded_resp_len = resp_desc->max_msg_len + QMI_HEADER_SIZE;
+	encoded_resp = kmalloc(encoded_resp_len, GFP_KERNEL);
+	if (!encoded_resp) {
+		pr_err("%s: Failed to allocate resp_msg_buf\n", __func__);
+		rc = -ENOMEM;
+		goto encode_and_send_resp_err1;
+	}
+	rc = qmi_kernel_encode(resp_desc,
+		(void *)(encoded_resp + QMI_HEADER_SIZE),
+		resp_desc->max_msg_len, resp);
+	if (rc < 0) {
+		pr_err("%s: Encode Failure %d\n", __func__, rc);
+		goto encode_and_send_resp_err2;
+	}
+	encoded_resp_len = rc;
 
-	mutex_lock(&qmi->txn_lock);
-	ret = idr_alloc_cyclic(&qmi->txns, txn, 0, INT_MAX, GFP_KERNEL);
-	if (ret < 0)
-		pr_err("failed to allocate transaction id\n");
+	/* Encode the header & Add to the txn_list */
+	if (req_h) {
+		txn_handle->txn_id = req_h->txn_id;
+		cntl_flag = QMI_RESPONSE_CONTROL_FLAG;
+	} else {
+		if (!handle->next_txn_id)
+			handle->next_txn_id++;
+		txn_handle->txn_id = handle->next_txn_id++;
+		cntl_flag = QMI_INDICATION_CONTROL_FLAG;
+	}
+	encode_qmi_header(encoded_resp, cntl_flag,
+			  txn_handle->txn_id, resp_desc->msg_id,
+			  encoded_resp_len);
+	encoded_resp_len += QMI_HEADER_SIZE;
 
-	txn->id = ret;
-	mutex_unlock(&qmi->txn_lock);
+	qmi_log(handle, cntl_flag, txn_handle->txn_id,
+			resp_desc->msg_id, encoded_resp_len);
+	/*
+	 * Check if this svc_clnt has transactions queued to its pending list
+	 * and if there are any pending transactions then add the current
+	 * transaction to the pending list rather than sending it. This avoids
+	 * out-of-order message transfers.
+	 */
+	mutex_lock(&conn_h->pending_txn_lock);
+	if (list_empty(&conn_h->pending_txn_list))
+		rc = msm_ipc_router_send_msg(
+			(struct msm_ipc_port *)(handle->src_port),
+			(struct msm_ipc_addr *)conn_h->clnt_addr,
+			encoded_resp, encoded_resp_len);
+	else
+		rc = -EAGAIN;
+
+	if (req_h)
+		rmv_req_handle(req_h);
+	if (rc == -EAGAIN) {
+		txn_handle->enc_data = encoded_resp;
+		txn_handle->enc_data_len = encoded_resp_len;
+		list_add_tail(&txn_handle->list, &conn_h->pending_txn_list);
+		mutex_unlock(&conn_h->pending_txn_lock);
+		return 0;
+	}
+	mutex_unlock(&conn_h->pending_txn_lock);
+	if (rc < 0)
+		pr_err("%s: send_msg failed %d\n", __func__, rc);
+encode_and_send_resp_err2:
+	kfree(encoded_resp);
+encode_and_send_resp_err1:
+	kfree(txn_handle);
+encode_and_send_resp_err0:
+	return rc;
+}
 
-	return ret;
+/**
+ * qmi_send_resp() - Send response to a request
+ * @handle: QMI handle from which the response is sent.
+ * @clnt: Client to which the response is sent.
+ * @req_handle: Request for which the response is sent.
+ * @resp_desc: Descriptor explaining the response structure.
+ * @resp: Pointer to the response structure.
+ * @resp_len: Length of the response structure.
+ *
+ * @return: 0 on success, < 0 on error.
+ */
+int qmi_send_resp(struct qmi_handle *handle, void *conn_handle,
+		  void *req_handle, struct msg_desc *resp_desc,
+		  void *resp, unsigned int resp_len)
+{
+	int rc;
+	struct qmi_svc_clnt_conn *conn_h;
+	struct req_handle *req_h;
+
+	if (!handle || !conn_handle || !req_handle ||
+	    !resp_desc || !resp || !resp_len)
+		return -EINVAL;
+
+	conn_h = (struct qmi_svc_clnt_conn *)conn_handle;
+	req_h = (struct req_handle *)req_handle;
+	mutex_lock(&handle->handle_lock);
+	rc = qmi_encode_and_send_resp(handle, conn_h, req_h,
+				      resp_desc, resp, resp_len);
+	if (rc < 0)
+		pr_err("%s: Error encoding and sending response\n", __func__);
+	mutex_unlock(&handle->handle_lock);
+	return rc;
 }
-EXPORT_SYMBOL(qmi_txn_init);
+EXPORT_SYMBOL(qmi_send_resp);
 
 /**
- * qmi_txn_wait() - wait for a response on a transaction
- * @txn:	transaction handle
- * @timeout:	timeout, in jiffies
+ * qmi_send_resp_from_cb() - Send response to a request from request_cb
+ * @handle: QMI handle from which the response is sent.
+ * @clnt: Client to which the response is sent.
+ * @req_handle: Request for which the response is sent.
+ * @resp_desc: Descriptor explaining the response structure.
+ * @resp: Pointer to the response structure.
+ * @resp_len: Length of the response structure.
  *
- * If the transaction is decoded by the means of @ei and @c_struct the return
- * value will be the returned value of qmi_decode_message(), otherwise it's up
- * to the specified message handler to fill out the result.
+ * @return: 0 on success, < 0 on error.
+ */
+int qmi_send_resp_from_cb(struct qmi_handle *handle, void *conn_handle,
+			  void *req_handle, struct msg_desc *resp_desc,
+			  void *resp, unsigned int resp_len)
+{
+	int rc;
+	struct qmi_svc_clnt_conn *conn_h;
+	struct req_handle *req_h;
+
+	if (!handle || !conn_handle || !req_handle ||
+	    !resp_desc || !resp || !resp_len)
+		return -EINVAL;
+
+	conn_h = (struct qmi_svc_clnt_conn *)conn_handle;
+	req_h = (struct req_handle *)req_handle;
+	rc = qmi_encode_and_send_resp(handle, conn_h, req_h,
+				      resp_desc, resp, resp_len);
+	if (rc < 0)
+		pr_err("%s: Error encoding and sending response\n", __func__);
+	return rc;
+}
+EXPORT_SYMBOL(qmi_send_resp_from_cb);
+
+/**
+ * qmi_send_ind() - Send unsolicited event/indication to a client
+ * @handle: QMI handle from which the indication is sent.
+ * @clnt: Client to which the indication is sent.
+ * @ind_desc: Descriptor explaining the indication structure.
+ * @ind: Pointer to the indication structure.
+ * @ind_len: Length of the indication structure.
  *
- * Return: the transaction response on success, negative errno on failure.
+ * @return: 0 on success, < 0 on error.
  */
-int qmi_txn_wait(struct qmi_txn *txn, unsigned long timeout)
+int qmi_send_ind(struct qmi_handle *handle, void *conn_handle,
+		 struct msg_desc *ind_desc, void *ind, unsigned int ind_len)
 {
-	struct qmi_handle *qmi = txn->qmi;
-	int ret;
+	int rc = 0;
+	struct qmi_svc_clnt_conn *conn_h;
+
+	if (!handle || !conn_handle || !ind_desc)
+		return -EINVAL;
+
+	if ((!ind && ind_len) || (ind && !ind_len))
+		return -EINVAL;
+
+	conn_h = (struct qmi_svc_clnt_conn *)conn_handle;
+	mutex_lock(&handle->handle_lock);
+	rc = qmi_encode_and_send_resp(handle, conn_h, NULL,
+				      ind_desc, ind, ind_len);
+	if (rc < 0)
+		pr_err("%s: Error encoding and sending ind.\n", __func__);
+	mutex_unlock(&handle->handle_lock);
+	return rc;
+}
+EXPORT_SYMBOL(qmi_send_ind);
 
-	ret = wait_for_completion_interruptible_timeout(&txn->completion,
-							timeout);
+/**
+ * qmi_send_ind_from_cb() - Send indication to a client from registration_cb
+ * @handle: QMI handle from which the indication is sent.
+ * @clnt: Client to which the indication is sent.
+ * @ind_desc: Descriptor explaining the indication structure.
+ * @ind: Pointer to the indication structure.
+ * @ind_len: Length of the indication structure.
+ *
+ * @return: 0 on success, < 0 on error.
+ */
+int qmi_send_ind_from_cb(struct qmi_handle *handle, void *conn_handle,
+		struct msg_desc *ind_desc, void *ind, unsigned int ind_len)
+{
+	int rc = 0;
+	struct qmi_svc_clnt_conn *conn_h;
 
-	mutex_lock(&qmi->txn_lock);
-	mutex_lock(&txn->lock);
-	idr_remove(&qmi->txns, txn->id);
-	mutex_unlock(&txn->lock);
-	mutex_unlock(&qmi->txn_lock);
+	if (!handle || !conn_handle || !ind_desc)
+		return -EINVAL;
 
-	if (ret < 0)
-		return ret;
-	else if (ret == 0)
-		return -ETIMEDOUT;
-	else
-		return txn->result;
+	if ((!ind && ind_len) || (ind && !ind_len))
+		return -EINVAL;
+
+	conn_h = (struct qmi_svc_clnt_conn *)conn_handle;
+	rc = qmi_encode_and_send_resp(handle, conn_h, NULL,
+				      ind_desc, ind, ind_len);
+	if (rc < 0)
+		pr_err("%s: Error encoding and sending ind.\n", __func__);
+	return rc;
 }
-EXPORT_SYMBOL(qmi_txn_wait);
+EXPORT_SYMBOL(qmi_send_ind_from_cb);
 
 /**
- * qmi_txn_cancel() - cancel an ongoing transaction
- * @txn:	transaction id
+ * translate_err_code() - Translate Linux error codes into QMI error codes
+ * @err: Standard Linux error codes to be translated.
+ *
+ * @return: Return QMI error code.
  */
-void qmi_txn_cancel(struct qmi_txn *txn)
+static int translate_err_code(int err)
 {
-	struct qmi_handle *qmi = txn->qmi;
+	int rc;
 
-	mutex_lock(&qmi->txn_lock);
-	mutex_lock(&txn->lock);
-	idr_remove(&qmi->txns, txn->id);
-	mutex_unlock(&txn->lock);
-	mutex_unlock(&qmi->txn_lock);
+	switch (err) {
+	case -ECONNREFUSED:
+		rc = QMI_ERR_CLIENT_IDS_EXHAUSTED_V01;
+		break;
+	case -EBADMSG:
+		rc = QMI_ERR_ENCODING_V01;
+		break;
+	case -ENOMEM:
+		rc = QMI_ERR_NO_MEMORY_V01;
+		break;
+	case -EOPNOTSUPP:
+		rc = QMI_ERR_MALFORMED_MSG_V01;
+		break;
+	case -ENOTSUPP:
+		rc = QMI_ERR_NOT_SUPPORTED_V01;
+		break;
+	default:
+		rc = QMI_ERR_INTERNAL_V01;
+		break;
+	}
+	return rc;
 }
-EXPORT_SYMBOL(qmi_txn_cancel);
 
 /**
- * qmi_invoke_handler() - find and invoke a handler for a message
- * @qmi:	qmi handle
- * @sq:		sockaddr of the sender
- * @txn:	transaction object for the message
- * @buf:	buffer containing the message
- * @len:	length of @buf
+ * send_err_resp() - Send the error response
+ * @handle: Service handle from which the response is sent.
+ * @conn_h: Client<->Service connection on which the response is sent.
+ * @addr: Client address to which the error response is sent.
+ * @msg_id: Request message id for which the error response is sent.
+ * @txn_id: Request Transaction ID for which the error response is sent.
+ * @err: Error code to be sent.
  *
- * Find handler and invoke handler for the incoming message.
+ * @return: 0 on success, standard Linux error codes on failure.
+ *
+ * This function is used to send an error response from within the QMI
+ * service interface. This function is called when the service returns
+ * an error to the QMI interface while handling a request.
  */
-static void qmi_invoke_handler(struct qmi_handle *qmi, struct sockaddr_qrtr *sq,
-			       struct qmi_txn *txn, const void *buf, size_t len)
+static int send_err_resp(struct qmi_handle *handle,
+			 struct qmi_svc_clnt_conn *conn_h, void *addr,
+			 uint16_t msg_id, uint16_t txn_id, int err)
 {
-	const struct qmi_msg_handler *handler;
-	const struct qmi_header *hdr = buf;
-	void *dest;
-	int ret;
-
-	if (!qmi->handlers)
-		return;
-
-	for (handler = qmi->handlers; handler->fn; handler++) {
-		if (handler->type == hdr->type &&
-		    handler->msg_id == hdr->msg_id)
-			break;
+	struct qmi_response_type_v01 err_resp;
+	struct qmi_txn *txn_handle;
+	struct msm_ipc_addr *dest_addr;
+	int rc;
+	int encoded_resp_len;
+	void *encoded_resp;
+
+	if (handle->handle_reset)
+		return -ENETRESET;
+
+	err_resp.result = QMI_RESULT_FAILURE_V01;
+	err_resp.error = translate_err_code(err);
+
+	/* Allocate Transaction Info */
+	txn_handle = kzalloc(sizeof(struct qmi_txn), GFP_KERNEL);
+	if (!txn_handle) {
+		pr_err("%s: Failed to allocate txn handle\n", __func__);
+		return -ENOMEM;
 	}
+	INIT_LIST_HEAD(&txn_handle->list);
+	init_waitqueue_head(&txn_handle->wait_q);
+	txn_handle->handle = handle;
+	txn_handle->enc_data = NULL;
+	txn_handle->enc_data_len = 0;
+
+	/* Encode the response msg */
+	encoded_resp_len = err_resp_desc.max_msg_len + QMI_HEADER_SIZE;
+	encoded_resp = kmalloc(encoded_resp_len, GFP_KERNEL);
+	if (!encoded_resp) {
+		pr_err("%s: Failed to allocate resp_msg_buf\n", __func__);
+		rc = -ENOMEM;
+		goto encode_and_send_err_resp_err0;
+	}
+	rc = qmi_kernel_encode(&err_resp_desc,
+		(void *)(encoded_resp + QMI_HEADER_SIZE),
+		err_resp_desc.max_msg_len, &err_resp);
+	if (rc < 0) {
+		pr_err("%s: Encode Failure %d\n", __func__, rc);
+		goto encode_and_send_err_resp_err1;
+	}
+	encoded_resp_len = rc;
 
-	if (!handler->fn)
-		return;
-
-	dest = kzalloc(handler->decoded_size, GFP_KERNEL);
-	if (!dest)
-		return;
+	/* Encode the header & Add to the txn_list */
+	txn_handle->txn_id = txn_id;
+	encode_qmi_header(encoded_resp, QMI_RESPONSE_CONTROL_FLAG,
+			  txn_handle->txn_id, msg_id,
+			  encoded_resp_len);
+	encoded_resp_len += QMI_HEADER_SIZE;
 
-	ret = qmi_decode_message(buf, len, handler->ei, dest);
-	if (ret < 0)
-		pr_err("failed to decode incoming message\n");
-	else
-		handler->fn(qmi, sq, txn, dest);
+	qmi_log(handle, QMI_RESPONSE_CONTROL_FLAG, txn_id,
+			msg_id, encoded_resp_len);
+	/*
+	 * Check if this svc_clnt has transactions queued to its pending list
+	 * and if there are any pending transactions then add the current
+	 * transaction to the pending list rather than sending it. This avoids
+	 * out-of-order message transfers.
+	 */
+	if (!conn_h) {
+		dest_addr = (struct msm_ipc_addr *)addr;
+		goto tx_err_resp;
+	}
 
-	kfree(dest);
+	mutex_lock(&conn_h->pending_txn_lock);
+	dest_addr = (struct msm_ipc_addr *)conn_h->clnt_addr;
+	if (!list_empty(&conn_h->pending_txn_list)) {
+		rc = -EAGAIN;
+		goto queue_err_resp;
+	}
+tx_err_resp:
+	rc = msm_ipc_router_send_msg(
+			(struct msm_ipc_port *)(handle->src_port),
+			dest_addr, encoded_resp, encoded_resp_len);
+queue_err_resp:
+	if (rc == -EAGAIN && conn_h) {
+		txn_handle->enc_data = encoded_resp;
+		txn_handle->enc_data_len = encoded_resp_len;
+		list_add_tail(&txn_handle->list, &conn_h->pending_txn_list);
+		mutex_unlock(&conn_h->pending_txn_lock);
+		return 0;
+	}
+	if (conn_h)
+		mutex_unlock(&conn_h->pending_txn_lock);
+	if (rc < 0)
+		pr_err("%s: send_msg failed %d\n", __func__, rc);
+encode_and_send_err_resp_err1:
+	kfree(encoded_resp);
+encode_and_send_err_resp_err0:
+	kfree(txn_handle);
+	return rc;
 }
 
 /**
- * qmi_handle_net_reset() - invoked to handle ENETRESET on a QMI handle
- * @qmi:	the QMI context
- *
- * As a result of registering a name service with the QRTR all open sockets are
- * flagged with ENETRESET and this function will be called. The typical case is
- * the initial boot, where this signals that the local node id has been
- * configured and as such any bound sockets needs to be rebound. So close the
- * socket, inform the client and re-initialize the socket.
- *
- * For clients it's generally sufficient to react to the del_server callbacks,
- * but server code is expected to treat the net_reset callback as a "bye" from
- * all nodes.
+ * handle_qmi_request() - Handle the QMI request
+ * @handle: QMI service handle on which the request has arrived.
+ * @req_msg: Request message to be handled.
+ * @txn_id: Transaction ID of the request message.
+ * @msg_id: Message ID of the request message.
+ * @msg_len: Message Length of the request message.
+ * @src_addr: Address of the source which sent the request.
+ * @src_addr_len: Length of the source address.
  *
- * Finally the QMI handle will send out registration requests for any lookups
- * and services.
+ * @return: 0 on success, standard Linux error codes on failure.
  */
-static void qmi_handle_net_reset(struct qmi_handle *qmi)
+static int handle_qmi_request(struct qmi_handle *handle,
+			      unsigned char *req_msg, uint16_t txn_id,
+			      uint16_t msg_id, uint16_t msg_len,
+			      void *src_addr, size_t src_addr_len)
 {
-	struct sockaddr_qrtr sq;
-	struct qmi_service *svc;
-	struct socket *sock;
-
-	sock = qmi_sock_create(qmi, &sq);
-	if (IS_ERR(sock))
-		return;
-
-	mutex_lock(&qmi->sock_lock);
-	sock_release(qmi->sock);
-	qmi->sock = NULL;
-	mutex_unlock(&qmi->sock_lock);
+	struct qmi_svc_clnt_conn *conn_h;
+	struct msg_desc *req_desc = NULL;
+	void *req_struct = NULL;
+	unsigned int req_struct_len = 0;
+	struct req_handle *req_h = NULL;
+	int rc = 0;
+
+	if (handle->handle_type != QMI_SERVICE_HANDLE)
+		return -EOPNOTSUPP;
+
+	conn_h = find_svc_clnt_conn(handle, src_addr, src_addr_len);
+	if (conn_h)
+		goto decode_req;
+
+	/* New client, establish a connection */
+	conn_h = add_svc_clnt_conn(handle, src_addr, src_addr_len);
+	if (!conn_h) {
+		pr_err("%s: Error adding a new conn_h\n", __func__);
+		rc = -ENOMEM;
+		goto out_handle_req;
+	}
+	rc = handle->svc_ops_options->connect_cb(handle, conn_h);
+	if (rc < 0) {
+		pr_err("%s: Error accepting new client\n", __func__);
+		rmv_svc_clnt_conn(conn_h);
+		conn_h = NULL;
+		goto out_handle_req;
+	}
 
-	qmi_recv_del_server(qmi, -1, -1);
+decode_req:
+	if (!msg_len)
+		goto process_req;
+
+	req_struct_len = handle->svc_ops_options->req_desc_cb(msg_id,
+							      &req_desc);
+	if (!req_desc || req_struct_len <= 0) {
+		pr_err("%s: Error getting req_desc for msg_id %d\n",
+			__func__, msg_id);
+		rc = -ENOTSUPP;
+		goto out_handle_req;
+	}
 
-	if (qmi->ops.net_reset)
-		qmi->ops.net_reset(qmi);
+	req_struct = kzalloc(req_struct_len, GFP_KERNEL);
+	if (!req_struct) {
+		pr_err("%s: Error allocating request struct\n", __func__);
+		rc = -ENOMEM;
+		goto out_handle_req;
+	}
 
-	mutex_lock(&qmi->sock_lock);
-	qmi->sock = sock;
-	qmi->sq = sq;
-	mutex_unlock(&qmi->sock_lock);
+	rc = qmi_kernel_decode(req_desc, req_struct,
+				(void *)(req_msg + QMI_HEADER_SIZE), msg_len);
+	if (rc < 0) {
+		pr_err("%s: Error decoding msg_id %d\n", __func__, msg_id);
+		rc = -EBADMSG;
+		goto out_handle_req;
+	}
 
-	list_for_each_entry(svc, &qmi->lookups, list_node)
-		qmi_send_new_lookup(qmi, svc);
+process_req:
+	req_h = add_req_handle(conn_h, msg_id, txn_id);
+	if (!req_h) {
+		pr_err("%s: Error adding new request handle\n", __func__);
+		rc = -ENOMEM;
+		goto out_handle_req;
+	}
+	rc = handle->svc_ops_options->req_cb(handle, conn_h, req_h,
+					      msg_id, req_struct);
+	if (rc < 0) {
+		pr_err("%s: Error while req_cb\n", __func__);
+		/* Check if the error is before or after sending a response */
+		if (verify_req_handle(conn_h, req_h))
+			rmv_req_handle(req_h);
+		else
+			rc = 0;
+	}
 
-	list_for_each_entry(svc, &qmi->services, list_node)
-		qmi_send_new_server(qmi, svc);
+out_handle_req:
+	kfree(req_struct);
+	if (rc < 0)
+		send_err_resp(handle, conn_h, src_addr, msg_id, txn_id, rc);
+	return rc;
 }
 
-static void qmi_handle_message(struct qmi_handle *qmi,
-			       struct sockaddr_qrtr *sq,
-			       const void *buf, size_t len)
+static struct qmi_txn *find_txn_handle(struct qmi_handle *handle,
+				       uint16_t txn_id)
 {
-	const struct qmi_header *hdr;
-	struct qmi_txn tmp_txn;
-	struct qmi_txn *txn = NULL;
-	int ret;
+	struct qmi_txn *txn_handle;
 
-	if (len < sizeof(*hdr)) {
-		pr_err("ignoring short QMI packet\n");
-		return;
+	list_for_each_entry(txn_handle, &handle->txn_list, list) {
+		if (txn_handle->txn_id == txn_id)
+			return txn_handle;
 	}
+	return NULL;
+}
 
-	hdr = buf;
-
-	/* If this is a response, find the matching transaction handle */
-	if (hdr->type == QMI_RESPONSE) {
-		mutex_lock(&qmi->txn_lock);
-		txn = idr_find(&qmi->txns, hdr->txn_id);
+static int handle_qmi_response(struct qmi_handle *handle,
+			       unsigned char *resp_msg, uint16_t txn_id,
+			       uint16_t msg_id, uint16_t msg_len)
+{
+	struct qmi_txn *txn_handle;
+	int rc;
+
+	/* Find the transaction handle */
+	txn_handle = find_txn_handle(handle, txn_id);
+	if (!txn_handle) {
+		pr_err("%s Response received for non-existent txn_id %d\n",
+			__func__, txn_id);
+		return 0;
+	}
 
-		/* Ignore unexpected responses */
-		if (!txn) {
-			mutex_unlock(&qmi->txn_lock);
-			return;
+	/* Decode the message */
+	rc = qmi_kernel_decode(txn_handle->resp_desc, txn_handle->resp,
+			       (void *)(resp_msg + QMI_HEADER_SIZE), msg_len);
+	if (rc < 0) {
+		pr_err("%s: Response Decode Failure <%d: %d: %d> rc: %d\n",
+			__func__, txn_id, msg_id, msg_len, rc);
+		wake_up(&txn_handle->wait_q);
+		if (txn_handle->type == QMI_ASYNC_TXN) {
+			list_del(&txn_handle->list);
+			kfree(txn_handle);
 		}
+		return rc;
+	}
 
-		mutex_lock(&txn->lock);
-		mutex_unlock(&qmi->txn_lock);
-
-		if (txn->dest && txn->ei) {
-			ret = qmi_decode_message(buf, len, txn->ei, txn->dest);
-			if (ret < 0)
-				pr_err("failed to decode incoming message\n");
-
-			txn->result = ret;
-			complete(&txn->completion);
-		} else  {
-			qmi_invoke_handler(qmi, sq, txn, buf, len);
-		}
+	/* Handle async or sync resp */
+	switch (txn_handle->type) {
+	case QMI_SYNC_TXN:
+		txn_handle->resp_received = 1;
+		wake_up(&txn_handle->wait_q);
+		rc = 0;
+		break;
 
-		mutex_unlock(&txn->lock);
-	} else {
-		/* Create a txn based on the txn_id of the incoming message */
-		memset(&tmp_txn, 0, sizeof(tmp_txn));
-		tmp_txn.id = hdr->txn_id;
+	case QMI_ASYNC_TXN:
+		if (txn_handle->resp_cb)
+			txn_handle->resp_cb(txn_handle->handle, msg_id,
+					    txn_handle->resp,
+					    txn_handle->resp_cb_data, 0);
+		list_del(&txn_handle->list);
+		kfree(txn_handle);
+		rc = 0;
+		break;
 
-		qmi_invoke_handler(qmi, sq, &tmp_txn, buf, len);
+	default:
+		pr_err("%s: Unrecognized transaction type\n", __func__);
+		return -EFAULT;
 	}
+	return rc;
 }
 
-static void qmi_data_ready_work(struct work_struct *work)
+static int handle_qmi_indication(struct qmi_handle *handle, void *msg,
+				 unsigned int msg_id, unsigned int msg_len)
 {
-	struct qmi_handle *qmi = container_of(work, struct qmi_handle, work);
-	struct qmi_ops *ops = &qmi->ops;
-	struct sockaddr_qrtr sq;
-	struct msghdr msg = { .msg_name = &sq, .msg_namelen = sizeof(sq) };
-	struct kvec iv;
-	ssize_t msglen;
-
-	for (;;) {
-		iv.iov_base = qmi->recv_buf;
-		iv.iov_len = qmi->recv_buf_size;
+	if (handle->ind_cb)
+		handle->ind_cb(handle, msg_id, msg + QMI_HEADER_SIZE,
+				msg_len, handle->ind_cb_priv);
+	return 0;
+}
 
-		mutex_lock(&qmi->sock_lock);
-		if (qmi->sock)
-			msglen = kernel_recvmsg(qmi->sock, &msg, &iv, 1,
-						iv.iov_len, MSG_DONTWAIT);
-		else
-			msglen = -EPIPE;
-		mutex_unlock(&qmi->sock_lock);
-		if (msglen == -EAGAIN)
-			break;
+int qmi_recv_msg(struct qmi_handle *handle)
+{
+	unsigned int recv_msg_len;
+	unsigned char *recv_msg = NULL;
+	struct msm_ipc_addr src_addr = {0};
+	unsigned char cntl_flag;
+	uint16_t txn_id, msg_id, msg_len;
+	int rc;
+
+	if (!handle)
+		return -EINVAL;
+
+	mutex_lock(&handle->handle_lock);
+	if (handle->handle_reset) {
+		mutex_unlock(&handle->handle_lock);
+		return -ENETRESET;
+	}
 
-		if (msglen == -ENETRESET) {
-			qmi_handle_net_reset(qmi);
+	/* Read the messages */
+	rc = msm_ipc_router_read_msg((struct msm_ipc_port *)(handle->src_port),
+				     &src_addr, &recv_msg, &recv_msg_len);
+	if (rc == -ENOMSG) {
+		mutex_unlock(&handle->handle_lock);
+		return rc;
+	}
 
-			/* The old qmi->sock is gone, our work is done */
-			break;
-		}
+	if (rc < 0) {
+		pr_err("%s: Read failed %d\n", __func__, rc);
+		mutex_unlock(&handle->handle_lock);
+		return rc;
+	}
 
-		if (msglen < 0) {
-			pr_err("qmi recvmsg failed: %zd\n", msglen);
-			break;
-		}
+	/* Decode the header & Handle the req, resp, indication message */
+	decode_qmi_header(recv_msg, &cntl_flag, &txn_id, &msg_id, &msg_len);
 
-		if (sq.sq_node == qmi->sq.sq_node &&
-		    sq.sq_port == QRTR_PORT_CTRL) {
-			qmi_recv_ctrl_pkt(qmi, qmi->recv_buf, msglen);
-		} else if (ops->msg_handler) {
-			ops->msg_handler(qmi, &sq, qmi->recv_buf, msglen);
-		} else {
-			qmi_handle_message(qmi, &sq, qmi->recv_buf, msglen);
-		}
-	}
-}
+	qmi_log(handle, cntl_flag, txn_id, msg_id, msg_len);
+	switch (cntl_flag) {
+	case QMI_REQUEST_CONTROL_FLAG:
+		rc = handle_qmi_request(handle, recv_msg, txn_id, msg_id,
+					msg_len, &src_addr, sizeof(src_addr));
+		break;
 
-static void qmi_data_ready(struct sock *sk)
-{
-	struct qmi_handle *qmi = sk->sk_user_data;
+	case QMI_RESPONSE_CONTROL_FLAG:
+		rc = handle_qmi_response(handle, recv_msg,
+					 txn_id, msg_id, msg_len);
+		break;
 
-	/*
-	 * This will be NULL if we receive data while being in
-	 * qmi_handle_release()
-	 */
-	if (!qmi)
-		return;
+	case QMI_INDICATION_CONTROL_FLAG:
+		rc = handle_qmi_indication(handle, recv_msg, msg_id, msg_len);
+		break;
 
-	queue_work(qmi->wq, &qmi->work);
+	default:
+		rc = -EFAULT;
+		pr_err("%s: Unsupported message type %d\n",
+			__func__, cntl_flag);
+		break;
+	}
+	kfree(recv_msg);
+	mutex_unlock(&handle->handle_lock);
+	return rc;
 }
+EXPORT_SYMBOL(qmi_recv_msg);
 
-static struct socket *qmi_sock_create(struct qmi_handle *qmi,
-				      struct sockaddr_qrtr *sq)
+int qmi_connect_to_service(struct qmi_handle *handle,
+			   uint32_t service_id,
+			   uint32_t service_vers,
+			   uint32_t service_ins)
 {
-	struct socket *sock;
-	int ret;
+	struct msm_ipc_port_name svc_name;
+	struct msm_ipc_server_info svc_info;
+	struct msm_ipc_addr *svc_dest_addr;
+	int rc;
+	uint32_t instance_id;
+
+	if (!handle)
+		return -EINVAL;
+
+	svc_dest_addr = kzalloc(sizeof(struct msm_ipc_addr),
+				GFP_KERNEL);
+	if (!svc_dest_addr) {
+		pr_err("%s: Failure allocating memory\n", __func__);
+		return -ENOMEM;
+	}
 
-	ret = sock_create_kern(&init_net, AF_QIPCRTR, SOCK_DGRAM,
-			       PF_QIPCRTR, &sock);
-	if (ret < 0)
-		return ERR_PTR(ret);
+	instance_id = BUILD_INSTANCE_ID(service_vers, service_ins);
+	svc_name.service = service_id;
+	svc_name.instance = instance_id;
 
-	ret = kernel_getsockname(sock, (struct sockaddr *)sq);
-	if (ret < 0) {
-		sock_release(sock);
-		return ERR_PTR(ret);
+	rc = msm_ipc_router_lookup_server_name(&svc_name, &svc_info,
+						1, LOOKUP_MASK);
+	if (rc <= 0) {
+		pr_err("%s: Server %08x:%08x not found\n",
+			__func__, service_id, instance_id);
+		return -ENODEV;
 	}
+	svc_dest_addr->addrtype = MSM_IPC_ADDR_ID;
+	svc_dest_addr->addr.port_addr.node_id = svc_info.node_id;
+	svc_dest_addr->addr.port_addr.port_id = svc_info.port_id;
+	mutex_lock(&handle->handle_lock);
+	if (handle->handle_reset) {
+		mutex_unlock(&handle->handle_lock);
+		return -ENETRESET;
+	}
+	handle->dest_info = svc_dest_addr;
+	handle->dest_service_id = service_id;
+	mutex_unlock(&handle->handle_lock);
 
-	sock->sk->sk_user_data = qmi;
-	sock->sk->sk_data_ready = qmi_data_ready;
-	sock->sk->sk_error_report = qmi_data_ready;
-
-	return sock;
+	return 0;
 }
+EXPORT_SYMBOL(qmi_connect_to_service);
 
 /**
- * qmi_handle_init() - initialize a QMI client handle
- * @qmi:	QMI handle to initialize
- * @recv_buf_size: maximum size of incoming message
- * @ops:	reference to callbacks for QRTR notifications
- * @handlers:	NULL-terminated list of QMI message handlers
+ * svc_event_add_svc_addr() - Add a specific service address to the list
+ * @event_nb:	Reference to the service event structure.
+ * @node_id:	Node id of the service address.
+ * @port_id:	Port id of the service address.
  *
- * This initializes the QMI client handle to allow sending and receiving QMI
- * messages. As messages are received the appropriate handler will be invoked.
+ * Return: 0 on success, standard error code otheriwse.
  *
- * Return: 0 on success, negative errno on failure.
+ * This function should be called with svc_addr_list_lock locked.
  */
-int qmi_handle_init(struct qmi_handle *qmi, size_t recv_buf_size,
-		    const struct qmi_ops *ops,
-		    const struct qmi_msg_handler *handlers)
+static int svc_event_add_svc_addr(struct svc_event_nb *event_nb,
+				uint32_t node_id, uint32_t port_id)
 {
-	int ret;
-
-	mutex_init(&qmi->txn_lock);
-	mutex_init(&qmi->sock_lock);
 
-	idr_init(&qmi->txns);
+	struct svc_addr *addr;
 
-	INIT_LIST_HEAD(&qmi->lookups);
-	INIT_LIST_HEAD(&qmi->lookup_results);
-	INIT_LIST_HEAD(&qmi->services);
-
-	INIT_WORK(&qmi->work, qmi_data_ready_work);
-
-	qmi->handlers = handlers;
-	if (ops)
-		qmi->ops = *ops;
-
-	/* Make room for the header */
-	recv_buf_size += sizeof(struct qmi_header);
-	/* Must also be sufficient to hold a control packet */
-	if (recv_buf_size < sizeof(struct qrtr_ctrl_pkt))
-		recv_buf_size = sizeof(struct qrtr_ctrl_pkt);
-
-	qmi->recv_buf_size = recv_buf_size;
-	qmi->recv_buf = kzalloc(recv_buf_size, GFP_KERNEL);
-	if (!qmi->recv_buf)
+	if (!event_nb)
+		return -EINVAL;
+	addr = kmalloc(sizeof(*addr), GFP_KERNEL);
+	if (!addr) {
+		pr_err("%s: Memory allocation failed for address list\n",
+			__func__);
 		return -ENOMEM;
+	}
+	addr->port_addr.node_id = node_id;
+	addr->port_addr.port_id = port_id;
+	list_add_tail(&addr->list_node, &event_nb->svc_addr_list);
+	return 0;
+}
 
-	qmi->wq = alloc_workqueue("qmi_msg_handler", WQ_UNBOUND, 1);
-	if (!qmi->wq) {
-		ret = -ENOMEM;
-		goto err_free_recv_buf;
+/**
+ * qmi_notify_svc_event_arrive() - Notify the clients about service arrival
+ * @service:	Service id for the specific service.
+ * @instance:	Instance id for the specific service.
+ * @node_id:	Node id of the processor where the service is hosted.
+ * @port_id:	Port id of the service port created by IPC Router.
+ *
+ * Return:	0 on Success or standard error code.
+ */
+static int qmi_notify_svc_event_arrive(uint32_t service,
+					uint32_t instance,
+					uint32_t node_id,
+					uint32_t port_id)
+{
+	struct svc_event_nb *temp;
+	unsigned long flags;
+	struct svc_addr *addr;
+	bool already_notified = false;
+
+	mutex_lock(&svc_event_nb_list_lock);
+	temp = find_svc_event_nb(service, instance);
+	if (!temp) {
+		mutex_unlock(&svc_event_nb_list_lock);
+		return -EINVAL;
 	}
+	mutex_unlock(&svc_event_nb_list_lock);
+
+	mutex_lock(&temp->svc_addr_list_lock);
+	list_for_each_entry(addr, &temp->svc_addr_list, list_node)
+		if (addr->port_addr.node_id == node_id &&
+			addr->port_addr.port_id == port_id)
+				already_notified = true;
+	if (!already_notified) {
+		/*
+		 * Notify only if the clients are not notified about the
+		 * service during registration.
+		 */
+		svc_event_add_svc_addr(temp, node_id, port_id);
+		spin_lock_irqsave(&temp->nb_lock, flags);
+		raw_notifier_call_chain(&temp->svc_event_rcvr_list,
+				QMI_SERVER_ARRIVE, NULL);
+		spin_unlock_irqrestore(&temp->nb_lock, flags);
+	}
+	mutex_unlock(&temp->svc_addr_list_lock);
+
+	return 0;
+}
 
-	qmi->sock = qmi_sock_create(qmi, &qmi->sq);
-	if (IS_ERR(qmi->sock)) {
-		pr_err("failed to create QMI socket\n");
-		ret = PTR_ERR(qmi->sock);
-		goto err_destroy_wq;
+/**
+ * qmi_notify_svc_event_exit() - Notify the clients about service exit
+ * @service:	Service id for the specific service.
+ * @instance:	Instance id for the specific service.
+ * @node_id:	Node id of the processor where the service is hosted.
+ * @port_id:	Port id of the service port created by IPC Router.
+ *
+ * Return:	0 on Success or standard error code.
+ */
+static int qmi_notify_svc_event_exit(uint32_t service,
+					uint32_t instance,
+					uint32_t node_id,
+					uint32_t port_id)
+{
+	struct svc_event_nb *temp;
+	unsigned long flags;
+	struct svc_addr *addr;
+	struct svc_addr *temp_addr;
+
+	mutex_lock(&svc_event_nb_list_lock);
+	temp = find_svc_event_nb(service, instance);
+	if (!temp) {
+		mutex_unlock(&svc_event_nb_list_lock);
+		return -EINVAL;
 	}
+	mutex_unlock(&svc_event_nb_list_lock);
+
+	mutex_lock(&temp->svc_addr_list_lock);
+	list_for_each_entry_safe(addr, temp_addr, &temp->svc_addr_list,
+					list_node) {
+		if (addr->port_addr.node_id == node_id &&
+			addr->port_addr.port_id == port_id) {
+			/*
+			 * Notify only if an already notified service has
+			 * gone down.
+			 */
+			spin_lock_irqsave(&temp->nb_lock, flags);
+			raw_notifier_call_chain(&temp->svc_event_rcvr_list,
+						QMI_SERVER_EXIT, NULL);
+			spin_unlock_irqrestore(&temp->nb_lock, flags);
+			list_del(&addr->list_node);
+			kfree(addr);
+		}
+	}
+
+	mutex_unlock(&temp->svc_addr_list_lock);
 
 	return 0;
+}
 
-err_destroy_wq:
-	destroy_workqueue(qmi->wq);
-err_free_recv_buf:
-	kfree(qmi->recv_buf);
+static struct svc_event_nb *find_svc_event_nb(uint32_t service_id,
+					      uint32_t instance_id)
+{
+	struct svc_event_nb *temp;
 
-	return ret;
+	list_for_each_entry(temp, &svc_event_nb_list, list) {
+		if (temp->service_id == service_id &&
+		    temp->instance_id == instance_id)
+			return temp;
+	}
+	return NULL;
 }
-EXPORT_SYMBOL(qmi_handle_init);
 
 /**
- * qmi_handle_release() - release the QMI client handle
- * @qmi:	QMI client handle
+ * find_and_add_svc_event_nb() - Find/Add a notifier block for specific service
+ * @service_id:	Service Id of the service
+ * @instance_id:Instance Id of the service
+ *
+ * Return:	Pointer to svc_event_nb structure for the specified service
  *
- * This closes the underlying socket and stops any handling of QMI messages.
+ * This function should only be called after acquiring svc_event_nb_list_lock.
  */
-void qmi_handle_release(struct qmi_handle *qmi)
+static struct svc_event_nb *find_and_add_svc_event_nb(uint32_t service_id,
+						      uint32_t instance_id)
 {
-	struct socket *sock = qmi->sock;
-	struct qmi_service *svc, *tmp;
+	struct svc_event_nb *temp;
 
-	sock->sk->sk_user_data = NULL;
-	cancel_work_sync(&qmi->work);
+	temp = find_svc_event_nb(service_id, instance_id);
+	if (temp)
+		return temp;
 
-	qmi_recv_del_server(qmi, -1, -1);
+	temp = kzalloc(sizeof(struct svc_event_nb), GFP_KERNEL);
+	if (!temp) {
+		pr_err("%s: Failed to alloc notifier block\n", __func__);
+		return temp;
+	}
 
-	mutex_lock(&qmi->sock_lock);
-	sock_release(sock);
-	qmi->sock = NULL;
-	mutex_unlock(&qmi->sock_lock);
+	spin_lock_init(&temp->nb_lock);
+	temp->service_id = service_id;
+	temp->instance_id = instance_id;
+	INIT_LIST_HEAD(&temp->list);
+	INIT_LIST_HEAD(&temp->svc_addr_list);
+	RAW_INIT_NOTIFIER_HEAD(&temp->svc_event_rcvr_list);
+	mutex_init(&temp->svc_addr_list_lock);
+	list_add_tail(&temp->list, &svc_event_nb_list);
 
-	destroy_workqueue(qmi->wq);
+	return temp;
+}
 
-	idr_destroy(&qmi->txns);
+int qmi_svc_event_notifier_register(uint32_t service_id,
+				    uint32_t service_vers,
+				    uint32_t service_ins,
+				    struct notifier_block *nb)
+{
+	struct svc_event_nb *temp;
+	unsigned long flags;
+	int ret;
+	int i;
+	int num_servers;
+	uint32_t instance_id;
+	struct msm_ipc_port_name svc_name;
+	struct msm_ipc_server_info *svc_info_arr = NULL;
+
+	mutex_lock(&qmi_svc_event_notifier_lock);
+	if (!qmi_svc_event_notifier_port && !qmi_svc_event_notifier_wq)
+		qmi_svc_event_notifier_init();
+	mutex_unlock(&qmi_svc_event_notifier_lock);
+
+	instance_id = BUILD_INSTANCE_ID(service_vers, service_ins);
+	mutex_lock(&svc_event_nb_list_lock);
+	temp = find_and_add_svc_event_nb(service_id, instance_id);
+	if (!temp) {
+		mutex_unlock(&svc_event_nb_list_lock);
+		return -EFAULT;
+	}
+	mutex_unlock(&svc_event_nb_list_lock);
+
+	mutex_lock(&temp->svc_addr_list_lock);
+	spin_lock_irqsave(&temp->nb_lock, flags);
+	ret = raw_notifier_chain_register(&temp->svc_event_rcvr_list, nb);
+	spin_unlock_irqrestore(&temp->nb_lock, flags);
+	if (!list_empty(&temp->svc_addr_list)) {
+		/* Notify this client only if Some services already exist. */
+		spin_lock_irqsave(&temp->nb_lock, flags);
+		nb->notifier_call(nb, QMI_SERVER_ARRIVE, NULL);
+		spin_unlock_irqrestore(&temp->nb_lock, flags);
+	} else {
+		/*
+		 * Check if we have missed a new server event that happened
+		 * earlier.
+		 */
+		svc_name.service = service_id;
+		svc_name.instance = instance_id;
+		num_servers = msm_ipc_router_lookup_server_name(&svc_name,
+								NULL,
+								0, LOOKUP_MASK);
+		if (num_servers > 0) {
+			svc_info_arr = kmalloc_array(num_servers,
+						sizeof(*svc_info_arr),
+						GFP_KERNEL);
+			if (!svc_info_arr)
+				return -ENOMEM;
+			num_servers = msm_ipc_router_lookup_server_name(
+								&svc_name,
+								svc_info_arr,
+								num_servers,
+								LOOKUP_MASK);
+			for (i = 0; i < num_servers; i++)
+				svc_event_add_svc_addr(temp,
+						svc_info_arr[i].node_id,
+						svc_info_arr[i].port_id);
+			kfree(svc_info_arr);
+
+			spin_lock_irqsave(&temp->nb_lock, flags);
+			raw_notifier_call_chain(&temp->svc_event_rcvr_list,
+						QMI_SERVER_ARRIVE, NULL);
+			spin_unlock_irqrestore(&temp->nb_lock, flags);
+		}
+	}
+	mutex_unlock(&temp->svc_addr_list_lock);
 
-	kfree(qmi->recv_buf);
+	return ret;
+}
+EXPORT_SYMBOL(qmi_svc_event_notifier_register);
 
-	/* Free registered lookup requests */
-	list_for_each_entry_safe(svc, tmp, &qmi->lookups, list_node) {
-		list_del(&svc->list_node);
-		kfree(svc);
+int qmi_svc_event_notifier_unregister(uint32_t service_id,
+				      uint32_t service_vers,
+				      uint32_t service_ins,
+				      struct notifier_block *nb)
+{
+	int ret;
+	struct svc_event_nb *temp;
+	unsigned long flags;
+	uint32_t instance_id;
+
+	instance_id = BUILD_INSTANCE_ID(service_vers, service_ins);
+	mutex_lock(&svc_event_nb_list_lock);
+	temp = find_svc_event_nb(service_id, instance_id);
+	if (!temp) {
+		mutex_unlock(&svc_event_nb_list_lock);
+		return -EINVAL;
 	}
 
-	/* Free registered service information */
-	list_for_each_entry_safe(svc, tmp, &qmi->services, list_node) {
-		list_del(&svc->list_node);
-		kfree(svc);
-	}
+	spin_lock_irqsave(&temp->nb_lock, flags);
+	ret = raw_notifier_chain_unregister(&temp->svc_event_rcvr_list, nb);
+	spin_unlock_irqrestore(&temp->nb_lock, flags);
+	mutex_unlock(&svc_event_nb_list_lock);
+
+	return ret;
 }
-EXPORT_SYMBOL(qmi_handle_release);
+EXPORT_SYMBOL(qmi_svc_event_notifier_unregister);
 
 /**
- * qmi_send_message() - send a QMI message
- * @qmi:	QMI client handle
- * @sq:		destination sockaddr
- * @txn:	transaction object to use for the message
- * @type:	type of message to send
- * @msg_id:	message id
- * @len:	max length of the QMI message
- * @ei:		QMI message description
- * @c_struct:	object to be encoded
+ * qmi_svc_event_worker() - Read control messages over service event port
+ * @work:	Reference to the work structure queued.
  *
- * This function encodes @c_struct using @ei into a message of type @type,
- * with @msg_id and @txn into a buffer of maximum size @len, and sends this to
- * @sq.
- *
- * Return: 0 on success, negative errno on failure.
  */
-static ssize_t qmi_send_message(struct qmi_handle *qmi,
-				struct sockaddr_qrtr *sq, struct qmi_txn *txn,
-				int type, int msg_id, size_t len,
-				struct qmi_elem_info *ei, const void *c_struct)
+static void qmi_svc_event_worker(struct work_struct *work)
 {
-	struct msghdr msghdr = {};
-	struct kvec iv;
-	void *msg;
+	union rr_control_msg *ctl_msg = NULL;
+	unsigned int ctl_msg_len;
+	struct msm_ipc_addr src_addr;
 	int ret;
 
-	msg = qmi_encode_message(type,
-				 msg_id, &len,
-				 txn->id, ei,
-				 c_struct);
-	if (IS_ERR(msg))
-		return PTR_ERR(msg);
+	while (1) {
+		ret = msm_ipc_router_read_msg(qmi_svc_event_notifier_port,
+			&src_addr, (unsigned char **)&ctl_msg, &ctl_msg_len);
+		if (ret == -ENOMSG)
+			break;
+		if (ret < 0) {
+			pr_err("%s:Error receiving control message\n",
+					__func__);
+			break;
+		}
+		if (ctl_msg->cmd == IPC_ROUTER_CTRL_CMD_NEW_SERVER)
+			qmi_notify_svc_event_arrive(ctl_msg->srv.service,
+							ctl_msg->srv.instance,
+							ctl_msg->srv.node_id,
+							ctl_msg->srv.port_id);
+		else if (ctl_msg->cmd == IPC_ROUTER_CTRL_CMD_REMOVE_SERVER)
+			qmi_notify_svc_event_exit(ctl_msg->srv.service,
+							ctl_msg->srv.instance,
+							ctl_msg->srv.node_id,
+							ctl_msg->srv.port_id);
+		kfree(ctl_msg);
+	}
+}
 
-	iv.iov_base = msg;
-	iv.iov_len = len;
+/**
+ * qmi_svc_event_notify() - Callback for any service event posted on the control port
+ * @event:	The event posted on the control port.
+ * @data:	Any out-of-band data associated with event.
+ * @odata_len:	Length of the out-of-band data, if any.
+ * @priv:	Private Data.
+ *
+ * This function is called by the underlying transport to notify the QMI
+ * interface regarding any incoming service related events. It is registered
+ * during service event control port creation.
+ */
+static void qmi_svc_event_notify(unsigned event, void *data,
+				size_t odata_len, void *priv)
+{
+	if (event == IPC_ROUTER_CTRL_CMD_NEW_SERVER
+		|| event == IPC_ROUTER_CTRL_CMD_REMOVE_CLIENT
+		|| event == IPC_ROUTER_CTRL_CMD_REMOVE_SERVER)
+		queue_work(qmi_svc_event_notifier_wq, &qmi_svc_event_work);
+}
 
-	if (sq) {
-		msghdr.msg_name = sq;
-		msghdr.msg_namelen = sizeof(*sq);
+/**
+ * qmi_svc_event_notifier_init() - Create a control port to get service events
+ *
+ * This function is called during first service notifier registration. It
+ * creates a control port to get notification about server events so that
+ * respective clients can be notified about the events.
+ */
+static void qmi_svc_event_notifier_init(void)
+{
+	qmi_svc_event_notifier_wq = create_singlethread_workqueue(
+					"qmi_svc_event_wq");
+	if (!qmi_svc_event_notifier_wq) {
+		pr_err("%s: ctrl workqueue allocation failed\n", __func__);
+		return;
 	}
-
-	mutex_lock(&qmi->sock_lock);
-	if (qmi->sock) {
-		ret = kernel_sendmsg(qmi->sock, &msghdr, &iv, 1, len);
-		if (ret < 0)
-			pr_err("failed to send QMI message\n");
-	} else {
-		ret = -EPIPE;
+	qmi_svc_event_notifier_port = msm_ipc_router_create_port(
+				qmi_svc_event_notify, NULL);
+	if (!qmi_svc_event_notifier_port) {
+		destroy_workqueue(qmi_svc_event_notifier_wq);
+		pr_err("%s: IPC Router Port creation failed\n", __func__);
+		return;
 	}
-	mutex_unlock(&qmi->sock_lock);
+	msm_ipc_router_bind_control_port(qmi_svc_event_notifier_port);
 
-	kfree(msg);
-
-	return ret < 0 ? ret : 0;
+	return;
 }
 
 /**
- * qmi_send_request() - send a request QMI message
- * @qmi:	QMI client handle
- * @sq:		destination sockaddr
- * @txn:	transaction object to use for the message
- * @msg_id:	message id
- * @len:	max length of the QMI message
- * @ei:		QMI message description
- * @c_struct:	object to be encoded
+ * qmi_log_init() - Init function for IPC Logging
  *
- * Return: 0 on success, negative errno on failure.
+ * Initialize log contexts for QMI request/response/indications.
  */
-ssize_t qmi_send_request(struct qmi_handle *qmi, struct sockaddr_qrtr *sq,
-			 struct qmi_txn *txn, int msg_id, size_t len,
-			 struct qmi_elem_info *ei, const void *c_struct)
+void qmi_log_init(void)
 {
-	return qmi_send_message(qmi, sq, txn, QMI_REQUEST, msg_id, len, ei,
-				c_struct);
+	qmi_req_resp_log_ctx =
+		ipc_log_context_create(QMI_REQ_RESP_LOG_PAGES,
+			"kqmi_req_resp", 0);
+	if (!qmi_req_resp_log_ctx)
+		pr_err("%s: Unable to create QMI IPC logging for Req/Resp",
+			__func__);
+	qmi_ind_log_ctx =
+		ipc_log_context_create(QMI_IND_LOG_PAGES, "kqmi_ind", 0);
+	if (!qmi_ind_log_ctx)
+		pr_err("%s: Unable to create QMI IPC %s",
+				"logging for Indications", __func__);
 }
-EXPORT_SYMBOL(qmi_send_request);
 
 /**
- * qmi_send_response() - send a response QMI message
- * @qmi:	QMI client handle
- * @sq:		destination sockaddr
- * @txn:	transaction object to use for the message
- * @msg_id:	message id
- * @len:	max length of the QMI message
- * @ei:		QMI message description
- * @c_struct:	object to be encoded
+ * qmi_svc_register() - Register a QMI service with a QMI handle
+ * @handle: QMI handle on which the service has to be registered.
+ * @ops_options: Service specific operations and options.
  *
- * Return: 0 on success, negative errno on failure.
+ * @return: 0 if successfully registered, < 0 on error.
  */
-ssize_t qmi_send_response(struct qmi_handle *qmi, struct sockaddr_qrtr *sq,
-			  struct qmi_txn *txn, int msg_id, size_t len,
-			  struct qmi_elem_info *ei, const void *c_struct)
+int qmi_svc_register(struct qmi_handle *handle, void *ops_options)
 {
-	return qmi_send_message(qmi, sq, txn, QMI_RESPONSE, msg_id, len, ei,
-				c_struct);
+	struct qmi_svc_ops_options *svc_ops_options;
+	struct msm_ipc_addr svc_name;
+	int rc;
+	uint32_t instance_id;
+
+	svc_ops_options = (struct qmi_svc_ops_options *)ops_options;
+	if (!handle || !svc_ops_options)
+		return -EINVAL;
+
+	/* Check if the required elements of opts_options are filled */
+	if (!svc_ops_options->service_id || !svc_ops_options->service_vers ||
+	    !svc_ops_options->connect_cb || !svc_ops_options->disconnect_cb ||
+	    !svc_ops_options->req_desc_cb || !svc_ops_options->req_cb)
+		return -EINVAL;
+
+	mutex_lock(&handle->handle_lock);
+	/* Check if another service/client is registered in that handle */
+	if (handle->handle_type == QMI_SERVICE_HANDLE || handle->dest_info) {
+		mutex_unlock(&handle->handle_lock);
+		return -EBUSY;
+	}
+	INIT_LIST_HEAD(&handle->conn_list);
+	mutex_unlock(&handle->handle_lock);
+
+	/*
+	 * Unlocked the handle_lock, because NEW_SERVER message will end up
+	 * in this handle's control port, which requires holding the same
+	 * mutex. Also it is safe to call register_server unlocked.
+	 */
+	/* Register the service */
+	instance_id = ((svc_ops_options->service_vers & 0xFF) |
+		       ((svc_ops_options->service_ins & 0xFF) << 8));
+	svc_name.addrtype = MSM_IPC_ADDR_NAME;
+	svc_name.addr.port_name.service = svc_ops_options->service_id;
+	svc_name.addr.port_name.instance = instance_id;
+	rc = msm_ipc_router_register_server(
+		(struct msm_ipc_port *)handle->src_port, &svc_name);
+	if (rc < 0) {
+		pr_err("%s: Error %d registering QMI service %08x:%08x\n",
+			__func__, rc, svc_ops_options->service_id,
+			instance_id);
+		return rc;
+	}
+	mutex_lock(&handle->handle_lock);
+	handle->svc_ops_options = svc_ops_options;
+	handle->handle_type = QMI_SERVICE_HANDLE;
+	mutex_unlock(&handle->handle_lock);
+	return rc;
 }
-EXPORT_SYMBOL(qmi_send_response);
+EXPORT_SYMBOL(qmi_svc_register);
+
 
 /**
- * qmi_send_indication() - send an indication QMI message
- * @qmi:	QMI client handle
- * @sq:		destination sockaddr
- * @msg_id:	message id
- * @len:	max length of the QMI message
- * @ei:		QMI message description
- * @c_struct:	object to be encoded
+ * qmi_svc_unregister() - Unregister the service from a QMI handle
+ * @handle: QMI handle from which the service has to be unregistered.
  *
- * Return: 0 on success, negative errno on failure.
+ * return: 0 on success, < 0 on error.
  */
-ssize_t qmi_send_indication(struct qmi_handle *qmi, struct sockaddr_qrtr *sq,
-			    int msg_id, size_t len, struct qmi_elem_info *ei,
-			    const void *c_struct)
+int qmi_svc_unregister(struct qmi_handle *handle)
 {
-	struct qmi_txn txn;
-	ssize_t rval;
-	int ret;
+	struct qmi_svc_clnt_conn *conn_h, *temp_conn_h;
 
-	ret = qmi_txn_init(qmi, &txn, NULL, NULL);
-	if (ret < 0)
-		return ret;
+	if (!handle || handle->handle_type != QMI_SERVICE_HANDLE)
+		return -EINVAL;
 
-	rval = qmi_send_message(qmi, sq, &txn, QMI_INDICATION, msg_id, len, ei,
-				c_struct);
-
-	/* We don't care about future messages on this txn */
-	qmi_txn_cancel(&txn);
+	mutex_lock(&handle->handle_lock);
+	handle->handle_type = QMI_CLIENT_HANDLE;
+	mutex_unlock(&handle->handle_lock);
+	/*
+	 * Unlocked the handle_lock, because REMOVE_SERVER message will end up
+	 * in this handle's control port, which requires holding the same
+	 * mutex. Also it is safe to call register_server unlocked.
+	 */
+	msm_ipc_router_unregister_server(
+		(struct msm_ipc_port *)handle->src_port);
+
+	mutex_lock(&handle->handle_lock);
+	list_for_each_entry_safe(conn_h, temp_conn_h,
+				 &handle->conn_list, list)
+		rmv_svc_clnt_conn(conn_h);
+	mutex_unlock(&handle->handle_lock);
+	return 0;
+}
+EXPORT_SYMBOL(qmi_svc_unregister);
 
-	return rval;
+static int __init qmi_interface_init(void)
+{
+	qmi_log_init();
+	return 0;
 }
-EXPORT_SYMBOL(qmi_send_indication);
+module_init(qmi_interface_init);
+
+MODULE_DESCRIPTION("MSM QMI Interface");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/qcom/smp2p.c b/drivers/soc/qcom/smp2p.c
index c22503cd1edf..79b8ffbc5ee7 100644
--- a/drivers/soc/qcom/smp2p.c
+++ b/drivers/soc/qcom/smp2p.c
@@ -1,6 +1,6 @@
-/*
- * Copyright (c) 2015, Sony Mobile Communications AB.
- * Copyright (c) 2012-2013, The Linux Foundation. All rights reserved.
+/* drivers/soc/qcom/smp2p.c
+ *
+ * Copyright (c) 2013-2016, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -11,596 +11,1941 @@
  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  * GNU General Public License for more details.
  */
-
-#include <linux/interrupt.h>
 #include <linux/list.h>
+#include <linux/ctype.h>
+#include <linux/slab.h>
+#include <linux/module.h>
 #include <linux/io.h>
 #include <linux/of.h>
-#include <linux/irq.h>
-#include <linux/irqdomain.h>
-#include <linux/mailbox_client.h>
-#include <linux/mfd/syscon.h>
-#include <linux/module.h>
 #include <linux/platform_device.h>
-#include <linux/regmap.h>
-#include <linux/soc/qcom/smem.h>
-#include <linux/soc/qcom/smem_state.h>
-#include <linux/spinlock.h>
-
-/*
- * The Shared Memory Point to Point (SMP2P) protocol facilitates communication
- * of a single 32-bit value between two processors.  Each value has a single
- * writer (the local side) and a single reader (the remote side). Values are
- * uniquely identified in the system by the directed edge (local processor ID
- * to remote processor ID) and a string identifier.
- *
- * Each processor is responsible for creating the outgoing SMEM items and each
- * item is writable by the local processor and readable by the remote
- * processor.  By using two separate SMEM items that are single-reader and
- * single-writer, SMP2P does not require any remote locking mechanisms.
- *
- * The driver uses the Linux GPIO and interrupt framework to expose a virtual
- * GPIO for each outbound entry and a virtual interrupt controller for each
- * inbound entry.
- */
-
-#define SMP2P_MAX_ENTRY 16
-#define SMP2P_MAX_ENTRY_NAME 16
-
-#define SMP2P_FEATURE_SSR_ACK 0x1
-
-#define SMP2P_MAGIC 0x504d5324
-
-/**
- * struct smp2p_smem_item - in memory communication structure
- * @magic:		magic number
- * @version:		version - must be 1
- * @features:		features flag - currently unused
- * @local_pid:		processor id of sending end
- * @remote_pid:		processor id of receiving end
- * @total_entries:	number of entries - always SMP2P_MAX_ENTRY
- * @valid_entries:	number of allocated entries
- * @flags:
- * @entries:		individual communication entries
- *     @name:		name of the entry
- *     @value:		content of the entry
- */
-struct smp2p_smem_item {
-	u32 magic;
-	u8 version;
-	unsigned features:24;
-	u16 local_pid;
-	u16 remote_pid;
-	u16 total_entries;
-	u16 valid_entries;
-	u32 flags;
-
-	struct {
-		u8 name[SMP2P_MAX_ENTRY_NAME];
-		u32 value;
-	} entries[SMP2P_MAX_ENTRY];
-} __packed;
-
-/**
- * struct smp2p_entry - driver context matching one entry
- * @node:	list entry to keep track of allocated entries
- * @smp2p:	reference to the device driver context
- * @name:	name of the entry, to match against smp2p_smem_item
- * @value:	pointer to smp2p_smem_item entry value
- * @last_value:	last handled value
- * @domain:	irq_domain for inbound entries
- * @irq_enabled:bitmap to track enabled irq bits
- * @irq_rising:	bitmap to mark irq bits for rising detection
- * @irq_falling:bitmap to mark irq bits for falling detection
- * @state:	smem state handle
- * @lock:	spinlock to protect read-modify-write of the value
- */
-struct smp2p_entry {
-	struct list_head node;
-	struct qcom_smp2p *smp2p;
-
-	const char *name;
-	u32 *value;
-	u32 last_value;
-
-	struct irq_domain *domain;
-	DECLARE_BITMAP(irq_enabled, 32);
-	DECLARE_BITMAP(irq_rising, 32);
-	DECLARE_BITMAP(irq_falling, 32);
-
-	struct qcom_smem_state *state;
-
-	spinlock_t lock;
-};
+#include <linux/interrupt.h>
+#include <linux/ipc_logging.h>
+#include <linux/err.h>
+#include <soc/qcom/smem.h>
+#include "smp2p_private_api.h"
+#include "smp2p_private.h"
+
+#define NUM_LOG_PAGES 3
 
-#define SMP2P_INBOUND	0
-#define SMP2P_OUTBOUND	1
+/**
+ * struct msm_smp2p_out - This structure represents the outbound SMP2P entry.
+ *
+ * @remote_pid: Outbound processor ID.
+ * @name: Entry name.
+ * @out_edge_list: Adds this structure into smp2p_out_list_item::list.
+ * @msm_smp2p_notifier_list: Notifier block head used to notify for open event.
+ * @open_nb: Notifier block used to notify for open event.
+ * @l_smp2p_entry: Pointer to the actual entry in the SMEM item.
+ */
+struct msm_smp2p_out {
+	int remote_pid;
+	char name[SMP2P_MAX_ENTRY_NAME];
+	struct list_head out_edge_list;
+	struct raw_notifier_head msm_smp2p_notifier_list;
+	struct notifier_block *open_nb;
+	uint32_t __iomem *l_smp2p_entry;
+};
 
 /**
- * struct qcom_smp2p - device driver context
- * @dev:	device driver handle
- * @in:		pointer to the inbound smem item
- * @smem_items:	ids of the two smem items
- * @valid_entries: already scanned inbound entries
- * @local_pid:	processor id of the inbound edge
- * @remote_pid:	processor id of the outbound edge
- * @ipc_regmap:	regmap for the outbound ipc
- * @ipc_offset:	offset within the regmap
- * @ipc_bit:	bit in regmap@offset to kick to signal remote processor
- * @mbox_client: mailbox client handle
- * @mbox_chan:	apcs ipc mailbox channel handle
- * @inbound:	list of inbound entries
- * @outbound:	list of outbound entries
+ * struct smp2p_out_list_item - Maintains the state of outbound edge.
+ *
+ * @out_item_lock_lha1: Lock protecting all elements of the structure.
+ * @list: list of outbound entries (struct msm_smp2p_out).
+ * @smem_edge_out: Pointer to outbound smem item.
+ * @smem_edge_state: State of the outbound edge.
+ * @ops_ptr: Pointer to internal version-specific SMEM item access functions.
+ *
+ * @feature_ssr_ack_enabled: SSR ACK Support Enabled
+ * @restart_ack: Current cached state of the local ack bit
  */
-struct qcom_smp2p {
-	struct device *dev;
+struct smp2p_out_list_item {
+	spinlock_t out_item_lock_lha1;
 
-	struct smp2p_smem_item *in;
-	struct smp2p_smem_item *out;
+	struct list_head list;
+	struct smp2p_smem __iomem *smem_edge_out;
+	enum msm_smp2p_edge_state smem_edge_state;
+	struct smp2p_version_if *ops_ptr;
+
+	bool feature_ssr_ack_enabled;
+	bool restart_ack;
+};
+static struct smp2p_out_list_item out_list[SMP2P_NUM_PROCS];
 
-	unsigned smem_items[SMP2P_OUTBOUND + 1];
+static void *log_ctx;
+static int smp2p_debug_mask = MSM_SMP2P_INFO | MSM_SMP2P_DEBUG;
+module_param_named(debug_mask, smp2p_debug_mask,
+		   int, S_IRUGO | S_IWUSR | S_IWGRP);
 
-	unsigned valid_entries;
+/**
+ * struct smp2p_in - Represents the entry on remote processor.
+ *
+ * @name: Name of the entry.
+ * @remote_pid: Outbound processor ID.
+ * @in_edge_list: Adds this structure into smp2p_in_list_item::list.
+ * @in_notifier_list: List for notifier block for entry opening/updates.
+ * @prev_entry_val: Previous value of the entry.
+ * @entry_ptr: Points to the current value in smem item.
+ * @notifier_count: Counts the number of notifier registered per pid,entry.
+ */
+struct smp2p_in {
+	int remote_pid;
+	char name[SMP2P_MAX_ENTRY_NAME];
+	struct list_head in_edge_list;
+	struct raw_notifier_head in_notifier_list;
+	uint32_t prev_entry_val;
+	uint32_t __iomem *entry_ptr;
+	uint32_t notifier_count;
+};
 
-	unsigned local_pid;
-	unsigned remote_pid;
+/**
+ * struct smp2p_in_list_item - Maintains the inbound edge state.
+ *
+ * @in_item_lock_lhb1: Lock protecting all elements of the structure.
+ * @list: List head for the entries on remote processor.
+ * @smem_edge_in: Pointer to the remote smem item.
+ */
+struct smp2p_in_list_item {
+	spinlock_t in_item_lock_lhb1;
+	struct list_head list;
+	struct smp2p_smem __iomem *smem_edge_in;
+	uint32_t item_size;
+	uint32_t safe_total_entries;
+};
+static struct smp2p_in_list_item in_list[SMP2P_NUM_PROCS];
 
-	struct regmap *ipc_regmap;
-	int ipc_offset;
-	int ipc_bit;
+/**
+ * SMEM Item access function interface.
+ *
+ * This interface is used to help isolate the implementation of
+ * the functionality from any changes in the shared data structures
+ * that may happen as versions are changed.
+ *
+ * @is_supported: True if this version is supported by SMP2P
+ * @negotiate_features: Returns (sub)set of supported features
+ * @negotiation_complete:  Called when negotiation has been completed
+ * @find_entry: Finds existing / next empty entry
+ * @create_entry: Creates a new entry
+ * @read_entry: Reads the value of an entry
+ * @write_entry: Writes a new value to an entry
+ * @modify_entry: Does a read/modify/write of an entry
+ * validate_size: Verifies the size of the remote SMEM item to ensure that
+ *                an invalid item size doesn't result in an out-of-bounds
+ *                memory access.
+ */
+struct smp2p_version_if {
+	/* common functions */
+	bool is_supported;
+	uint32_t (*negotiate_features)(uint32_t features);
+	void (*negotiation_complete)(struct smp2p_out_list_item *);
+	void (*find_entry)(struct smp2p_smem __iomem *item,
+			uint32_t entries_total,	char *name,
+			uint32_t **entry_ptr, int *empty_spot);
+
+	/* outbound entry functions */
+	int (*create_entry)(struct msm_smp2p_out *);
+	int (*read_entry)(struct msm_smp2p_out *, uint32_t *);
+	int (*write_entry)(struct msm_smp2p_out *, uint32_t);
+	int (*modify_entry)(struct msm_smp2p_out *, uint32_t, uint32_t, bool);
+
+	/* inbound entry functions */
+	struct smp2p_smem __iomem *(*validate_size)(int remote_pid,
+			struct smp2p_smem __iomem *, uint32_t);
+};
 
-	struct mbox_client mbox_client;
-	struct mbox_chan *mbox_chan;
+static int smp2p_do_negotiation(int remote_pid, struct smp2p_out_list_item *p);
+static void smp2p_send_interrupt(int remote_pid);
+
+/* v0 (uninitialized SMEM item) interface functions */
+static uint32_t smp2p_negotiate_features_v0(uint32_t features);
+static void smp2p_negotiation_complete_v0(struct smp2p_out_list_item *out_item);
+static void smp2p_find_entry_v0(struct smp2p_smem __iomem *item,
+		uint32_t entries_total, char *name, uint32_t **entry_ptr,
+		int *empty_spot);
+static int smp2p_out_create_v0(struct msm_smp2p_out *);
+static int smp2p_out_read_v0(struct msm_smp2p_out *, uint32_t *);
+static int smp2p_out_write_v0(struct msm_smp2p_out *, uint32_t);
+static int smp2p_out_modify_v0(struct msm_smp2p_out *,
+					uint32_t, uint32_t, bool);
+static struct smp2p_smem __iomem *smp2p_in_validate_size_v0(int remote_pid,
+		struct smp2p_smem __iomem *smem_item, uint32_t size);
+
+/* v1 interface functions */
+static uint32_t smp2p_negotiate_features_v1(uint32_t features);
+static void smp2p_negotiation_complete_v1(struct smp2p_out_list_item *out_item);
+static void smp2p_find_entry_v1(struct smp2p_smem __iomem *item,
+		uint32_t entries_total, char *name, uint32_t **entry_ptr,
+		int *empty_spot);
+static int smp2p_out_create_v1(struct msm_smp2p_out *);
+static int smp2p_out_read_v1(struct msm_smp2p_out *, uint32_t *);
+static int smp2p_out_write_v1(struct msm_smp2p_out *, uint32_t);
+static int smp2p_out_modify_v1(struct msm_smp2p_out *,
+					uint32_t, uint32_t, bool);
+static struct smp2p_smem __iomem *smp2p_in_validate_size_v1(int remote_pid,
+		struct smp2p_smem __iomem *smem_item, uint32_t size);
+
+/* Version interface functions */
+static struct smp2p_version_if version_if[] = {
+	[0] = {
+		.negotiate_features = smp2p_negotiate_features_v0,
+		.negotiation_complete = smp2p_negotiation_complete_v0,
+		.find_entry = smp2p_find_entry_v0,
+		.create_entry = smp2p_out_create_v0,
+		.read_entry = smp2p_out_read_v0,
+		.write_entry = smp2p_out_write_v0,
+		.modify_entry = smp2p_out_modify_v0,
+		.validate_size = smp2p_in_validate_size_v0,
+	},
+	[1] = {
+		.is_supported = true,
+		.negotiate_features = smp2p_negotiate_features_v1,
+		.negotiation_complete = smp2p_negotiation_complete_v1,
+		.find_entry = smp2p_find_entry_v1,
+		.create_entry = smp2p_out_create_v1,
+		.read_entry = smp2p_out_read_v1,
+		.write_entry = smp2p_out_write_v1,
+		.modify_entry = smp2p_out_modify_v1,
+		.validate_size = smp2p_in_validate_size_v1,
+	},
+};
 
-	struct list_head inbound;
-	struct list_head outbound;
+/* interrupt configuration (filled by device tree) */
+static struct smp2p_interrupt_config smp2p_int_cfgs[SMP2P_NUM_PROCS] = {
+	[SMP2P_MODEM_PROC].name = "modem",
+	[SMP2P_AUDIO_PROC].name = "lpass",
+	[SMP2P_SENSOR_PROC].name = "dsps",
+	[SMP2P_WIRELESS_PROC].name = "wcnss",
+	[SMP2P_TZ_PROC].name = "tz",
+	[SMP2P_REMOTE_MOCK_PROC].name = "mock",
 };
 
-static void qcom_smp2p_kick(struct qcom_smp2p *smp2p)
+/**
+ * smp2p_get_log_ctx - Return log context for other SMP2P modules.
+ *
+ * @returns: Log context or NULL if none.
+ */
+void *smp2p_get_log_ctx(void)
 {
-	/* Make sure any updated data is written before the kick */
-	wmb();
+	return log_ctx;
+}
 
-	if (smp2p->mbox_chan) {
-		mbox_send_message(smp2p->mbox_chan, NULL);
-		mbox_client_txdone(smp2p->mbox_chan, 0);
-	} else {
-		regmap_write(smp2p->ipc_regmap, smp2p->ipc_offset, BIT(smp2p->ipc_bit));
-	}
+/**
+ * smp2p_get_debug_mask - Return debug mask.
+ *
+ * @returns: Current debug mask.
+ */
+int smp2p_get_debug_mask(void)
+{
+	return smp2p_debug_mask;
 }
 
 /**
- * qcom_smp2p_intr() - interrupt handler for incoming notifications
- * @irq:	unused
- * @data:	smp2p driver context
+ * smp2p_interrupt_config -  Return interrupt configuration.
  *
- * Handle notifications from the remote side to handle newly allocated entries
- * or any changes to the state bits of existing entries.
+ * @returns interrupt configuration array for usage by debugfs.
  */
-static irqreturn_t qcom_smp2p_intr(int irq, void *data)
+struct smp2p_interrupt_config *smp2p_get_interrupt_config(void)
 {
-	struct smp2p_smem_item *in;
-	struct smp2p_entry *entry;
-	struct qcom_smp2p *smp2p = data;
-	unsigned smem_id = smp2p->smem_items[SMP2P_INBOUND];
-	unsigned pid = smp2p->remote_pid;
-	size_t size;
-	int irq_pin;
-	u32 status;
-	char buf[SMP2P_MAX_ENTRY_NAME];
-	u32 val;
-	int i;
+	return smp2p_int_cfgs;
+}
 
-	in = smp2p->in;
+/**
+ * smp2p_pid_to_name -  Lookup name for remote pid.
+ *
+ * @returns: name (may be NULL).
+ */
+const char *smp2p_pid_to_name(int remote_pid)
+{
+	if (remote_pid >= SMP2P_NUM_PROCS)
+		return NULL;
 
-	/* Acquire smem item, if not already found */
-	if (!in) {
-		in = qcom_smem_get(pid, smem_id, &size);
-		if (IS_ERR(in)) {
-			dev_err(smp2p->dev,
-				"Unable to acquire remote smp2p item\n");
-			return IRQ_HANDLED;
-		}
+	return smp2p_int_cfgs[remote_pid].name;
+}
+
+/**
+ * smp2p_get_in_item - Return pointer to remote smem item.
+ *
+ * @remote_pid: Processor ID of the remote system.
+ * @returns:    Pointer to inbound SMEM item
+ *
+ * This is used by debugfs to print the smem items.
+ */
+struct smp2p_smem __iomem *smp2p_get_in_item(int remote_pid)
+{
+	void *ret = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&in_list[remote_pid].in_item_lock_lhb1, flags);
+	if (remote_pid < SMP2P_NUM_PROCS)
+		ret = in_list[remote_pid].smem_edge_in;
+	spin_unlock_irqrestore(&in_list[remote_pid].in_item_lock_lhb1,
+								flags);
+
+	return ret;
+}
+
+/**
+ * smp2p_get_out_item - Return pointer to outbound SMEM item.
+ *
+ * @remote_pid: Processor ID of remote system.
+ * @state:      Edge state of the outbound SMEM item.
+ * @returns:    Pointer to outbound (remote) SMEM item.
+ */
+struct smp2p_smem __iomem *smp2p_get_out_item(int remote_pid, int *state)
+{
+	void *ret = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
+	if (remote_pid < SMP2P_NUM_PROCS) {
+		ret = out_list[remote_pid].smem_edge_out;
+		if (state)
+			*state = out_list[remote_pid].smem_edge_state;
+	}
+	spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1, flags);
 
-		smp2p->in = in;
+	return ret;
+}
+
+/**
+ * smp2p_get_smem_item_id - Return the proper SMEM item ID.
+ *
+ * @write_id:	Processor that will write to the item.
+ * @read_id:    Processor that will read from the item.
+ * @returns:    SMEM ID
+ */
+static int smp2p_get_smem_item_id(int write_pid, int read_pid)
+{
+	int ret = -EINVAL;
+
+	switch (write_pid) {
+	case SMP2P_APPS_PROC:
+		ret = SMEM_SMP2P_APPS_BASE + read_pid;
+		break;
+	case SMP2P_MODEM_PROC:
+		ret = SMEM_SMP2P_MODEM_BASE + read_pid;
+		break;
+	case SMP2P_AUDIO_PROC:
+		ret = SMEM_SMP2P_AUDIO_BASE + read_pid;
+		break;
+	case SMP2P_SENSOR_PROC:
+		ret = SMEM_SMP2P_SENSOR_BASE + read_pid;
+		break;
+	case SMP2P_WIRELESS_PROC:
+		ret = SMEM_SMP2P_WIRLESS_BASE + read_pid;
+		break;
+	case SMP2P_POWER_PROC:
+		ret = SMEM_SMP2P_POWER_BASE + read_pid;
+		break;
+	case SMP2P_TZ_PROC:
+		ret = SMEM_SMP2P_TZ_BASE + read_pid;
+		break;
 	}
 
-	/* Match newly created entries */
-	for (i = smp2p->valid_entries; i < in->valid_entries; i++) {
-		list_for_each_entry(entry, &smp2p->inbound, node) {
-			memcpy(buf, in->entries[i].name, sizeof(buf));
-			if (!strcmp(buf, entry->name)) {
-				entry->value = &in->entries[i].value;
-				break;
+	return ret;
+}
+
+/**
+ * Return pointer to SMEM item owned by the local processor.
+ *
+ * @remote_pid: Remote processor ID
+ * @returns:    NULL for failure; otherwise pointer to SMEM item
+ *
+ * Must be called with out_item_lock_lha1 locked for mock proc.
+ */
+static void *smp2p_get_local_smem_item(int remote_pid)
+{
+	struct smp2p_smem __iomem *item_ptr = NULL;
+
+	if (remote_pid < SMP2P_REMOTE_MOCK_PROC) {
+		unsigned size;
+		int smem_id;
+
+		/* lookup or allocate SMEM item */
+		smem_id = smp2p_get_smem_item_id(SMP2P_APPS_PROC, remote_pid);
+		if (smem_id >= 0) {
+			item_ptr = smem_get_entry(smem_id, &size,
+								remote_pid, 0);
+
+			if (!item_ptr) {
+				size = sizeof(struct smp2p_smem_item);
+				item_ptr = smem_alloc(smem_id, size,
+								remote_pid, 0);
 			}
 		}
+	} else if (remote_pid == SMP2P_REMOTE_MOCK_PROC) {
+		/*
+		 * This path is only used during unit testing so
+		 * the GFP_ATOMIC allocation should not be a
+		 * concern.
+		 */
+		if (!out_list[SMP2P_REMOTE_MOCK_PROC].smem_edge_out)
+			item_ptr = kzalloc(
+					sizeof(struct smp2p_smem_item),
+					GFP_ATOMIC);
 	}
-	smp2p->valid_entries = i;
+	return item_ptr;
+}
 
-	/* Fire interrupts based on any value changes */
-	list_for_each_entry(entry, &smp2p->inbound, node) {
-		/* Ignore entries not yet allocated by the remote side */
-		if (!entry->value)
-			continue;
+/**
+ * smp2p_get_remote_smem_item - Return remote SMEM item.
+ *
+ * @remote_pid: Remote processor ID
+ * @out_item:   Pointer to the output item structure
+ * @returns:    NULL for failure; otherwise pointer to SMEM item
+ *
+ * Return pointer to SMEM item owned by the remote processor.
+ *
+ * Note that this function does an SMEM lookup which uses a remote spinlock,
+ * so this function should not be called more than necessary.
+ *
+ * Must be called with out_item_lock_lha1 and in_item_lock_lhb1 locked.
+ */
+static void *smp2p_get_remote_smem_item(int remote_pid,
+	struct smp2p_out_list_item *out_item)
+{
+	void *item_ptr = NULL;
+	unsigned size = 0;
 
-		val = readl(entry->value);
+	if (!out_item)
+		return item_ptr;
 
-		status = val ^ entry->last_value;
-		entry->last_value = val;
+	if (remote_pid < SMP2P_REMOTE_MOCK_PROC) {
+		int smem_id;
 
-		/* No changes of this entry? */
-		if (!status)
-			continue;
+		smem_id = smp2p_get_smem_item_id(remote_pid, SMP2P_APPS_PROC);
+		if (smem_id >= 0)
+			item_ptr = smem_get_entry(smem_id, &size,
+								remote_pid, 0);
+	} else if (remote_pid == SMP2P_REMOTE_MOCK_PROC) {
+		item_ptr = msm_smp2p_get_remote_mock_smem_item(&size);
+	}
+	item_ptr = out_item->ops_ptr->validate_size(remote_pid, item_ptr, size);
 
-		for_each_set_bit(i, entry->irq_enabled, 32) {
-			if (!(status & BIT(i)))
-				continue;
+	return item_ptr;
+}
 
-			if ((val & BIT(i) && test_bit(i, entry->irq_rising)) ||
-			    (!(val & BIT(i)) && test_bit(i, entry->irq_falling))) {
-				irq_pin = irq_find_mapping(entry->domain, i);
-				handle_nested_irq(irq_pin);
-			}
-		}
-	}
+/**
+ * smp2p_ssr_ack_needed - Returns true if SSR ACK required
+ *
+ * @rpid: Remote processor ID
+ *
+ * Must be called with out_item_lock_lha1 and in_item_lock_lhb1 locked.
+ */
+static bool smp2p_ssr_ack_needed(uint32_t rpid)
+{
+	bool ssr_done;
 
-	return IRQ_HANDLED;
+	if (!out_list[rpid].feature_ssr_ack_enabled)
+		return false;
+
+	ssr_done = SMP2P_GET_RESTART_DONE(in_list[rpid].smem_edge_in->flags);
+	if (ssr_done != out_list[rpid].restart_ack)
+		return true;
+
+	return false;
 }
 
-static void smp2p_mask_irq(struct irq_data *irqd)
+/**
+ * smp2p_do_ssr_ack - Handles SSR ACK
+ *
+ * @rpid: Remote processor ID
+ *
+ * Must be called with out_item_lock_lha1 and in_item_lock_lhb1 locked.
+ */
+static void smp2p_do_ssr_ack(uint32_t rpid)
 {
-	struct smp2p_entry *entry = irq_data_get_irq_chip_data(irqd);
-	irq_hw_number_t irq = irqd_to_hwirq(irqd);
+	bool ack;
+
+	if (!smp2p_ssr_ack_needed(rpid))
+		return;
 
-	clear_bit(irq, entry->irq_enabled);
+	ack = !out_list[rpid].restart_ack;
+	SMP2P_INFO("%s: ssr ack pid %d: %d -> %d\n", __func__, rpid,
+			out_list[rpid].restart_ack, ack);
+	out_list[rpid].restart_ack = ack;
+	SMP2P_SET_RESTART_ACK(out_list[rpid].smem_edge_out->flags, ack);
+	smp2p_send_interrupt(rpid);
 }
 
-static void smp2p_unmask_irq(struct irq_data *irqd)
+/**
+ * smp2p_negotiate_features_v1 - Initial feature negotiation.
+ *
+ * @features: Inbound feature set.
+ * @returns: Supported features (will be a same/subset of @features).
+ */
+static uint32_t smp2p_negotiate_features_v1(uint32_t features)
 {
-	struct smp2p_entry *entry = irq_data_get_irq_chip_data(irqd);
-	irq_hw_number_t irq = irqd_to_hwirq(irqd);
+	return SMP2P_FEATURE_SSR_ACK;
+}
 
-	set_bit(irq, entry->irq_enabled);
+/**
+ * smp2p_negotiation_complete_v1 - Negotiation completed
+ *
+ * @out_item:   Pointer to the output item structure
+ *
+ * Can be used to do final configuration based upon the negotiated feature set.
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static void smp2p_negotiation_complete_v1(struct smp2p_out_list_item *out_item)
+{
+	uint32_t features;
+
+	features = SMP2P_GET_FEATURES(out_item->smem_edge_out->feature_version);
+
+	if (features & SMP2P_FEATURE_SSR_ACK)
+		out_item->feature_ssr_ack_enabled = true;
 }
 
-static int smp2p_set_irq_type(struct irq_data *irqd, unsigned int type)
+/**
+ * smp2p_find_entry_v1 - Search for an entry in SMEM item.
+ *
+ * @item: Pointer to the smem item.
+ * @entries_total: Total number of entries in @item.
+ * @name: Name of the entry.
+ * @entry_ptr: Set to pointer of entry if found, NULL otherwise.
+ * @empty_spot: If non-null, set to the value of the next empty entry.
+ *
+ * Searches for entry @name in the SMEM item.  If found, a pointer
+ * to the item is returned.  If it isn't found, the first empty
+ * index is returned in @empty_spot.
+ */
+static void smp2p_find_entry_v1(struct smp2p_smem __iomem *item,
+		uint32_t entries_total, char *name, uint32_t **entry_ptr,
+		int *empty_spot)
 {
-	struct smp2p_entry *entry = irq_data_get_irq_chip_data(irqd);
-	irq_hw_number_t irq = irqd_to_hwirq(irqd);
+	int i;
+	struct smp2p_entry_v1 *pos;
+	char entry_name[SMP2P_MAX_ENTRY_NAME];
+
+	if (!item || !name || !entry_ptr) {
+		SMP2P_ERR("%s: invalid arguments %d %d %d\n",
+				__func__, !item, !name, !entry_ptr);
+		return;
+	}
+
+	*entry_ptr = NULL;
+	if (empty_spot)
+		*empty_spot = -1;
+
+	pos = (struct smp2p_entry_v1 *)(char *)(item + 1);
+	for (i = 0; i < entries_total; i++, ++pos) {
+		memcpy_fromio(entry_name, pos->name, SMP2P_MAX_ENTRY_NAME);
+		if (entry_name[0]) {
+			if (!strcmp(entry_name, name)) {
+				*entry_ptr = &pos->entry;
+				break;
+			}
+		} else if (empty_spot && *empty_spot < 0) {
+			*empty_spot = i;
+		}
+	}
+}
 
-	if (!(type & IRQ_TYPE_EDGE_BOTH))
+/**
+ * smp2p_out_create_v1 - Creates a outbound SMP2P entry.
+ *
+ * @out_entry: Pointer to the SMP2P entry structure.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static int smp2p_out_create_v1(struct msm_smp2p_out *out_entry)
+{
+	struct smp2p_smem __iomem *smp2p_h_ptr;
+	struct smp2p_out_list_item *p_list;
+	uint32_t *state_entry_ptr;
+	uint32_t empty_spot;
+	uint32_t entries_total;
+	uint32_t entries_valid;
+
+	if (!out_entry)
 		return -EINVAL;
 
-	if (type & IRQ_TYPE_EDGE_RISING)
-		set_bit(irq, entry->irq_rising);
-	else
-		clear_bit(irq, entry->irq_rising);
+	p_list = &out_list[out_entry->remote_pid];
+	if (p_list->smem_edge_state != SMP2P_EDGE_STATE_OPENED) {
+		SMP2P_ERR("%s: item '%s':%d opened - wrong create called\n",
+			__func__, out_entry->name, out_entry->remote_pid);
+		return -ENODEV;
+	}
 
-	if (type & IRQ_TYPE_EDGE_FALLING)
-		set_bit(irq, entry->irq_falling);
-	else
-		clear_bit(irq, entry->irq_falling);
+	smp2p_h_ptr = p_list->smem_edge_out;
+	entries_total = SMP2P_GET_ENT_TOTAL(smp2p_h_ptr->valid_total_ent);
+	entries_valid = SMP2P_GET_ENT_VALID(smp2p_h_ptr->valid_total_ent);
+
+	p_list->ops_ptr->find_entry(smp2p_h_ptr, entries_total,
+			out_entry->name, &state_entry_ptr, &empty_spot);
+	if (state_entry_ptr) {
+		/* re-use existing entry */
+		out_entry->l_smp2p_entry = state_entry_ptr;
+
+		SMP2P_DBG("%s: item '%s':%d reused\n", __func__,
+				out_entry->name, out_entry->remote_pid);
+	} else if (entries_valid >= entries_total) {
+		/* need to allocate entry, but not more space */
+		SMP2P_ERR("%s: no space for item '%s':%d\n",
+			__func__, out_entry->name, out_entry->remote_pid);
+		return -ENOMEM;
+	} else {
+		/* allocate a new entry */
+		struct smp2p_entry_v1 *entry_ptr;
+
+		entry_ptr = (struct smp2p_entry_v1 *)((char *)(smp2p_h_ptr + 1)
+			+ empty_spot * sizeof(struct smp2p_entry_v1));
+		memcpy_toio(entry_ptr->name, out_entry->name,
+						sizeof(entry_ptr->name));
+		out_entry->l_smp2p_entry = &entry_ptr->entry;
+		++entries_valid;
+		SMP2P_DBG("%s: item '%s':%d fully created as entry %d of %d\n",
+				__func__, out_entry->name,
+				out_entry->remote_pid,
+				entries_valid, entries_total);
+		SMP2P_SET_ENT_VALID(smp2p_h_ptr->valid_total_ent,
+				entries_valid);
+		smp2p_send_interrupt(out_entry->remote_pid);
+	}
+	raw_notifier_call_chain(&out_entry->msm_smp2p_notifier_list,
+		  SMP2P_OPEN, 0);
 
 	return 0;
 }
 
-static struct irq_chip smp2p_irq_chip = {
-	.name           = "smp2p",
-	.irq_mask       = smp2p_mask_irq,
-	.irq_unmask     = smp2p_unmask_irq,
-	.irq_set_type	= smp2p_set_irq_type,
-};
+/**
+ * smp2p_out_read_v1 -  Read the data from an outbound entry.
+ *
+ * @out_entry: Pointer to the SMP2P entry structure.
+ * @data: Out pointer, the data is available in this argument on success.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static int smp2p_out_read_v1(struct msm_smp2p_out *out_entry, uint32_t *data)
+{
+	struct smp2p_smem __iomem  *smp2p_h_ptr;
+	uint32_t remote_pid;
+
+	if (!out_entry)
+		return -EINVAL;
+
+	smp2p_h_ptr = out_list[out_entry->remote_pid].smem_edge_out;
+	remote_pid = SMP2P_GET_REMOTE_PID(smp2p_h_ptr->rem_loc_proc_id);
+
+	if (remote_pid != out_entry->remote_pid)
+		return -EINVAL;
 
-static int smp2p_irq_map(struct irq_domain *d,
-			 unsigned int irq,
-			 irq_hw_number_t hw)
+	if (out_entry->l_smp2p_entry) {
+		*data = readl_relaxed(out_entry->l_smp2p_entry);
+	} else {
+		SMP2P_ERR("%s: '%s':%d not yet OPEN\n", __func__,
+				out_entry->name, remote_pid);
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+/**
+ * smp2p_out_write_v1 - Writes an outbound entry value.
+ *
+ * @out_entry: Pointer to the SMP2P entry structure.
+ * @data: The data to be written.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static int smp2p_out_write_v1(struct msm_smp2p_out *out_entry, uint32_t data)
 {
-	struct smp2p_entry *entry = d->host_data;
+	struct smp2p_smem __iomem  *smp2p_h_ptr;
+	uint32_t remote_pid;
+
+	if (!out_entry)
+		return -EINVAL;
+
+	smp2p_h_ptr = out_list[out_entry->remote_pid].smem_edge_out;
+	remote_pid = SMP2P_GET_REMOTE_PID(smp2p_h_ptr->rem_loc_proc_id);
 
-	irq_set_chip_and_handler(irq, &smp2p_irq_chip, handle_level_irq);
-	irq_set_chip_data(irq, entry);
-	irq_set_nested_thread(irq, 1);
-	irq_set_noprobe(irq);
+	if (remote_pid != out_entry->remote_pid)
+		return -EINVAL;
 
+	if (out_entry->l_smp2p_entry) {
+		writel_relaxed(data, out_entry->l_smp2p_entry);
+		smp2p_send_interrupt(remote_pid);
+	} else {
+		SMP2P_ERR("%s: '%s':%d not yet OPEN\n", __func__,
+				out_entry->name, remote_pid);
+		return -ENODEV;
+	}
 	return 0;
 }
 
-static const struct irq_domain_ops smp2p_irq_ops = {
-	.map = smp2p_irq_map,
-	.xlate = irq_domain_xlate_twocell,
-};
+/**
+ * smp2p_out_modify_v1 - Modifies and outbound value.
+ *
+ * @set_mask:  Mask containing the bits that needs to be set.
+ * @clear_mask: Mask containing the bits that needs to be cleared.
+ * @send_irq: Flag to send interrupt to remote processor.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * The clear mask is applied first, so  if a bit is set in both clear and
+ * set mask, the result will be that the bit is set.
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static int smp2p_out_modify_v1(struct msm_smp2p_out *out_entry,
+		uint32_t set_mask, uint32_t clear_mask, bool send_irq)
+{
+	struct smp2p_smem __iomem  *smp2p_h_ptr;
+	uint32_t remote_pid;
+
+	if (!out_entry)
+		return -EINVAL;
+
+	smp2p_h_ptr = out_list[out_entry->remote_pid].smem_edge_out;
+	remote_pid = SMP2P_GET_REMOTE_PID(smp2p_h_ptr->rem_loc_proc_id);
 
-static int qcom_smp2p_inbound_entry(struct qcom_smp2p *smp2p,
-				    struct smp2p_entry *entry,
-				    struct device_node *node)
+	if (remote_pid != out_entry->remote_pid)
+			return -EINVAL;
+
+	if (out_entry->l_smp2p_entry) {
+		uint32_t curr_value;
+
+		curr_value = readl_relaxed(out_entry->l_smp2p_entry);
+		writel_relaxed((curr_value & ~clear_mask) | set_mask,
+			out_entry->l_smp2p_entry);
+	} else {
+		SMP2P_ERR("%s: '%s':%d not yet OPEN\n", __func__,
+				out_entry->name, remote_pid);
+		return -ENODEV;
+	}
+
+	if (send_irq)
+		smp2p_send_interrupt(remote_pid);
+	return 0;
+}
+
+/**
+ * smp2p_in_validate_size_v1 - Size validation for version 1.
+ *
+ * @remote_pid: Remote processor ID.
+ * @smem_item:  Pointer to the inbound SMEM item.
+ * @size:       Size of the SMEM item.
+ * @returns:    Validated smem_item pointer (or NULL if size is too small).
+ *
+ * Validates we don't end up with out-of-bounds array access due to invalid
+ * smem item size.  If out-of-bound array access can't be avoided, then an
+ * error message is printed and NULL is returned to prevent usage of the
+ * item.
+ *
+ * Must be called with in_item_lock_lhb1 locked.
+ */
+static struct smp2p_smem __iomem *smp2p_in_validate_size_v1(int remote_pid,
+		struct smp2p_smem __iomem *smem_item, uint32_t size)
 {
-	entry->domain = irq_domain_add_linear(node, 32, &smp2p_irq_ops, entry);
-	if (!entry->domain) {
-		dev_err(smp2p->dev, "failed to add irq_domain\n");
-		return -ENOMEM;
+	uint32_t total_entries;
+	unsigned expected_size;
+	struct smp2p_smem __iomem *item_ptr;
+	struct smp2p_in_list_item *in_item;
+
+	if (remote_pid >= SMP2P_NUM_PROCS || !smem_item)
+		return NULL;
+
+	in_item = &in_list[remote_pid];
+	item_ptr = (struct smp2p_smem __iomem *)smem_item;
+
+	total_entries = SMP2P_GET_ENT_TOTAL(item_ptr->valid_total_ent);
+	if (total_entries > 0) {
+		in_item->safe_total_entries = total_entries;
+		in_item->item_size = size;
+
+		expected_size =	sizeof(struct smp2p_smem) +
+			(total_entries * sizeof(struct smp2p_entry_v1));
+
+		if (size < expected_size) {
+			unsigned new_size;
+
+			new_size = size;
+			new_size -= sizeof(struct smp2p_smem);
+			new_size /= sizeof(struct smp2p_entry_v1);
+			in_item->safe_total_entries = new_size;
+
+			SMP2P_ERR(
+				"%s pid %d item too small for %d entries; expected: %d actual: %d; reduced to %d entries\n",
+				__func__, remote_pid, total_entries,
+				expected_size, size, new_size);
+		}
+	} else {
+		/*
+		 * Total entries is 0, so the entry is still being initialized
+		 * or is invalid.  Either way, treat it as if the item does
+		 * not exist yet.
+		 */
+		in_item->safe_total_entries = 0;
+		in_item->item_size = 0;
 	}
+	return item_ptr;
+}
 
+/**
+ * smp2p_negotiate_features_v0 - Initial feature negotiation.
+ *
+ * @features: Inbound feature set.
+ * @returns: 0 (no features supported for v0).
+ */
+static uint32_t smp2p_negotiate_features_v0(uint32_t features)
+{
+	/* no supported features */
 	return 0;
 }
 
-static int smp2p_update_bits(void *data, u32 mask, u32 value)
+/**
+ * smp2p_negotiation_complete_v0 - Negotiation completed
+ *
+ * @out_item:   Pointer to the output item structure
+ *
+ * Can be used to do final configuration based upon the negotiated feature set.
+ */
+static void smp2p_negotiation_complete_v0(struct smp2p_out_list_item *out_item)
+{
+	SMP2P_ERR("%s: invalid negotiation complete for v0 pid %d\n",
+		__func__,
+		SMP2P_GET_REMOTE_PID(out_item->smem_edge_out->rem_loc_proc_id));
+}
+
+/**
+ * smp2p_find_entry_v0 - Stub function.
+ *
+ * @item: Pointer to the smem item.
+ * @entries_total: Total number of entries in @item.
+ * @name: Name of the entry.
+ * @entry_ptr: Set to pointer of entry if found, NULL otherwise.
+ * @empty_spot: If non-null, set to the value of the next empty entry.
+ *
+ * Entries cannot be searched for until item negotiation has been completed.
+ */
+static void smp2p_find_entry_v0(struct smp2p_smem __iomem *item,
+		uint32_t entries_total, char *name, uint32_t **entry_ptr,
+		int *empty_spot)
 {
-	struct smp2p_entry *entry = data;
-	u32 orig;
-	u32 val;
+	if (entry_ptr)
+		*entry_ptr = NULL;
+
+	if (empty_spot)
+		*empty_spot = -1;
+
+	SMP2P_ERR("%s: invalid - item negotiation incomplete\n", __func__);
+}
 
-	spin_lock(&entry->lock);
-	val = orig = readl(entry->value);
-	val &= ~mask;
-	val |= value;
-	writel(val, entry->value);
-	spin_unlock(&entry->lock);
+/**
+ * smp2p_out_create_v0 - Initial creation function.
+ *
+ * @out_entry: Pointer to the SMP2P entry structure.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * If the outbound SMEM item negotiation is not complete, then
+ * this function is called to start the negotiation process.
+ * Eventually when the negotiation process is complete, this
+ * function pointer is switched with the appropriate function
+ * for the version of SMP2P being created.
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static int smp2p_out_create_v0(struct msm_smp2p_out *out_entry)
+{
+	int edge_state;
+	struct smp2p_out_list_item *item_ptr;
 
-	if (val != orig)
-		qcom_smp2p_kick(entry->smp2p);
+	if (!out_entry)
+		return -EINVAL;
 
+	edge_state = out_list[out_entry->remote_pid].smem_edge_state;
+
+	switch (edge_state) {
+	case SMP2P_EDGE_STATE_CLOSED:
+		/* start negotiation */
+		item_ptr = &out_list[out_entry->remote_pid];
+		edge_state = smp2p_do_negotiation(out_entry->remote_pid,
+				item_ptr);
+		break;
+
+	case SMP2P_EDGE_STATE_OPENING:
+		/* still negotiating */
+		break;
+
+	case SMP2P_EDGE_STATE_OPENED:
+		SMP2P_ERR("%s: item '%s':%d opened - wrong create called\n",
+			__func__, out_entry->name, out_entry->remote_pid);
+		break;
+
+	default:
+		SMP2P_ERR("%s: item '%s':%d invalid SMEM item state %d\n",
+			__func__, out_entry->name, out_entry->remote_pid,
+			edge_state);
+		break;
+	}
 	return 0;
 }
 
-static const struct qcom_smem_state_ops smp2p_state_ops = {
-	.update_bits = smp2p_update_bits,
-};
+/**
+ * smp2p_out_read_v0 - Stub function.
+ *
+ * @out_entry: Pointer to the SMP2P entry structure.
+ * @data: Out pointer, the data is available in this argument on success.
+ * @returns: -ENODEV
+ */
+static int smp2p_out_read_v0(struct msm_smp2p_out *out_entry, uint32_t *data)
+{
+	SMP2P_ERR("%s: item '%s':%d not OPEN\n",
+		__func__, out_entry->name, out_entry->remote_pid);
+
+	return -ENODEV;
+}
+
+/**
+ * smp2p_out_write_v0 - Stub function.
+ *
+ * @out_entry: Pointer to the SMP2P entry structure.
+ * @data: The data to be written.
+ * @returns: -ENODEV
+ */
+static int smp2p_out_write_v0(struct msm_smp2p_out *out_entry, uint32_t data)
+{
+	SMP2P_ERR("%s: item '%s':%d not yet OPEN\n",
+		__func__, out_entry->name, out_entry->remote_pid);
+
+	return -ENODEV;
+}
 
-static int qcom_smp2p_outbound_entry(struct qcom_smp2p *smp2p,
-				     struct smp2p_entry *entry,
-				     struct device_node *node)
+/**
+ * smp2p_out_modify_v0 - Stub function.
+ *
+ * @set_mask:  Mask containing the bits that needs to be set.
+ * @clear_mask: Mask containing the bits that needs to be cleared.
+ * @send_irq: Flag to send interrupt to remote processor.
+ * @returns: -ENODEV
+ */
+static int smp2p_out_modify_v0(struct msm_smp2p_out *out_entry,
+		uint32_t set_mask, uint32_t clear_mask, bool send_irq)
 {
-	struct smp2p_smem_item *out = smp2p->out;
-	char buf[SMP2P_MAX_ENTRY_NAME] = {};
+	SMP2P_ERR("%s: item '%s':%d not yet OPEN\n",
+		__func__, out_entry->name, out_entry->remote_pid);
+
+	return -ENODEV;
+}
 
-	/* Allocate an entry from the smem item */
-	strlcpy(buf, entry->name, SMP2P_MAX_ENTRY_NAME);
-	memcpy(out->entries[out->valid_entries].name, buf, SMP2P_MAX_ENTRY_NAME);
+/**
+ * smp2p_in_validate_size_v0 - Stub function.
+ *
+ * @remote_pid: Remote processor ID.
+ * @smem_item:  Pointer to the inbound SMEM item.
+ * @size:       Size of the SMEM item.
+ * @returns:    Validated smem_item pointer (or NULL if size is too small).
+ *
+ * Validates we don't end up with out-of-bounds array access due to invalid
+ * smem item size.  If out-of-bound array access can't be avoided, then an
+ * error message is printed and NULL is returned to prevent usage of the
+ * item.
+ *
+ * Must be called with in_item_lock_lhb1 locked.
+ */
+static struct smp2p_smem __iomem *smp2p_in_validate_size_v0(int remote_pid,
+		struct smp2p_smem __iomem *smem_item, uint32_t size)
+{
+	struct smp2p_in_list_item *in_item;
 
-	/* Make the logical entry reference the physical value */
-	entry->value = &out->entries[out->valid_entries].value;
+	if (remote_pid >= SMP2P_NUM_PROCS || !smem_item)
+		return NULL;
 
-	out->valid_entries++;
+	in_item = &in_list[remote_pid];
 
-	entry->state = qcom_smem_state_register(node, &smp2p_state_ops, entry);
-	if (IS_ERR(entry->state)) {
-		dev_err(smp2p->dev, "failed to register qcom_smem_state\n");
-		return PTR_ERR(entry->state);
+	if (size < sizeof(struct smp2p_smem)) {
+		SMP2P_ERR(
+			"%s pid %d item size too small; expected: %zu actual: %d\n",
+			__func__, remote_pid,
+			sizeof(struct smp2p_smem), size);
+		smem_item = NULL;
+		in_item->item_size = 0;
+	} else {
+		in_item->item_size = size;
 	}
+	return smem_item;
+}
 
-	return 0;
+/**
+ * smp2p_init_header - Initializes the header of the smem item.
+ *
+ * @header_ptr: Pointer to the smp2p header.
+ * @local_pid: Local processor ID.
+ * @remote_pid: Remote processor ID.
+ * @feature: Features of smp2p implementation.
+ * @version: Version of smp2p implementation.
+ *
+ * Initializes the header as defined in the protocol specification.
+ */
+void smp2p_init_header(struct smp2p_smem __iomem *header_ptr,
+		int local_pid, int remote_pid,
+		uint32_t features, uint32_t version)
+{
+	header_ptr->magic = SMP2P_MAGIC;
+	SMP2P_SET_LOCAL_PID(header_ptr->rem_loc_proc_id, local_pid);
+	SMP2P_SET_REMOTE_PID(header_ptr->rem_loc_proc_id, remote_pid);
+	SMP2P_SET_FEATURES(header_ptr->feature_version, features);
+	SMP2P_SET_ENT_TOTAL(header_ptr->valid_total_ent, SMP2P_MAX_ENTRY);
+	SMP2P_SET_ENT_VALID(header_ptr->valid_total_ent, 0);
+	header_ptr->flags = 0;
+
+	/* ensure that all fields are valid before version is written */
+	wmb();
+	SMP2P_SET_VERSION(header_ptr->feature_version, version);
 }
 
-static int qcom_smp2p_alloc_outbound_item(struct qcom_smp2p *smp2p)
+/**
+ * smp2p_do_negotiation - Implements negotiation algorithm.
+ *
+ * @remote_pid: Remote processor ID.
+ * @out_item: Pointer to the outbound list item.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Must be called with out_item_lock_lha1 locked.  Will internally lock
+ * in_item_lock_lhb1.
+ */
+static int smp2p_do_negotiation(int remote_pid,
+		struct smp2p_out_list_item *out_item)
 {
-	struct smp2p_smem_item *out;
-	unsigned smem_id = smp2p->smem_items[SMP2P_OUTBOUND];
-	unsigned pid = smp2p->remote_pid;
-	int ret;
+	struct smp2p_smem __iomem *r_smem_ptr;
+	struct smp2p_smem __iomem *l_smem_ptr;
+	uint32_t r_version;
+	uint32_t r_feature;
+	uint32_t l_version, l_feature;
+	int prev_state;
+
+	if (remote_pid >= SMP2P_NUM_PROCS || !out_item)
+		return -EINVAL;
+	if (out_item->smem_edge_state == SMP2P_EDGE_STATE_FAILED)
+		return -EPERM;
+
+	prev_state = out_item->smem_edge_state;
+
+	/* create local item */
+	if (!out_item->smem_edge_out) {
+		out_item->smem_edge_out = smp2p_get_local_smem_item(remote_pid);
+		if (!out_item->smem_edge_out) {
+			SMP2P_ERR(
+				"%s unable to allocate SMEM item for pid %d\n",
+				__func__, remote_pid);
+			return -ENODEV;
+		}
+		out_item->smem_edge_state = SMP2P_EDGE_STATE_OPENING;
+	}
+	l_smem_ptr = out_item->smem_edge_out;
 
-	ret = qcom_smem_alloc(pid, smem_id, sizeof(*out));
-	if (ret < 0 && ret != -EEXIST) {
-		if (ret != -EPROBE_DEFER)
-			dev_err(smp2p->dev,
-				"unable to allocate local smp2p item\n");
-		return ret;
+	/* retrieve remote side and version */
+	spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+	r_smem_ptr = smp2p_get_remote_smem_item(remote_pid, out_item);
+	spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+
+	r_version = 0;
+	if (r_smem_ptr) {
+		r_version = SMP2P_GET_VERSION(r_smem_ptr->feature_version);
+		r_feature = SMP2P_GET_FEATURES(r_smem_ptr->feature_version);
 	}
 
-	out = qcom_smem_get(pid, smem_id, NULL);
-	if (IS_ERR(out)) {
-		dev_err(smp2p->dev, "Unable to acquire local smp2p item\n");
-		return PTR_ERR(out);
+	if (r_version == 0) {
+		/*
+		 * Either remote side doesn't exist, or is in the
+		 * process of being initialized (the version is set last).
+		 *
+		 * In either case, treat as if the other side doesn't exist
+		 * and write out our maximum supported version.
+		 */
+		r_smem_ptr = NULL;
+		r_version = ARRAY_SIZE(version_if) - 1;
+		r_feature = ~0U;
 	}
 
-	memset(out, 0, sizeof(*out));
-	out->magic = SMP2P_MAGIC;
-	out->local_pid = smp2p->local_pid;
-	out->remote_pid = smp2p->remote_pid;
-	out->total_entries = SMP2P_MAX_ENTRY;
-	out->valid_entries = 0;
+	/* find maximum supported version and feature set */
+	l_version = min(r_version, (uint32_t)ARRAY_SIZE(version_if) - 1);
+	for (; l_version > 0; --l_version) {
+		if (!version_if[l_version].is_supported)
+			continue;
 
-	/*
-	 * Make sure the rest of the header is written before we validate the
-	 * item by writing a valid version number.
-	 */
-	wmb();
-	out->version = 1;
+		/* found valid version */
+		l_feature = version_if[l_version].negotiate_features(~0U);
+		if (l_version == r_version)
+			l_feature &= r_feature;
+		break;
+	}
 
-	qcom_smp2p_kick(smp2p);
+	if (l_version == 0) {
+		SMP2P_ERR(
+			"%s: negotiation failure pid %d: RV %d RF %x\n",
+			__func__, remote_pid, r_version, r_feature
+			);
+		SMP2P_SET_VERSION(l_smem_ptr->feature_version,
+			SMP2P_EDGE_STATE_FAILED);
+		smp2p_send_interrupt(remote_pid);
+		out_item->smem_edge_state = SMP2P_EDGE_STATE_FAILED;
+		return -EPERM;
+	}
 
-	smp2p->out = out;
+	/* update header and notify remote side */
+	smp2p_init_header(l_smem_ptr, SMP2P_APPS_PROC, remote_pid,
+		l_feature, l_version);
+	smp2p_send_interrupt(remote_pid);
+
+	/* handle internal state changes */
+	if (r_smem_ptr && l_version == r_version &&
+			l_feature == r_feature) {
+		struct msm_smp2p_out *pos;
+
+		/* negotiation complete */
+		out_item->ops_ptr = &version_if[l_version];
+		out_item->ops_ptr->negotiation_complete(out_item);
+		out_item->smem_edge_state = SMP2P_EDGE_STATE_OPENED;
+		SMP2P_INFO(
+			"%s: negotiation complete pid %d: State %d->%d F0x%08x\n",
+			__func__, remote_pid, prev_state,
+			out_item->smem_edge_state, l_feature);
+
+		/* create any pending outbound entries */
+		list_for_each_entry(pos, &out_item->list, out_edge_list) {
+			out_item->ops_ptr->create_entry(pos);
+		}
 
+		/* update inbound edge */
+		spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+		(void)out_item->ops_ptr->validate_size(remote_pid, r_smem_ptr,
+				in_list[remote_pid].item_size);
+		in_list[remote_pid].smem_edge_in = r_smem_ptr;
+		spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+	} else {
+		SMP2P_INFO("%s: negotiation pid %d: State %d->%d F0x%08x\n",
+			__func__, remote_pid, prev_state,
+			out_item->smem_edge_state, l_feature);
+	}
 	return 0;
 }
 
-static int smp2p_parse_ipc(struct qcom_smp2p *smp2p)
+/**
+ * msm_smp2p_out_open - Opens an outbound entry.
+ *
+ * @remote_pid: Outbound processor ID.
+ * @name: Name of the entry.
+ * @open_notifier: Notifier block for the open notification.
+ * @handle: Handle to the smem entry structure.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Opens an outbound entry with the name specified by entry, from the
+ * local processor to the remote processor(remote_pid). If the entry, remote_pid
+ * and open_notifier are valid, then handle will be set and zero will be
+ * returned. The smem item that holds this entry will be created if it has
+ * not been created according to the version negotiation algorithm.
+ * The open_notifier will be used to notify the clients about the
+ * availability of the entry.
+ */
+int msm_smp2p_out_open(int remote_pid, const char *name,
+				   struct notifier_block *open_notifier,
+				   struct msm_smp2p_out **handle)
 {
-	struct device_node *syscon;
-	struct device *dev = smp2p->dev;
-	const char *key;
-	int ret;
+	struct msm_smp2p_out *out_entry;
+	struct msm_smp2p_out *pos;
+	int ret = 0;
+	unsigned long flags;
 
-	syscon = of_parse_phandle(dev->of_node, "qcom,ipc", 0);
-	if (!syscon) {
-		dev_err(dev, "no qcom,ipc node\n");
-		return -ENODEV;
+	if (handle)
+		*handle = NULL;
+
+	if (remote_pid >= SMP2P_NUM_PROCS || !name || !open_notifier || !handle)
+		return -EINVAL;
+
+	if ((remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+						__func__, remote_pid, name);
+		return -EPROBE_DEFER;
 	}
 
-	smp2p->ipc_regmap = syscon_node_to_regmap(syscon);
-	if (IS_ERR(smp2p->ipc_regmap))
-		return PTR_ERR(smp2p->ipc_regmap);
+	/* Allocate the smp2p object and node */
+	out_entry = kzalloc(sizeof(*out_entry), GFP_KERNEL);
+	if (!out_entry)
+		return -ENOMEM;
 
-	key = "qcom,ipc";
-	ret = of_property_read_u32_index(dev->of_node, key, 1, &smp2p->ipc_offset);
-	if (ret < 0) {
-		dev_err(dev, "no offset in %s\n", key);
-		return -EINVAL;
+	/* Handle duplicate registration */
+	spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
+	list_for_each_entry(pos, &out_list[remote_pid].list,
+			out_edge_list) {
+		if (!strcmp(pos->name, name)) {
+			spin_unlock_irqrestore(
+				&out_list[remote_pid].out_item_lock_lha1,
+				flags);
+			kfree(out_entry);
+			SMP2P_ERR("%s: duplicate registration '%s':%d\n",
+				__func__, name, remote_pid);
+			return -EBUSY;
+		}
 	}
 
-	ret = of_property_read_u32_index(dev->of_node, key, 2, &smp2p->ipc_bit);
-	if (ret < 0) {
-		dev_err(dev, "no bit in %s\n", key);
+	out_entry->remote_pid = remote_pid;
+	RAW_INIT_NOTIFIER_HEAD(&out_entry->msm_smp2p_notifier_list);
+	strlcpy(out_entry->name, name, SMP2P_MAX_ENTRY_NAME);
+	out_entry->open_nb = open_notifier;
+	raw_notifier_chain_register(&out_entry->msm_smp2p_notifier_list,
+		  out_entry->open_nb);
+	list_add(&out_entry->out_edge_list, &out_list[remote_pid].list);
+
+	ret = out_list[remote_pid].ops_ptr->create_entry(out_entry);
+	if (ret) {
+		list_del(&out_entry->out_edge_list);
+		raw_notifier_chain_unregister(
+			&out_entry->msm_smp2p_notifier_list,
+			out_entry->open_nb);
+		spin_unlock_irqrestore(
+			&out_list[remote_pid].out_item_lock_lha1, flags);
+		kfree(out_entry);
+		SMP2P_ERR("%s: unable to open '%s':%d error %d\n",
+				__func__, name, remote_pid, ret);
+		return ret;
+	}
+	spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
+			flags);
+	*handle = out_entry;
+
+	return 0;
+}
+EXPORT_SYMBOL(msm_smp2p_out_open);
+
+/**
+ * msm_smp2p_out_close - Closes the handle to an outbound entry.
+ *
+ * @handle: Pointer to smp2p out entry handle.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * The actual entry will not be deleted and can be re-opened at a later
+ * time.  The handle will be set to NULL.
+ */
+int msm_smp2p_out_close(struct msm_smp2p_out **handle)
+{
+	unsigned long flags;
+	struct msm_smp2p_out *out_entry;
+	struct smp2p_out_list_item *out_item;
+
+	if (!handle || !*handle)
 		return -EINVAL;
+
+	out_entry = *handle;
+	*handle = NULL;
+
+	if ((out_entry->remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[out_entry->remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+			__func__, out_entry->remote_pid, out_entry->name);
+		return -EPROBE_DEFER;
 	}
 
+	out_item = &out_list[out_entry->remote_pid];
+	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	list_del(&out_entry->out_edge_list);
+	raw_notifier_chain_unregister(&out_entry->msm_smp2p_notifier_list,
+		out_entry->open_nb);
+	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
+
+	kfree(out_entry);
+
 	return 0;
 }
+EXPORT_SYMBOL(msm_smp2p_out_close);
 
-static int qcom_smp2p_probe(struct platform_device *pdev)
+/**
+ * msm_smp2p_out_read - Allows reading the entry.
+ *
+ * @handle: Handle to the smem entry structure.
+ * @data: Out pointer that holds the read data.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Allows reading of the outbound entry for read-modify-write
+ * operation.
+ */
+int msm_smp2p_out_read(struct msm_smp2p_out *handle, uint32_t *data)
 {
-	struct smp2p_entry *entry;
-	struct device_node *node;
-	struct qcom_smp2p *smp2p;
-	const char *key;
-	int irq;
-	int ret;
+	int ret = -EINVAL;
+	unsigned long flags;
+	struct smp2p_out_list_item *out_item;
 
-	smp2p = devm_kzalloc(&pdev->dev, sizeof(*smp2p), GFP_KERNEL);
-	if (!smp2p)
-		return -ENOMEM;
+	if (!handle || !data)
+		return ret;
 
-	smp2p->dev = &pdev->dev;
-	INIT_LIST_HEAD(&smp2p->inbound);
-	INIT_LIST_HEAD(&smp2p->outbound);
+	if ((handle->remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[handle->remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+			__func__, handle->remote_pid, handle->name);
+		return -EPROBE_DEFER;
+	}
 
-	platform_set_drvdata(pdev, smp2p);
+	out_item = &out_list[handle->remote_pid];
+	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	ret = out_item->ops_ptr->read_entry(handle, data);
+	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
-	key = "qcom,smem";
-	ret = of_property_read_u32_array(pdev->dev.of_node, key,
-					 smp2p->smem_items, 2);
-	if (ret)
+	return ret;
+}
+EXPORT_SYMBOL(msm_smp2p_out_read);
+
+/**
+ * msm_smp2p_out_write - Allows writing to the entry.
+ *
+ * @handle: Handle to smem entry structure.
+ * @data: Data that has to be written.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Writes a new value to the output entry. Multiple back-to-back writes
+ * may overwrite previous writes before the remote processor get a chance
+ * to see them leading to ABA race condition. The client must implement
+ * their own synchronization mechanism (such as echo mechanism) if this is
+ * not acceptable.
+ */
+int msm_smp2p_out_write(struct msm_smp2p_out *handle, uint32_t data)
+{
+	int ret = -EINVAL;
+	unsigned long flags;
+	struct smp2p_out_list_item *out_item;
+
+	if (!handle)
 		return ret;
 
-	key = "qcom,local-pid";
-	ret = of_property_read_u32(pdev->dev.of_node, key, &smp2p->local_pid);
-	if (ret)
-		goto report_read_failure;
+	if ((handle->remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[handle->remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+			__func__, handle->remote_pid, handle->name);
+		return -EPROBE_DEFER;
+	}
 
-	key = "qcom,remote-pid";
-	ret = of_property_read_u32(pdev->dev.of_node, key, &smp2p->remote_pid);
-	if (ret)
-		goto report_read_failure;
+	out_item = &out_list[handle->remote_pid];
+	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	ret = out_item->ops_ptr->write_entry(handle, data);
+	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
-	irq = platform_get_irq(pdev, 0);
-	if (irq < 0) {
-		dev_err(&pdev->dev, "unable to acquire smp2p interrupt\n");
-		return irq;
+	return ret;
+
+}
+EXPORT_SYMBOL(msm_smp2p_out_write);
+
+/**
+ * msm_smp2p_out_modify - Modifies the entry.
+ *
+ * @handle: Handle to the smem entry structure.
+ * @set_mask: Specifies the bits that needs to be set.
+ * @clear_mask: Specifies the bits that needs to be cleared.
+ * @send_irq: Flag to send interrupt to remote processor.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * The modification is done by doing a bitwise AND of clear mask followed by
+ * the bit wise OR of set mask. The clear bit mask is applied first to the
+ * data, so if a bit is set in both the clear mask and the set mask, then in
+ * the result is a set bit.  Multiple back-to-back modifications may overwrite
+ * previous values before the remote processor gets a chance to see them
+ * leading to ABA race condition. The client must implement their own
+ * synchronization mechanism (such as echo mechanism) if this is not
+ * acceptable.
+ */
+int msm_smp2p_out_modify(struct msm_smp2p_out *handle, uint32_t set_mask,
+					uint32_t clear_mask, bool send_irq)
+{
+	int ret = -EINVAL;
+	unsigned long flags;
+	struct smp2p_out_list_item *out_item;
+
+	if (!handle)
+		return ret;
+
+	if ((handle->remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[handle->remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+			__func__, handle->remote_pid, handle->name);
+		return -EPROBE_DEFER;
 	}
 
-	smp2p->mbox_client.dev = &pdev->dev;
-	smp2p->mbox_client.knows_txdone = true;
-	smp2p->mbox_chan = mbox_request_channel(&smp2p->mbox_client, 0);
-	if (IS_ERR(smp2p->mbox_chan)) {
-		if (PTR_ERR(smp2p->mbox_chan) != -ENODEV)
-			return PTR_ERR(smp2p->mbox_chan);
+	out_item = &out_list[handle->remote_pid];
+	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	ret = out_item->ops_ptr->modify_entry(handle, set_mask,
+						clear_mask, send_irq);
+	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
-		smp2p->mbox_chan = NULL;
+	return ret;
+}
+EXPORT_SYMBOL(msm_smp2p_out_modify);
 
-		ret = smp2p_parse_ipc(smp2p);
-		if (ret)
-			return ret;
+/**
+ * msm_smp2p_in_read - Read an entry on a remote processor.
+ *
+ * @remote_pid: Processor ID of the remote processor.
+ * @name: Name of the entry that is to be read.
+ * @data: Output pointer, the value will be placed here if successful.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ */
+int msm_smp2p_in_read(int remote_pid, const char *name, uint32_t *data)
+{
+	unsigned long flags;
+	struct smp2p_out_list_item *out_item;
+	uint32_t *entry_ptr = NULL;
+
+	if (remote_pid >= SMP2P_NUM_PROCS)
+		return -EINVAL;
+
+	if ((remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+						__func__, remote_pid, name);
+		return -EPROBE_DEFER;
 	}
 
-	ret = qcom_smp2p_alloc_outbound_item(smp2p);
-	if (ret < 0)
-		goto release_mbox;
+	out_item = &out_list[remote_pid];
+	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
 
-	for_each_available_child_of_node(pdev->dev.of_node, node) {
-		entry = devm_kzalloc(&pdev->dev, sizeof(*entry), GFP_KERNEL);
-		if (!entry) {
-			ret = -ENOMEM;
-			goto unwind_interfaces;
-		}
+	if (in_list[remote_pid].smem_edge_in)
+		out_item->ops_ptr->find_entry(
+			in_list[remote_pid].smem_edge_in,
+			in_list[remote_pid].safe_total_entries,
+			(char *)name, &entry_ptr, NULL);
 
-		entry->smp2p = smp2p;
-		spin_lock_init(&entry->lock);
+	spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
-		ret = of_property_read_string(node, "qcom,entry-name", &entry->name);
-		if (ret < 0)
-			goto unwind_interfaces;
+	if (!entry_ptr)
+		return -ENODEV;
 
-		if (of_property_read_bool(node, "interrupt-controller")) {
-			ret = qcom_smp2p_inbound_entry(smp2p, entry, node);
-			if (ret < 0)
-				goto unwind_interfaces;
+	*data = readl_relaxed(entry_ptr);
+	return 0;
+}
+EXPORT_SYMBOL(msm_smp2p_in_read);
 
-			list_add(&entry->node, &smp2p->inbound);
-		} else  {
-			ret = qcom_smp2p_outbound_entry(smp2p, entry, node);
-			if (ret < 0)
-				goto unwind_interfaces;
+/**
+ * msm_smp2p_in_register -  Notifies the change in value of the entry.
+ *
+ * @pid: Remote processor ID.
+ * @name: Name of the entry.
+ * @in_notifier: Notifier block used to notify about the event.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ *
+ * Register for change notifications for a remote entry. If the remote entry
+ * does not exist yet, then the registration request will be held until the
+ * remote side opens. Once the entry is open, then the SMP2P_OPEN notification
+ * will be sent. Any changes to the entry will trigger a call to the notifier
+ * block with an SMP2P_ENTRY_UPDATE event and the data field will point to an
+ * msm_smp2p_update_notif structure containing the current and previous value.
+ */
+int msm_smp2p_in_register(int pid, const char *name,
+	struct notifier_block *in_notifier)
+{
+	struct smp2p_in *pos;
+	struct smp2p_in *in = NULL;
+	int ret;
+	unsigned long flags;
+	struct msm_smp2p_update_notif data;
+	uint32_t *entry_ptr;
 
-			list_add(&entry->node, &smp2p->outbound);
+	if (pid >= SMP2P_NUM_PROCS || !name || !in_notifier)
+		return -EINVAL;
+
+	if ((pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+						__func__, pid, name);
+		return -EPROBE_DEFER;
+	}
+
+	/* Pre-allocate before spinlock since we will likely needed it */
+	in = kzalloc(sizeof(*in), GFP_KERNEL);
+	if (!in)
+		return -ENOMEM;
+
+	/* Search for existing entry */
+	spin_lock_irqsave(&out_list[pid].out_item_lock_lha1, flags);
+	spin_lock(&in_list[pid].in_item_lock_lhb1);
+
+	list_for_each_entry(pos, &in_list[pid].list, in_edge_list) {
+		if (!strncmp(pos->name, name,
+					SMP2P_MAX_ENTRY_NAME)) {
+			kfree(in);
+			in = pos;
+			break;
 		}
 	}
 
-	/* Kick the outgoing edge after allocating entries */
-	qcom_smp2p_kick(smp2p);
+	/* Create and add it to the list */
+	if (!in->notifier_count) {
+		in->remote_pid = pid;
+		strlcpy(in->name, name, SMP2P_MAX_ENTRY_NAME);
+		RAW_INIT_NOTIFIER_HEAD(&in->in_notifier_list);
+		list_add(&in->in_edge_list, &in_list[pid].list);
+	}
 
-	ret = devm_request_threaded_irq(&pdev->dev, irq,
-					NULL, qcom_smp2p_intr,
-					IRQF_ONESHOT,
-					"smp2p", (void *)smp2p);
+	ret = raw_notifier_chain_register(&in->in_notifier_list,
+			in_notifier);
 	if (ret) {
-		dev_err(&pdev->dev, "failed to request interrupt\n");
-		goto unwind_interfaces;
+		if (!in->notifier_count) {
+			list_del(&in->in_edge_list);
+			kfree(in);
+		}
+		SMP2P_DBG("%s: '%s':%d failed %d\n", __func__, name, pid, ret);
+		goto bail;
+	}
+	in->notifier_count++;
+
+	if (out_list[pid].smem_edge_state == SMP2P_EDGE_STATE_OPENED) {
+		out_list[pid].ops_ptr->find_entry(
+				in_list[pid].smem_edge_in,
+				in_list[pid].safe_total_entries, (char *)name,
+				&entry_ptr, NULL);
+		if (entry_ptr) {
+			in->entry_ptr = entry_ptr;
+			in->prev_entry_val = readl_relaxed(entry_ptr);
+
+			data.previous_value = in->prev_entry_val;
+			data.current_value = in->prev_entry_val;
+			in_notifier->notifier_call(in_notifier, SMP2P_OPEN,
+					(void *)&data);
+		}
 	}
+	SMP2P_DBG("%s: '%s':%d registered\n", __func__, name, pid);
+
+bail:
+	spin_unlock(&in_list[pid].in_item_lock_lhb1);
+	spin_unlock_irqrestore(&out_list[pid].out_item_lock_lha1, flags);
+	return ret;
 
+}
+EXPORT_SYMBOL(msm_smp2p_in_register);
 
-	return 0;
+/**
+ * msm_smp2p_in_unregister - Unregister the notifier for remote entry.
+ *
+ * @remote_pid: Processor Id of the remote processor.
+ * @name: The name of the entry.
+ * @in_notifier: Notifier block passed during registration.
+ * @returns: 0 on success, standard Linux error code otherwise.
+ */
+int msm_smp2p_in_unregister(int remote_pid, const char *name,
+				struct notifier_block *in_notifier)
+{
+	struct smp2p_in *pos;
+	struct smp2p_in *in = NULL;
+	int ret = -ENODEV;
+	unsigned long flags;
 
-unwind_interfaces:
-	list_for_each_entry(entry, &smp2p->inbound, node)
-		irq_domain_remove(entry->domain);
+	if (remote_pid >= SMP2P_NUM_PROCS || !name || !in_notifier)
+		return -EINVAL;
 
-	list_for_each_entry(entry, &smp2p->outbound, node)
-		qcom_smem_state_unregister(entry->state);
+	if ((remote_pid != SMP2P_REMOTE_MOCK_PROC) &&
+			!smp2p_int_cfgs[remote_pid].is_configured) {
+		SMP2P_INFO("%s before msm_smp2p_init(): pid[%d] name[%s]\n",
+						__func__, remote_pid, name);
+		return -EPROBE_DEFER;
+	}
 
-	smp2p->out->valid_entries = 0;
+	spin_lock_irqsave(&in_list[remote_pid].in_item_lock_lhb1, flags);
+	list_for_each_entry(pos, &in_list[remote_pid].list,
+			in_edge_list) {
+		if (!strncmp(pos->name, name, SMP2P_MAX_ENTRY_NAME)) {
+			in = pos;
+			break;
+		}
+	}
+	if (!in)
+		goto fail;
+
+	ret = raw_notifier_chain_unregister(&pos->in_notifier_list,
+			in_notifier);
+	if (ret == 0) {
+		pos->notifier_count--;
+		if (!pos->notifier_count) {
+			list_del(&pos->in_edge_list);
+			kfree(pos);
+			ret = 0;
+		}
+	} else {
+		SMP2P_ERR("%s: unregister failure '%s':%d\n", __func__,
+			name, remote_pid);
+		ret = -ENODEV;
+	}
 
-release_mbox:
-	mbox_free_channel(smp2p->mbox_chan);
+fail:
+	spin_unlock_irqrestore(&in_list[remote_pid].in_item_lock_lhb1, flags);
 
 	return ret;
+}
+EXPORT_SYMBOL(msm_smp2p_in_unregister);
 
-report_read_failure:
-	dev_err(&pdev->dev, "failed to read %s\n", key);
-	return -EINVAL;
+/**
+ * smp2p_send_interrupt - Send interrupt to remote system.
+ *
+ * @remote_pid:  Processor ID of the remote system
+ *
+ * Must be called with out_item_lock_lha1 locked.
+ */
+static void smp2p_send_interrupt(int remote_pid)
+{
+	if (smp2p_int_cfgs[remote_pid].name)
+		SMP2P_DBG("SMP2P Int Apps->%s(%d)\n",
+			smp2p_int_cfgs[remote_pid].name, remote_pid);
+
+	++smp2p_int_cfgs[remote_pid].out_interrupt_count;
+	if (remote_pid != SMP2P_REMOTE_MOCK_PROC &&
+			smp2p_int_cfgs[remote_pid].out_int_mask) {
+		/* flush any pending writes before triggering interrupt */
+		wmb();
+		writel_relaxed(smp2p_int_cfgs[remote_pid].out_int_mask,
+			smp2p_int_cfgs[remote_pid].out_int_ptr);
+	} else {
+		smp2p_remote_mock_rx_interrupt();
+	}
 }
 
-static int qcom_smp2p_remove(struct platform_device *pdev)
+/**
+ * smp2p_in_edge_notify - Notifies the entry changed on remote processor.
+ *
+ * @pid: Processor ID of the remote processor.
+ *
+ * This function is invoked on an incoming interrupt, it scans
+ * the list of the clients registered for the entries on the remote
+ * processor and notifies them if  the data changes.
+ *
+ * Note:  Edge state must be OPENED to avoid a race condition with
+ *        out_list[pid].ops_ptr->find_entry.
+ */
+static void smp2p_in_edge_notify(int pid)
 {
-	struct qcom_smp2p *smp2p = platform_get_drvdata(pdev);
-	struct smp2p_entry *entry;
+	struct smp2p_in *pos;
+	uint32_t *entry_ptr;
+	unsigned long flags;
+	struct smp2p_smem __iomem *smem_h_ptr;
+	uint32_t curr_data;
+	struct  msm_smp2p_update_notif data;
+
+	spin_lock_irqsave(&in_list[pid].in_item_lock_lhb1, flags);
+	smem_h_ptr = in_list[pid].smem_edge_in;
+	if (!smem_h_ptr) {
+		SMP2P_DBG("%s: No remote SMEM item for pid %d\n",
+			__func__, pid);
+		spin_unlock_irqrestore(&in_list[pid].in_item_lock_lhb1, flags);
+		return;
+	}
 
-	list_for_each_entry(entry, &smp2p->inbound, node)
-		irq_domain_remove(entry->domain);
+	list_for_each_entry(pos, &in_list[pid].list, in_edge_list) {
+		if (pos->entry_ptr == NULL) {
+			/* entry not open - try to open it */
+			out_list[pid].ops_ptr->find_entry(smem_h_ptr,
+				in_list[pid].safe_total_entries, pos->name,
+				&entry_ptr, NULL);
+
+			if (entry_ptr) {
+				pos->entry_ptr = entry_ptr;
+				pos->prev_entry_val = 0;
+				data.previous_value = 0;
+				data.current_value = readl_relaxed(entry_ptr);
+				raw_notifier_call_chain(
+					    &pos->in_notifier_list,
+					    SMP2P_OPEN, (void *)&data);
+			}
+		}
 
-	list_for_each_entry(entry, &smp2p->outbound, node)
-		qcom_smem_state_unregister(entry->state);
+		if (pos->entry_ptr != NULL) {
+			/* send update notification */
+			curr_data = readl_relaxed(pos->entry_ptr);
+			if (curr_data != pos->prev_entry_val) {
+				data.previous_value = pos->prev_entry_val;
+				data.current_value = curr_data;
+				pos->prev_entry_val = curr_data;
+				raw_notifier_call_chain(
+					&pos->in_notifier_list,
+					SMP2P_ENTRY_UPDATE, (void *)&data);
+			}
+		}
+	}
+	spin_unlock_irqrestore(&in_list[pid].in_item_lock_lhb1, flags);
+}
 
-	mbox_free_channel(smp2p->mbox_chan);
+/**
+ * smp2p_interrupt_handler - Incoming interrupt handler.
+ *
+ * @irq: Interrupt ID
+ * @data: Edge
+ * @returns: IRQ_HANDLED or IRQ_NONE for invalid interrupt
+ */
+static irqreturn_t smp2p_interrupt_handler(int irq, void *data)
+{
+	unsigned long flags;
+	uint32_t remote_pid = (uint32_t)(uintptr_t)data;
+
+	if (remote_pid >= SMP2P_NUM_PROCS) {
+		SMP2P_ERR("%s: invalid interrupt pid %d\n",
+			__func__, remote_pid);
+		return IRQ_NONE;
+	}
 
-	smp2p->out->valid_entries = 0;
+	if (smp2p_int_cfgs[remote_pid].name)
+		SMP2P_DBG("SMP2P Int %s(%d)->Apps\n",
+			smp2p_int_cfgs[remote_pid].name, remote_pid);
+
+	spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
+	++smp2p_int_cfgs[remote_pid].in_interrupt_count;
+
+	if (out_list[remote_pid].smem_edge_state != SMP2P_EDGE_STATE_OPENED)
+		smp2p_do_negotiation(remote_pid, &out_list[remote_pid]);
+
+	if (out_list[remote_pid].smem_edge_state == SMP2P_EDGE_STATE_OPENED) {
+		bool do_restart_ack;
+
+		/*
+		 * Follow double-check pattern for restart ack since:
+		 * 1) we must notify clients of the X->0 transition
+		 *    that is part of the restart
+		 * 2) lock cannot be held during the
+		 *    smp2p_in_edge_notify() call because clients may do
+		 *    re-entrant calls into our APIs.
+		 *
+		 * smp2p_do_ssr_ack() will only do the ack if it is
+		 * necessary to handle the race condition exposed by
+		 * unlocking the spinlocks.
+		 */
+		spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+		do_restart_ack = smp2p_ssr_ack_needed(remote_pid);
+		spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+		spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
+			flags);
+
+		smp2p_in_edge_notify(remote_pid);
+
+		if (do_restart_ack) {
+			spin_lock_irqsave(
+				&out_list[remote_pid].out_item_lock_lha1,
+				flags);
+			spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+
+			smp2p_do_ssr_ack(remote_pid);
+
+			spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+			spin_unlock_irqrestore(
+				&out_list[remote_pid].out_item_lock_lha1,
+				flags);
+		}
+	} else {
+		spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
+			flags);
+	}
 
+	return IRQ_HANDLED;
+}
+
+/**
+ * smp2p_reset_mock_edge - Reinitializes the mock edge.
+ *
+ * @returns: 0 on success, -EAGAIN to retry later.
+ *
+ * Reinitializes the mock edge to initial power-up state values.
+ */
+int smp2p_reset_mock_edge(void)
+{
+	const int rpid = SMP2P_REMOTE_MOCK_PROC;
+	unsigned long flags;
+	int ret = 0;
+
+	spin_lock_irqsave(&out_list[rpid].out_item_lock_lha1, flags);
+	spin_lock(&in_list[rpid].in_item_lock_lhb1);
+
+	if (!list_empty(&out_list[rpid].list) ||
+			!list_empty(&in_list[rpid].list)) {
+		ret = -EAGAIN;
+		goto fail;
+	}
+
+	kfree(out_list[rpid].smem_edge_out);
+	out_list[rpid].smem_edge_out = NULL;
+	out_list[rpid].ops_ptr = &version_if[0];
+	out_list[rpid].smem_edge_state = SMP2P_EDGE_STATE_CLOSED;
+	out_list[rpid].feature_ssr_ack_enabled = false;
+	out_list[rpid].restart_ack = false;
+
+	in_list[rpid].smem_edge_in = NULL;
+	in_list[rpid].item_size = 0;
+	in_list[rpid].safe_total_entries = 0;
+
+fail:
+	spin_unlock(&in_list[rpid].in_item_lock_lhb1);
+	spin_unlock_irqrestore(&out_list[rpid].out_item_lock_lha1, flags);
+
+	return ret;
+}
+
+/**
+ * msm_smp2p_interrupt_handler - Triggers incoming interrupt.
+ *
+ * @remote_pid: Remote processor ID
+ *
+ * This function is used with the remote mock infrastructure
+ * used for testing. It simulates triggering of interrupt in
+ * a testing environment.
+ */
+void msm_smp2p_interrupt_handler(int remote_pid)
+{
+	smp2p_interrupt_handler(0, (void *)(uintptr_t)remote_pid);
+}
+
+/**
+ * msm_smp2p_probe - Device tree probe function.
+ *
+ * @pdev: Pointer to device tree data.
+ * @returns: 0 on success; -ENODEV otherwise
+ */
+static int msm_smp2p_probe(struct platform_device *pdev)
+{
+	struct resource *r;
+	void *irq_out_ptr = NULL;
+	char *key;
+	uint32_t edge;
+	int ret;
+	struct device_node *node;
+	uint32_t irq_bitmask;
+	uint32_t irq_line;
+	void *temp_p;
+	unsigned temp_sz;
+
+	node = pdev->dev.of_node;
+
+	key = "qcom,remote-pid";
+	ret = of_property_read_u32(node, key, &edge);
+	if (ret) {
+		SMP2P_ERR("%s: missing edge '%s'\n", __func__, key);
+		ret = -ENODEV;
+		goto fail;
+	}
+
+	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!r) {
+		SMP2P_ERR("%s: failed gathering irq-reg resource for edge %d\n"
+				, __func__, edge);
+		ret = -ENODEV;
+		goto fail;
+	}
+	irq_out_ptr = ioremap_nocache(r->start, resource_size(r));
+	if (!irq_out_ptr) {
+		SMP2P_ERR("%s: failed remap from phys to virt for edge %d\n",
+				__func__, edge);
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	key = "qcom,irq-bitmask";
+	ret = of_property_read_u32(node, key, &irq_bitmask);
+	if (ret)
+		goto missing_key;
+
+	key = "interrupts";
+	irq_line = platform_get_irq(pdev, 0);
+	if (irq_line == -ENXIO)
+		goto missing_key;
+
+	/*
+	 * We depend on the SMEM driver, so do a test access to see if SMEM is
+	 * ready.  We don't want any side effects at this time (so no alloc)
+	 * and the return doesn't matter, so long as it is not -EPROBE_DEFER.
+	 */
+	temp_p = smem_get_entry(
+		smp2p_get_smem_item_id(SMP2P_APPS_PROC, SMP2P_MODEM_PROC),
+		&temp_sz,
+		0,
+		SMEM_ANY_HOST_FLAG);
+	if (PTR_ERR(temp_p) == -EPROBE_DEFER) {
+		SMP2P_INFO("%s: edge:%d probe before smem ready\n", __func__,
+									edge);
+		ret = -EPROBE_DEFER;
+		goto fail;
+	}
+
+	ret = request_irq(irq_line, smp2p_interrupt_handler,
+			IRQF_TRIGGER_RISING, "smp2p", (void *)(uintptr_t)edge);
+	if (ret < 0) {
+		SMP2P_ERR("%s: request_irq() failed on %d (edge %d)\n",
+				__func__, irq_line, edge);
+		ret = -ENODEV;
+		goto fail;
+	}
+
+	ret = enable_irq_wake(irq_line);
+	if (ret < 0)
+		SMP2P_ERR("%s: enable_irq_wake() failed on %d (edge %d)\n",
+				__func__, irq_line, edge);
+
+	/*
+	 * Set entry (keep is_configured last to prevent usage before
+	 * initialization).
+	 */
+	smp2p_int_cfgs[edge].in_int_id = irq_line;
+	smp2p_int_cfgs[edge].out_int_mask = irq_bitmask;
+	smp2p_int_cfgs[edge].out_int_ptr = irq_out_ptr;
+	smp2p_int_cfgs[edge].is_configured = true;
 	return 0;
+
+missing_key:
+	SMP2P_ERR("%s: missing '%s' for edge %d\n", __func__, key, edge);
+	ret = -ENODEV;
+fail:
+	if (irq_out_ptr)
+		iounmap(irq_out_ptr);
+	return ret;
 }
 
-static const struct of_device_id qcom_smp2p_of_match[] = {
+static struct of_device_id msm_smp2p_match_table[] = {
 	{ .compatible = "qcom,smp2p" },
-	{}
+	{},
 };
-MODULE_DEVICE_TABLE(of, qcom_smp2p_of_match);
-
-static struct platform_driver qcom_smp2p_driver = {
-	.probe = qcom_smp2p_probe,
-	.remove = qcom_smp2p_remove,
-	.driver  = {
-		.name  = "qcom_smp2p",
-		.of_match_table = qcom_smp2p_of_match,
+
+static struct platform_driver msm_smp2p_driver = {
+	.probe = msm_smp2p_probe,
+	.driver = {
+		.name = "msm_smp2p",
+		.owner = THIS_MODULE,
+		.of_match_table = msm_smp2p_match_table,
 	},
 };
-module_platform_driver(qcom_smp2p_driver);
 
-MODULE_DESCRIPTION("Qualcomm Shared Memory Point to Point driver");
+/**
+ * msm_smp2p_init -  Initialization function for the module.
+ *
+ * @returns: 0 on success, standard Linux error code otherwise.
+ */
+static int __init msm_smp2p_init(void)
+{
+	int i;
+	int rc;
+
+	for (i = 0; i < SMP2P_NUM_PROCS; i++) {
+		spin_lock_init(&out_list[i].out_item_lock_lha1);
+		INIT_LIST_HEAD(&out_list[i].list);
+		out_list[i].smem_edge_out = NULL;
+		out_list[i].smem_edge_state = SMP2P_EDGE_STATE_CLOSED;
+		out_list[i].ops_ptr = &version_if[0];
+		out_list[i].feature_ssr_ack_enabled = false;
+		out_list[i].restart_ack = false;
+
+		spin_lock_init(&in_list[i].in_item_lock_lhb1);
+		INIT_LIST_HEAD(&in_list[i].list);
+		in_list[i].smem_edge_in = NULL;
+	}
+
+	log_ctx = ipc_log_context_create(NUM_LOG_PAGES, "smp2p", 0);
+	if (!log_ctx)
+		SMP2P_ERR("%s: unable to create log context\n", __func__);
+
+	rc = platform_driver_register(&msm_smp2p_driver);
+	if (rc) {
+		SMP2P_ERR("%s: msm_smp2p_driver register failed %d\n",
+			__func__, rc);
+		return rc;
+	}
+
+	return 0;
+}
+module_init(msm_smp2p_init);
+
+MODULE_DESCRIPTION("MSM Shared Memory Point to Point");
 MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/qcom/spm.c b/drivers/soc/qcom/spm.c
index f9d7a85b2822..292d73ea4b97 100644
--- a/drivers/soc/qcom/spm.c
+++ b/drivers/soc/qcom/spm.c
@@ -1,8 +1,4 @@
-/*
- * Copyright (c) 2011-2014, The Linux Foundation. All rights reserved.
- * Copyright (c) 2014,2015, Linaro Ltd.
- *
- * SAW power controller driver
+/* Copyright (c) 2011-2015, The Linux Foundation. All rights reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 and
@@ -12,372 +8,708 @@
  * but WITHOUT ANY WARRANTY; without even the implied warranty of
  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  * GNU General Public License for more details.
+ *
  */
 
+#include <linux/module.h>
 #include <linux/kernel.h>
+#include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/io.h>
 #include <linux/slab.h>
-#include <linux/of.h>
-#include <linux/of_address.h>
-#include <linux/of_device.h>
-#include <linux/err.h>
-#include <linux/platform_device.h>
-#include <linux/cpuidle.h>
-#include <linux/cpu_pm.h>
-#include <linux/qcom_scm.h>
-
-#include <asm/cpuidle.h>
-#include <asm/proc-fns.h>
-#include <asm/suspend.h>
-
-#define MAX_PMIC_DATA		2
-#define MAX_SEQ_DATA		64
-#define SPM_CTL_INDEX		0x7f
-#define SPM_CTL_INDEX_SHIFT	4
-#define SPM_CTL_EN		BIT(0)
-
-enum pm_sleep_mode {
-	PM_SLEEP_MODE_STBY,
-	PM_SLEEP_MODE_RET,
-	PM_SLEEP_MODE_SPC,
-	PM_SLEEP_MODE_PC,
-	PM_SLEEP_MODE_NR,
-};
 
-enum spm_reg {
-	SPM_REG_CFG,
-	SPM_REG_SPM_CTL,
-	SPM_REG_DLY,
-	SPM_REG_PMIC_DLY,
-	SPM_REG_PMIC_DATA_0,
-	SPM_REG_PMIC_DATA_1,
-	SPM_REG_VCTL,
-	SPM_REG_SEQ_ENTRY,
-	SPM_REG_SPM_STS,
-	SPM_REG_PMIC_STS,
-	SPM_REG_NR,
-};
+#include "spm_driver.h"
+
+#define MSM_SPM_PMIC_STATE_IDLE  0
 
-struct spm_reg_data {
-	const u8 *reg_offset;
-	u32 spm_cfg;
-	u32 spm_dly;
-	u32 pmic_dly;
-	u32 pmic_data[MAX_PMIC_DATA];
-	u8 seq[MAX_SEQ_DATA];
-	u8 start_index[PM_SLEEP_MODE_NR];
+enum {
+	MSM_SPM_DEBUG_SHADOW = 1U << 0,
+	MSM_SPM_DEBUG_VCTL = 1U << 1,
 };
 
-struct spm_driver_data {
-	void __iomem *reg_base;
-	const struct spm_reg_data *reg_data;
+static int msm_spm_debug_mask;
+module_param_named(
+	debug_mask, msm_spm_debug_mask, int, S_IRUGO | S_IWUSR | S_IWGRP
+);
+
+struct saw2_data {
+	const char *ver_name;
+	uint32_t major;
+	uint32_t minor;
+	uint32_t *spm_reg_offset_ptr;
 };
 
-static const u8 spm_reg_offset_v2_1[SPM_REG_NR] = {
-	[SPM_REG_CFG]		= 0x08,
-	[SPM_REG_SPM_CTL]	= 0x30,
-	[SPM_REG_DLY]		= 0x34,
-	[SPM_REG_SEQ_ENTRY]	= 0x80,
+static uint32_t msm_spm_reg_offsets_saw2_v2_1[MSM_SPM_REG_NR] = {
+	[MSM_SPM_REG_SAW_SECURE]		= 0x00,
+	[MSM_SPM_REG_SAW_ID]			= 0x04,
+	[MSM_SPM_REG_SAW_CFG]			= 0x08,
+	[MSM_SPM_REG_SAW_SPM_STS]		= 0x0C,
+	[MSM_SPM_REG_SAW_AVS_STS]		= 0x10,
+	[MSM_SPM_REG_SAW_PMIC_STS]		= 0x14,
+	[MSM_SPM_REG_SAW_RST]			= 0x18,
+	[MSM_SPM_REG_SAW_VCTL]			= 0x1C,
+	[MSM_SPM_REG_SAW_AVS_CTL]		= 0x20,
+	[MSM_SPM_REG_SAW_AVS_LIMIT]		= 0x24,
+	[MSM_SPM_REG_SAW_AVS_DLY]		= 0x28,
+	[MSM_SPM_REG_SAW_AVS_HYSTERESIS]	= 0x2C,
+	[MSM_SPM_REG_SAW_SPM_CTL]		= 0x30,
+	[MSM_SPM_REG_SAW_SPM_DLY]		= 0x34,
+	[MSM_SPM_REG_SAW_PMIC_DATA_0]		= 0x40,
+	[MSM_SPM_REG_SAW_PMIC_DATA_1]		= 0x44,
+	[MSM_SPM_REG_SAW_PMIC_DATA_2]		= 0x48,
+	[MSM_SPM_REG_SAW_PMIC_DATA_3]		= 0x4C,
+	[MSM_SPM_REG_SAW_PMIC_DATA_4]		= 0x50,
+	[MSM_SPM_REG_SAW_PMIC_DATA_5]		= 0x54,
+	[MSM_SPM_REG_SAW_PMIC_DATA_6]		= 0x58,
+	[MSM_SPM_REG_SAW_PMIC_DATA_7]		= 0x5C,
+	[MSM_SPM_REG_SAW_SEQ_ENTRY]		= 0x80,
+	[MSM_SPM_REG_SAW_VERSION]		= 0xFD0,
 };
 
-/* SPM register data for 8974, 8084 */
-static const struct spm_reg_data spm_reg_8974_8084_cpu  = {
-	.reg_offset = spm_reg_offset_v2_1,
-	.spm_cfg = 0x1,
-	.spm_dly = 0x3C102800,
-	.seq = { 0x03, 0x0B, 0x0F, 0x00, 0x20, 0x80, 0x10, 0xE8, 0x5B, 0x03,
-		0x3B, 0xE8, 0x5B, 0x82, 0x10, 0x0B, 0x30, 0x06, 0x26, 0x30,
-		0x0F },
-	.start_index[PM_SLEEP_MODE_STBY] = 0,
-	.start_index[PM_SLEEP_MODE_SPC] = 3,
+static uint32_t msm_spm_reg_offsets_saw2_v3_0[MSM_SPM_REG_NR] = {
+	[MSM_SPM_REG_SAW_SECURE]		= 0x00,
+	[MSM_SPM_REG_SAW_ID]			= 0x04,
+	[MSM_SPM_REG_SAW_CFG]			= 0x08,
+	[MSM_SPM_REG_SAW_SPM_STS]		= 0x0C,
+	[MSM_SPM_REG_SAW_AVS_STS]		= 0x10,
+	[MSM_SPM_REG_SAW_PMIC_STS]		= 0x14,
+	[MSM_SPM_REG_SAW_RST]			= 0x18,
+	[MSM_SPM_REG_SAW_VCTL]			= 0x1C,
+	[MSM_SPM_REG_SAW_AVS_CTL]		= 0x20,
+	[MSM_SPM_REG_SAW_AVS_LIMIT]		= 0x24,
+	[MSM_SPM_REG_SAW_AVS_DLY]		= 0x28,
+	[MSM_SPM_REG_SAW_AVS_HYSTERESIS]	= 0x2C,
+	[MSM_SPM_REG_SAW_SPM_CTL]		= 0x30,
+	[MSM_SPM_REG_SAW_SPM_DLY]		= 0x34,
+	[MSM_SPM_REG_SAW_STS2]			= 0x38,
+	[MSM_SPM_REG_SAW_PMIC_DATA_0]		= 0x40,
+	[MSM_SPM_REG_SAW_PMIC_DATA_1]		= 0x44,
+	[MSM_SPM_REG_SAW_PMIC_DATA_2]		= 0x48,
+	[MSM_SPM_REG_SAW_PMIC_DATA_3]		= 0x4C,
+	[MSM_SPM_REG_SAW_PMIC_DATA_4]		= 0x50,
+	[MSM_SPM_REG_SAW_PMIC_DATA_5]		= 0x54,
+	[MSM_SPM_REG_SAW_PMIC_DATA_6]		= 0x58,
+	[MSM_SPM_REG_SAW_PMIC_DATA_7]		= 0x5C,
+	[MSM_SPM_REG_SAW_SEQ_ENTRY]		= 0x400,
+	[MSM_SPM_REG_SAW_VERSION]		= 0xFD0,
 };
 
-static const u8 spm_reg_offset_v1_1[SPM_REG_NR] = {
-	[SPM_REG_CFG]		= 0x08,
-	[SPM_REG_SPM_CTL]	= 0x20,
-	[SPM_REG_PMIC_DLY]	= 0x24,
-	[SPM_REG_PMIC_DATA_0]	= 0x28,
-	[SPM_REG_PMIC_DATA_1]	= 0x2C,
-	[SPM_REG_SEQ_ENTRY]	= 0x80,
+static uint32_t msm_spm_reg_offsets_saw2_v4_1[MSM_SPM_REG_NR] = {
+	[MSM_SPM_REG_SAW_SECURE]		= 0xC00,
+	[MSM_SPM_REG_SAW_ID]			= 0xC04,
+	[MSM_SPM_REG_SAW_STS2]			= 0xC10,
+	[MSM_SPM_REG_SAW_SPM_STS]		= 0xC0C,
+	[MSM_SPM_REG_SAW_AVS_STS]		= 0xC14,
+	[MSM_SPM_REG_SAW_PMIC_STS]		= 0xC18,
+	[MSM_SPM_REG_SAW_RST]			= 0xC1C,
+	[MSM_SPM_REG_SAW_VCTL]			= 0x900,
+	[MSM_SPM_REG_SAW_AVS_CTL]		= 0x904,
+	[MSM_SPM_REG_SAW_AVS_LIMIT]		= 0x908,
+	[MSM_SPM_REG_SAW_AVS_DLY]		= 0x90C,
+	[MSM_SPM_REG_SAW_SPM_CTL]		= 0x0,
+	[MSM_SPM_REG_SAW_SPM_DLY]		= 0x4,
+	[MSM_SPM_REG_SAW_CFG]			= 0x0C,
+	[MSM_SPM_REG_SAW_PMIC_DATA_0]		= 0x40,
+	[MSM_SPM_REG_SAW_PMIC_DATA_1]		= 0x44,
+	[MSM_SPM_REG_SAW_PMIC_DATA_2]		= 0x48,
+	[MSM_SPM_REG_SAW_PMIC_DATA_3]		= 0x4C,
+	[MSM_SPM_REG_SAW_PMIC_DATA_4]		= 0x50,
+	[MSM_SPM_REG_SAW_PMIC_DATA_5]		= 0x54,
+	[MSM_SPM_REG_SAW_PMIC_DATA_6]		= 0x58,
+	[MSM_SPM_REG_SAW_PMIC_DATA_7]		= 0x5C,
+	[MSM_SPM_REG_SAW_SEQ_ENTRY]		= 0x400,
+	[MSM_SPM_REG_SAW_VERSION]		= 0xFD0,
 };
 
-/* SPM register data for 8064 */
-static const struct spm_reg_data spm_reg_8064_cpu = {
-	.reg_offset = spm_reg_offset_v1_1,
-	.spm_cfg = 0x1F,
-	.pmic_dly = 0x02020004,
-	.pmic_data[0] = 0x0084009C,
-	.pmic_data[1] = 0x00A4001C,
-	.seq = { 0x03, 0x0F, 0x00, 0x24, 0x54, 0x10, 0x09, 0x03, 0x01,
-		0x10, 0x54, 0x30, 0x0C, 0x24, 0x30, 0x0F },
-	.start_index[PM_SLEEP_MODE_STBY] = 0,
-	.start_index[PM_SLEEP_MODE_SPC] = 2,
+static struct saw2_data saw2_info[] = {
+	[0] = {
+		"SAW_v2.1",
+		0x2,
+		0x1,
+		msm_spm_reg_offsets_saw2_v2_1,
+	},
+	[1] = {
+		"SAW_v2.3",
+		0x3,
+		0x0,
+		msm_spm_reg_offsets_saw2_v3_0,
+	},
+	[2] = {
+		"SAW_v3.0",
+		0x1,
+		0x0,
+		msm_spm_reg_offsets_saw2_v3_0,
+	},
+	[3] = {
+		"SAW_v4.0",
+		0x4,
+		0x1,
+		msm_spm_reg_offsets_saw2_v4_1,
+	},
 };
 
-static DEFINE_PER_CPU(struct spm_driver_data *, cpu_spm_drv);
+static uint32_t num_pmic_data;
+
+static void msm_spm_drv_flush_shadow(struct msm_spm_driver_data *dev,
+		unsigned int reg_index)
+{
+	BUG_ON(!dev);
 
-typedef int (*idle_fn)(void);
-static DEFINE_PER_CPU(idle_fn*, qcom_idle_ops);
+	BUG_ON(!dev->reg_shadow);
 
-static inline void spm_register_write(struct spm_driver_data *drv,
-					enum spm_reg reg, u32 val)
+	__raw_writel(dev->reg_shadow[reg_index],
+		dev->reg_base_addr + dev->reg_offsets[reg_index]);
+}
+
+static void msm_spm_drv_load_shadow(struct msm_spm_driver_data *dev,
+		unsigned int reg_index)
 {
-	if (drv->reg_data->reg_offset[reg])
-		writel_relaxed(val, drv->reg_base +
-				drv->reg_data->reg_offset[reg]);
+	dev->reg_shadow[reg_index] =
+		__raw_readl(dev->reg_base_addr +
+				dev->reg_offsets[reg_index]);
 }
 
-/* Ensure a guaranteed write, before return */
-static inline void spm_register_write_sync(struct spm_driver_data *drv,
-					enum spm_reg reg, u32 val)
+static inline uint32_t msm_spm_drv_get_num_spm_entry(
+		struct msm_spm_driver_data *dev)
 {
-	u32 ret;
+	BUG_ON(!dev);
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_ID);
+	return (dev->reg_shadow[MSM_SPM_REG_SAW_ID] >> 24) & 0xFF;
+}
 
-	if (!drv->reg_data->reg_offset[reg])
-		return;
+static inline void msm_spm_drv_set_start_addr(
+		struct msm_spm_driver_data *dev, uint32_t ctl)
+{
+	dev->reg_shadow[MSM_SPM_REG_SAW_SPM_CTL] = ctl;
+}
 
-	do {
-		writel_relaxed(val, drv->reg_base +
-				drv->reg_data->reg_offset[reg]);
-		ret = readl_relaxed(drv->reg_base +
-				drv->reg_data->reg_offset[reg]);
-		if (ret == val)
-			break;
-		cpu_relax();
-	} while (1);
+static inline bool msm_spm_pmic_arb_present(struct msm_spm_driver_data *dev)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_ID);
+	return (dev->reg_shadow[MSM_SPM_REG_SAW_ID] >> 2) & 0x1;
 }
 
-static inline u32 spm_register_read(struct spm_driver_data *drv,
-					enum spm_reg reg)
+static inline void msm_spm_drv_set_vctl2(struct msm_spm_driver_data *dev,
+		uint32_t vlevel)
 {
-	return readl_relaxed(drv->reg_base + drv->reg_data->reg_offset[reg]);
+	unsigned int pmic_data = 0;
+
+	/**
+	 * VCTL_PORT has to be 0, for PMIC_STS register to be updated.
+	 * Ensure that vctl_port is always set to 0.
+	 */
+	WARN_ON(dev->vctl_port);
+
+	pmic_data |= vlevel;
+	pmic_data |= (dev->vctl_port & 0x7) << 16;
+
+	dev->reg_shadow[MSM_SPM_REG_SAW_VCTL] &= ~0x700FF;
+	dev->reg_shadow[MSM_SPM_REG_SAW_VCTL] |= pmic_data;
+
+	dev->reg_shadow[MSM_SPM_REG_SAW_PMIC_DATA_3] &= ~0x700FF;
+	dev->reg_shadow[MSM_SPM_REG_SAW_PMIC_DATA_3] |= pmic_data;
+
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_VCTL);
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_PMIC_DATA_3);
 }
 
-static void spm_set_low_power_mode(struct spm_driver_data *drv,
-					enum pm_sleep_mode mode)
+static inline uint32_t msm_spm_drv_get_num_pmic_data(
+		struct msm_spm_driver_data *dev)
 {
-	u32 start_index;
-	u32 ctl_val;
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_ID);
+	mb();
+	return (dev->reg_shadow[MSM_SPM_REG_SAW_ID] >> 4) & 0x7;
+}
 
-	start_index = drv->reg_data->start_index[mode];
+static inline uint32_t msm_spm_drv_get_sts_pmic_state(
+		struct msm_spm_driver_data *dev)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_PMIC_STS);
+	return (dev->reg_shadow[MSM_SPM_REG_SAW_PMIC_STS] >> 16) &
+				0x03;
+}
 
-	ctl_val = spm_register_read(drv, SPM_REG_SPM_CTL);
-	ctl_val &= ~(SPM_CTL_INDEX << SPM_CTL_INDEX_SHIFT);
-	ctl_val |= start_index << SPM_CTL_INDEX_SHIFT;
-	ctl_val |= SPM_CTL_EN;
-	spm_register_write_sync(drv, SPM_REG_SPM_CTL, ctl_val);
+uint32_t msm_spm_drv_get_sts_curr_pmic_data(
+		struct msm_spm_driver_data *dev)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_PMIC_STS);
+	return dev->reg_shadow[MSM_SPM_REG_SAW_PMIC_STS] & 0xFF;
 }
 
-static int qcom_pm_collapse(unsigned long int unused)
+static inline void msm_spm_drv_get_saw2_ver(struct msm_spm_driver_data *dev,
+		uint32_t *major, uint32_t *minor)
 {
-	qcom_scm_cpu_power_down(QCOM_SCM_CPU_PWR_DOWN_L2_ON);
+	uint32_t val = 0;
 
-	/*
-	 * Returns here only if there was a pending interrupt and we did not
-	 * power down as a result.
-	 */
-	return -1;
+	dev->reg_shadow[MSM_SPM_REG_SAW_VERSION] =
+			__raw_readl(dev->reg_base_addr + dev->ver_reg);
+
+	val = dev->reg_shadow[MSM_SPM_REG_SAW_VERSION];
+
+	*major = (val >> 28) & 0xF;
+	*minor = (val >> 16) & 0xFFF;
 }
 
-static int qcom_cpu_spc(void)
+inline int msm_spm_drv_set_spm_enable(
+		struct msm_spm_driver_data *dev, bool enable)
 {
-	int ret;
-	struct spm_driver_data *drv = __this_cpu_read(cpu_spm_drv);
+	uint32_t value = enable ? 0x01 : 0x00;
 
-	spm_set_low_power_mode(drv, PM_SLEEP_MODE_SPC);
-	ret = cpu_suspend(0, qcom_pm_collapse);
-	/*
-	 * ARM common code executes WFI without calling into our driver and
-	 * if the SPM mode is not reset, then we may accidently power down the
-	 * cpu when we intended only to gate the cpu clock.
-	 * Ensure the state is set to standby before returning.
-	 */
-	spm_set_low_power_mode(drv, PM_SLEEP_MODE_STBY);
+	if (!dev)
+		return -EINVAL;
+
+	if ((dev->reg_shadow[MSM_SPM_REG_SAW_SPM_CTL] & 0x01) ^ value) {
+
+		dev->reg_shadow[MSM_SPM_REG_SAW_SPM_CTL] &= ~0x1;
+		dev->reg_shadow[MSM_SPM_REG_SAW_SPM_CTL] |= value;
 
-	return ret;
+		msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_SPM_CTL);
+		wmb();
+	}
+	return 0;
 }
 
-static int qcom_idle_enter(unsigned long index)
+int msm_spm_drv_get_avs_enable(struct msm_spm_driver_data *dev)
 {
-	return __this_cpu_read(qcom_idle_ops)[index]();
+	if (!dev)
+		return -EINVAL;
+
+	return dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] & 0x01;
 }
 
-static const struct of_device_id qcom_idle_state_match[] __initconst = {
-	{ .compatible = "qcom,idle-state-spc", .data = qcom_cpu_spc },
-	{ },
-};
+int msm_spm_drv_set_avs_enable(struct msm_spm_driver_data *dev,
+		 bool enable)
+{
+	uint32_t value = enable ? 0x1 : 0x0;
+
+	if (!dev)
+		return -EINVAL;
 
-static int __init qcom_cpuidle_init(struct device_node *cpu_node, int cpu)
+	if ((dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] & 0x1) ^ value) {
+		dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] &= ~0x1;
+		dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] |= value;
+
+		msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+	}
+
+	return 0;
+}
+
+int msm_spm_drv_set_avs_limit(struct msm_spm_driver_data *dev,
+		uint32_t min_lvl, uint32_t max_lvl)
 {
-	const struct of_device_id *match_id;
-	struct device_node *state_node;
-	int i;
-	int state_count = 1;
-	idle_fn idle_fns[CPUIDLE_STATE_MAX];
-	idle_fn *fns;
-	cpumask_t mask;
-	bool use_scm_power_down = false;
-
-	for (i = 0; ; i++) {
-		state_node = of_parse_phandle(cpu_node, "cpu-idle-states", i);
-		if (!state_node)
-			break;
+	uint32_t value = (max_lvl & 0xff) << 16 | (min_lvl & 0xff);
 
-		if (!of_device_is_available(state_node))
-			continue;
+	if (!dev)
+		return -EINVAL;
 
-		if (i == CPUIDLE_STATE_MAX) {
-			pr_warn("%s: cpuidle states reached max possible\n",
-					__func__);
-			break;
-		}
+	dev->reg_shadow[MSM_SPM_REG_SAW_AVS_LIMIT] = value;
 
-		match_id = of_match_node(qcom_idle_state_match, state_node);
-		if (!match_id)
-			return -ENODEV;
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_LIMIT);
 
-		idle_fns[state_count] = match_id->data;
+	return 0;
+}
 
-		/* Check if any of the states allow power down */
-		if (match_id->data == qcom_cpu_spc)
-			use_scm_power_down = true;
+static int msm_spm_drv_avs_irq_mask(enum msm_spm_avs_irq irq)
+{
+	switch (irq) {
+	case MSM_SPM_AVS_IRQ_MIN:
+		return BIT(1);
+	case MSM_SPM_AVS_IRQ_MAX:
+		return BIT(2);
+	default:
+		return -EINVAL;
+	}
+}
 
-		state_count++;
+int msm_spm_drv_set_avs_irq_enable(struct msm_spm_driver_data *dev,
+		enum msm_spm_avs_irq irq, bool enable)
+{
+	int mask = msm_spm_drv_avs_irq_mask(irq);
+	uint32_t value;
+
+	if (!dev)
+		return -EINVAL;
+	else if (mask < 0)
+		return mask;
+
+	value = enable ? mask : 0;
+
+	if ((dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] & mask) ^ value) {
+		dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] &= ~mask;
+		dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] |= value;
+		msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
 	}
 
-	if (state_count == 1)
-		goto check_spm;
+	return 0;
+}
+
+int msm_spm_drv_avs_clear_irq(struct msm_spm_driver_data *dev,
+		enum msm_spm_avs_irq irq)
+{
+	int mask = msm_spm_drv_avs_irq_mask(irq);
 
-	fns = devm_kcalloc(get_cpu_device(cpu), state_count, sizeof(*fns),
-			GFP_KERNEL);
-	if (!fns)
-		return -ENOMEM;
+	if (!dev)
+		return -EINVAL;
+	else if (mask < 0)
+		return mask;
+
+	if (dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] & mask) {
+		/*
+		 * The interrupt status is cleared by disabling and then
+		 * re-enabling the interrupt.
+		 */
+		dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] &= ~mask;
+		msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+		dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] |= mask;
+		msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+	}
 
-	for (i = 1; i < state_count; i++)
-		fns[i] = idle_fns[i];
+	return 0;
+}
+
+void msm_spm_drv_flush_seq_entry(struct msm_spm_driver_data *dev)
+{
+	int i;
+	int num_spm_entry = msm_spm_drv_get_num_spm_entry(dev);
 
-	if (use_scm_power_down) {
-		/* We have atleast one power down mode */
-		cpumask_clear(&mask);
-		cpumask_set_cpu(cpu, &mask);
-		qcom_scm_set_warm_boot_addr(cpu_resume_arm, &mask);
+	if (!dev) {
+		__WARN();
+		return;
 	}
 
-	per_cpu(qcom_idle_ops, cpu) = fns;
+	for (i = 0; i < num_spm_entry; i++) {
+		__raw_writel(dev->reg_seq_entry_shadow[i],
+			dev->reg_base_addr
+			+ dev->reg_offsets[MSM_SPM_REG_SAW_SEQ_ENTRY]
+			+ 4 * i);
+	}
+	mb();
+}
 
-	/*
-	 * SPM probe for the cpu should have happened by now, if the
-	 * SPM device does not exist, return -ENXIO to indicate that the
-	 * cpu does not support idle states.
-	 */
-check_spm:
-	return per_cpu(cpu_spm_drv, cpu) ? 0 : -ENXIO;
+void dump_regs(struct msm_spm_driver_data *dev, int cpu)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_SPM_STS);
+	mb();
+	pr_err("CPU%d: spm register MSM_SPM_REG_SAW_SPM_STS: 0x%x\n", cpu,
+			dev->reg_shadow[MSM_SPM_REG_SAW_SPM_STS]);
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_SPM_CTL);
+	mb();
+	pr_err("CPU%d: spm register MSM_SPM_REG_SAW_SPM_CTL: 0x%x\n", cpu,
+			dev->reg_shadow[MSM_SPM_REG_SAW_SPM_CTL]);
 }
 
-static const struct cpuidle_ops qcom_cpuidle_ops __initconst = {
-	.suspend = qcom_idle_enter,
-	.init = qcom_cpuidle_init,
-};
+int msm_spm_drv_write_seq_data(struct msm_spm_driver_data *dev,
+		uint8_t *cmd, uint32_t *offset)
+{
+	uint32_t cmd_w;
+	uint32_t offset_w = *offset / 4;
+	uint8_t last_cmd;
 
-CPUIDLE_METHOD_OF_DECLARE(qcom_idle_v1, "qcom,kpss-acc-v1", &qcom_cpuidle_ops);
-CPUIDLE_METHOD_OF_DECLARE(qcom_idle_v2, "qcom,kpss-acc-v2", &qcom_cpuidle_ops);
-
-static struct spm_driver_data *spm_get_drv(struct platform_device *pdev,
-		int *spm_cpu)
-{
-	struct spm_driver_data *drv = NULL;
-	struct device_node *cpu_node, *saw_node;
-	int cpu;
-	bool found = 0;
-
-	for_each_possible_cpu(cpu) {
-		cpu_node = of_cpu_device_node_get(cpu);
-		if (!cpu_node)
-			continue;
-		saw_node = of_parse_phandle(cpu_node, "qcom,saw", 0);
-		found = (saw_node == pdev->dev.of_node);
-		of_node_put(saw_node);
-		of_node_put(cpu_node);
-		if (found)
+	if (!cmd)
+		return -EINVAL;
+
+	while (1) {
+		int i;
+		cmd_w = 0;
+		last_cmd = 0;
+		cmd_w = dev->reg_seq_entry_shadow[offset_w];
+
+		for (i = (*offset % 4); i < 4; i++) {
+			last_cmd = *(cmd++);
+			cmd_w |=  last_cmd << (i * 8);
+			(*offset)++;
+			if (last_cmd == 0x0f)
+				break;
+		}
+
+		dev->reg_seq_entry_shadow[offset_w++] = cmd_w;
+		if (last_cmd == 0x0f)
 			break;
 	}
 
-	if (found) {
-		drv = devm_kzalloc(&pdev->dev, sizeof(*drv), GFP_KERNEL);
-		if (drv)
-			*spm_cpu = cpu;
+	return 0;
+}
+
+int msm_spm_drv_set_low_power_mode(struct msm_spm_driver_data *dev,
+		uint32_t ctl)
+{
+
+	/* SPM is configured to reset start address to zero after end of Program
+	 */
+	if (!dev)
+		return -EINVAL;
+
+	msm_spm_drv_set_start_addr(dev, ctl);
+
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_SPM_CTL);
+	wmb();
+
+	if (msm_spm_debug_mask & MSM_SPM_DEBUG_SHADOW) {
+		int i;
+		for (i = 0; i < MSM_SPM_REG_NR; i++)
+			pr_info("%s: reg %02x = 0x%08x\n", __func__,
+				dev->reg_offsets[i], dev->reg_shadow[i]);
 	}
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_SPM_STS);
 
-	return drv;
+	return 0;
 }
 
-static const struct of_device_id spm_match_table[] = {
-	{ .compatible = "qcom,msm8974-saw2-v2.1-cpu",
-	  .data = &spm_reg_8974_8084_cpu },
-	{ .compatible = "qcom,apq8084-saw2-v2.1-cpu",
-	  .data = &spm_reg_8974_8084_cpu },
-	{ .compatible = "qcom,apq8064-saw2-v1.1-cpu",
-	  .data = &spm_reg_8064_cpu },
-	{ },
-};
+uint32_t msm_spm_drv_get_vdd(struct msm_spm_driver_data *dev)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_PMIC_STS);
+	return dev->reg_shadow[MSM_SPM_REG_SAW_PMIC_STS] & 0xFF;
+}
+
+#ifdef CONFIG_MSM_AVS_HW
+static bool msm_spm_drv_is_avs_enabled(struct msm_spm_driver_data *dev)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+	return dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] & BIT(0);
+}
+
+static void msm_spm_drv_disable_avs(struct msm_spm_driver_data *dev)
+{
+	msm_spm_drv_load_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+	dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] &= ~BIT(27);
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+}
+
+static void msm_spm_drv_enable_avs(struct msm_spm_driver_data *dev)
+{
+	dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] |= BIT(27);
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+}
+
+static void msm_spm_drv_set_avs_vlevel(struct msm_spm_driver_data *dev,
+		unsigned int vlevel)
+{
+	vlevel &= 0x3f;
+	dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] &= ~0x7efc00;
+	dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] |= ((vlevel - 4) << 10);
+	dev->reg_shadow[MSM_SPM_REG_SAW_AVS_CTL] |= (vlevel << 17);
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_AVS_CTL);
+}
+
+#else
+static bool msm_spm_drv_is_avs_enabled(struct msm_spm_driver_data *dev)
+{
+	return false;
+}
+
+static void msm_spm_drv_disable_avs(struct msm_spm_driver_data *dev) { }
 
-static int spm_dev_probe(struct platform_device *pdev)
+static void msm_spm_drv_enable_avs(struct msm_spm_driver_data *dev) { }
+
+static void msm_spm_drv_set_avs_vlevel(struct msm_spm_driver_data *dev,
+		unsigned int vlevel) { }
+#endif
+
+int msm_spm_drv_set_vdd(struct msm_spm_driver_data *dev, unsigned int vlevel)
 {
-	struct spm_driver_data *drv;
-	struct resource *res;
-	const struct of_device_id *match_id;
-	void __iomem *addr;
-	int cpu;
+	uint32_t timeout_us, new_level;
+	bool avs_enabled;
 
-	drv = spm_get_drv(pdev, &cpu);
-	if (!drv)
+	if (!dev)
 		return -EINVAL;
 
-	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
-	drv->reg_base = devm_ioremap_resource(&pdev->dev, res);
-	if (IS_ERR(drv->reg_base))
-		return PTR_ERR(drv->reg_base);
+	avs_enabled  = msm_spm_drv_is_avs_enabled(dev);
 
-	match_id = of_match_node(spm_match_table, pdev->dev.of_node);
-	if (!match_id)
-		return -ENODEV;
+	if (!msm_spm_pmic_arb_present(dev))
+		return -ENOSYS;
+
+	if (msm_spm_debug_mask & MSM_SPM_DEBUG_VCTL)
+		pr_info("%s: requesting vlevel %#x\n", __func__, vlevel);
+
+	if (avs_enabled)
+		msm_spm_drv_disable_avs(dev);
+
+	/* Kick the state machine back to idle */
+	dev->reg_shadow[MSM_SPM_REG_SAW_RST] = 1;
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_RST);
 
-	drv->reg_data = match_id->data;
+	msm_spm_drv_set_vctl2(dev, vlevel);
 
-	/* Write the SPM sequences first.. */
-	addr = drv->reg_base + drv->reg_data->reg_offset[SPM_REG_SEQ_ENTRY];
-	__iowrite32_copy(addr, drv->reg_data->seq,
-			ARRAY_SIZE(drv->reg_data->seq) / 4);
+	timeout_us = dev->vctl_timeout_us;
+	/* Confirm the voltage we set was what hardware sent */
+	do {
+		new_level = msm_spm_drv_get_sts_curr_pmic_data(dev);
+		if (new_level == vlevel)
+			break;
+		udelay(1);
+	} while (--timeout_us);
+	if (!timeout_us) {
+		pr_info("Wrong level %#x\n", new_level);
+		goto set_vdd_bail;
+	}
+
+	if (msm_spm_debug_mask & MSM_SPM_DEBUG_VCTL)
+		pr_info("%s: done, remaining timeout %u us\n",
+			__func__, timeout_us);
+
+	/* Set AVS min/max */
+	if (avs_enabled) {
+		msm_spm_drv_set_avs_vlevel(dev, vlevel);
+		msm_spm_drv_enable_avs(dev);
+	}
+
+	return 0;
+
+set_vdd_bail:
+	if (avs_enabled)
+		msm_spm_drv_enable_avs(dev);
+
+	pr_err("%s: failed %#x, remaining timeout %uus, vlevel %#x\n",
+		__func__, vlevel, timeout_us, new_level);
+	return -EIO;
+}
 
-	/*
-	 * ..and then the control registers.
-	 * On some SoC if the control registers are written first and if the
-	 * CPU was held in reset, the reset signal could trigger the SPM state
-	 * machine, before the sequences are completely written.
+static int msm_spm_drv_get_pmic_port(struct msm_spm_driver_data *dev,
+		enum msm_spm_pmic_port port)
+{
+	int index = -1;
+
+	switch (port) {
+	case MSM_SPM_PMIC_VCTL_PORT:
+		index = dev->vctl_port;
+		break;
+	case MSM_SPM_PMIC_PHASE_PORT:
+		index = dev->phase_port;
+		break;
+	case MSM_SPM_PMIC_PFM_PORT:
+		index = dev->pfm_port;
+		break;
+	default:
+		break;
+	}
+
+	return index;
+}
+
+int msm_spm_drv_set_pmic_data(struct msm_spm_driver_data *dev,
+		enum msm_spm_pmic_port port, unsigned int data)
+{
+	unsigned int pmic_data = 0;
+	unsigned int timeout_us = 0;
+	int index = 0;
+
+	if (!msm_spm_pmic_arb_present(dev))
+		return -ENOSYS;
+
+	index = msm_spm_drv_get_pmic_port(dev, port);
+	if (index < 0)
+		return -ENODEV;
+
+	pmic_data |= data & 0xFF;
+	pmic_data |= (index & 0x7) << 16;
+
+	dev->reg_shadow[MSM_SPM_REG_SAW_VCTL] &= ~0x700FF;
+	dev->reg_shadow[MSM_SPM_REG_SAW_VCTL] |= pmic_data;
+	msm_spm_drv_flush_shadow(dev, MSM_SPM_REG_SAW_VCTL);
+	mb();
+
+	timeout_us = dev->vctl_timeout_us;
+	/**
+	 * Confirm the pmic data set was what hardware sent by
+	 * checking the PMIC FSM state.
+	 * We cannot use the sts_pmic_data and check it against
+	 * the value like we do fot set_vdd, since the PMIC_STS
+	 * is only updated for SAW_VCTL sent with port index 0.
 	 */
-	spm_register_write(drv, SPM_REG_CFG, drv->reg_data->spm_cfg);
-	spm_register_write(drv, SPM_REG_DLY, drv->reg_data->spm_dly);
-	spm_register_write(drv, SPM_REG_PMIC_DLY, drv->reg_data->pmic_dly);
-	spm_register_write(drv, SPM_REG_PMIC_DATA_0,
-				drv->reg_data->pmic_data[0]);
-	spm_register_write(drv, SPM_REG_PMIC_DATA_1,
-				drv->reg_data->pmic_data[1]);
+	do {
+		if (msm_spm_drv_get_sts_pmic_state(dev) ==
+				MSM_SPM_PMIC_STATE_IDLE)
+			break;
+		udelay(1);
+	} while (--timeout_us);
 
-	/* Set up Standby as the default low power mode */
-	spm_set_low_power_mode(drv, PM_SLEEP_MODE_STBY);
+	if (!timeout_us) {
+		pr_err("%s: failed, remaining timeout %u us, data %d\n",
+				__func__, timeout_us, data);
+		return -EIO;
+	}
+
+	return 0;
+}
 
-	per_cpu(cpu_spm_drv, cpu) = drv;
+void msm_spm_drv_reinit(struct msm_spm_driver_data *dev, bool seq_write)
+{
+	int i;
 
+	if (seq_write)
+		msm_spm_drv_flush_seq_entry(dev);
+
+	for (i = 0; i < MSM_SPM_REG_SAW_PMIC_DATA_0 + num_pmic_data; i++)
+		msm_spm_drv_load_shadow(dev, i);
+
+	for (i = MSM_SPM_REG_NR_INITIALIZE + 1; i < MSM_SPM_REG_NR; i++)
+		msm_spm_drv_load_shadow(dev, i);
+}
+
+int msm_spm_drv_reg_init(struct msm_spm_driver_data *dev,
+		struct msm_spm_platform_data *data)
+{
+	int i;
+	bool found = false;
+
+	dev->ver_reg = data->ver_reg;
+	dev->reg_base_addr = data->reg_base_addr;
+	msm_spm_drv_get_saw2_ver(dev, &dev->major, &dev->minor);
+	for (i = 0; i < ARRAY_SIZE(saw2_info); i++)
+		if (dev->major == saw2_info[i].major &&
+			dev->minor == saw2_info[i].minor) {
+			pr_debug("%s: Version found\n",
+					saw2_info[i].ver_name);
+			dev->reg_offsets = saw2_info[i].spm_reg_offset_ptr;
+			found = true;
+			break;
+		}
+
+	if (!found) {
+		pr_err("%s: No SAW version found\n", __func__);
+		BUG_ON(!found);
+	}
 	return 0;
 }
 
-static struct platform_driver spm_driver = {
-	.probe = spm_dev_probe,
-	.driver = {
-		.name = "saw",
-		.of_match_table = spm_match_table,
-	},
-};
+void msm_spm_drv_upd_reg_shadow(struct msm_spm_driver_data *dev, int id,
+		int val)
+{
+	dev->reg_shadow[id] = val;
+	msm_spm_drv_flush_shadow(dev, id);
+	/* Complete the above writes before other accesses */
+	mb();
+}
+
+int msm_spm_drv_init(struct msm_spm_driver_data *dev,
+		struct msm_spm_platform_data *data)
+{
+	int num_spm_entry;
 
-builtin_platform_driver(spm_driver);
+	BUG_ON(!dev || !data);
+
+	dev->vctl_port = data->vctl_port;
+	dev->phase_port = data->phase_port;
+	dev->pfm_port = data->pfm_port;
+	dev->reg_base_addr = data->reg_base_addr;
+	memcpy(dev->reg_shadow, data->reg_init_values,
+			sizeof(data->reg_init_values));
+
+	dev->vctl_timeout_us = data->vctl_timeout_us;
+
+
+	if (!num_pmic_data)
+		num_pmic_data = msm_spm_drv_get_num_pmic_data(dev);
+
+	num_spm_entry = msm_spm_drv_get_num_spm_entry(dev);
+
+	dev->reg_seq_entry_shadow =
+		kzalloc(sizeof(*dev->reg_seq_entry_shadow) * num_spm_entry,
+				GFP_KERNEL);
+
+	if (!dev->reg_seq_entry_shadow)
+		return -ENOMEM;
+
+	return 0;
+}
diff --git a/include/linux/clkdev.h b/include/linux/clkdev.h
index 4890ff033220..8ef7eb1143e4 100644
--- a/include/linux/clkdev.h
+++ b/include/linux/clkdev.h
@@ -22,6 +22,7 @@ struct clk_lookup {
 	struct list_head	node;
 	const char		*dev_id;
 	const char		*con_id;
+	int			of_idx;
 	struct clk		*clk;
 	struct clk_hw		*clk_hw;
 };
diff --git a/include/linux/cpu.h b/include/linux/cpu.h
index 3233fbe23594..0053ef6ef5f5 100644
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@ -29,6 +29,19 @@ struct cpu {
 	struct device dev;
 };
 
+struct cpu_pstate_pwr {
+	unsigned int freq;
+	uint32_t power;
+};
+
+struct cpu_pwr_stats {
+	int cpu;
+	long temp;
+	struct cpu_pstate_pwr *ptable;
+	bool throttling;
+	int len;
+};
+
 extern void boot_cpu_init(void);
 extern void boot_cpu_hotplug_init(void);
 extern void cpu_init(void);
@@ -138,6 +151,10 @@ static inline int disable_nonboot_cpus(void) { return 0; }
 static inline void enable_nonboot_cpus(void) {}
 #endif /* !CONFIG_PM_SLEEP_SMP */
 
+struct cpu_pwr_stats *get_cpu_pwr_stats(void);
+void trigger_cpu_pwr_stats_calc(void);
+int register_cpu_pwr_stats_ready_notifier(struct notifier_block *nb);
+
 void cpu_startup_entry(enum cpuhp_state state);
 
 void cpu_idle_poll_ctrl(bool enable);
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index 882a9b9e34bc..274aa0ebd08b 100644
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -429,6 +429,7 @@ static inline void cpufreq_resume(void) {}
 
 #define CPUFREQ_TRANSITION_NOTIFIER	(0)
 #define CPUFREQ_POLICY_NOTIFIER		(1)
+#define CPUFREQ_GOVINFO_NOTIFIER	(2)
 
 /* Transition notifiers */
 #define CPUFREQ_PRECHANGE		(0)
@@ -447,6 +448,17 @@ void cpufreq_freq_transition_begin(struct cpufreq_policy *policy,
 void cpufreq_freq_transition_end(struct cpufreq_policy *policy,
 		struct cpufreq_freqs *freqs, int transition_failed);
 
+/*
+ * Governor specific info that can be passed to modules that subscribe
+ * to CPUFREQ_GOVINFO_NOTIFIER
+ */
+struct cpufreq_govinfo {
+	unsigned int cpu;
+	unsigned int load;
+	unsigned int sampling_rate_us;
+};
+extern struct atomic_notifier_head cpufreq_govinfo_notifier_list;
+
 #else /* CONFIG_CPU_FREQ */
 static inline int cpufreq_register_notifier(struct notifier_block *nb,
 						unsigned int list)
diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f9cc309507d9..df501b9d1d08 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -70,6 +70,20 @@
  */
 #define DMA_ATTR_PRIVILEGED		(1UL << 9)
 
+/* 
+* DMA_ATTR_STRONGLY_ORDERED: allocates memory with a very restrictive type
+* of mapping (no unaligned accesses, no re-ordering, no write merging, no
+* buffering, no pre-fetching). This has severe performance penalties and
+* should not be used for general purpose DMA allocations. It should only
+* be used if one of the restrictions on strongly ordered memory is required.
+ */
+#define DMA_ATTR_STRONGLY_ORDERED (1UL << 10)
+
+/* 
+* DMA_ATTR_SKIP_ZEROING: 
+ */
+#define DMA_ATTR_SKIP_ZEROING (1UL << 11)
+
 /*
  * A dma_addr_t can hold any valid DMA or bus address for the platform.
  * It can be given to a device to use as a DMA source or target.  A CPU cannot
@@ -130,6 +144,11 @@ struct dma_map_ops {
 			enum dma_data_direction direction);
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
+	void *(*remap)(struct device *dev, void *cpu_addr,
+			dma_addr_t dma_handle, size_t size,
+			unsigned long attrs);
+	void (*unremap)(struct device *dev, void *remapped_address,
+			size_t size);
 #ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
 	u64 (*get_required_mask)(struct device *dev);
 #endif
@@ -602,6 +621,38 @@ static inline int dma_set_mask(struct device *dev, u64 mask)
 }
 #endif
 
+#ifdef CONFIG_HAS_DMA
+#include <asm/dma-mapping.h>
+#else
+#include <asm-generic/dma-mapping-broken.h>
+#endif
+#ifndef CONFIG_NO_DMA
+static inline void *dma_remap(struct device *dev, void *cpu_addr,
+		dma_addr_t dma_handle, size_t size, unsigned long attrs)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	if (!ops->remap) {
+		WARN_ONCE(1, "Remap function not implemented for %pS\n",
+				ops->remap);
+		return NULL;
+	}
+	return ops->remap(dev, cpu_addr, dma_handle, size, attrs);
+}
+static inline void dma_unremap(struct device *dev, void *remapped_addr,
+				size_t size)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	if (!ops->unremap) {
+		WARN_ONCE(1, "unremap function not implemented for %pS\n",
+				ops->unremap);
+		return;
+	}
+	return ops->unremap(dev, remapped_addr, size);
+}
+#endif
+
 static inline u64 dma_get_mask(struct device *dev)
 {
 	if (dev && dev->dma_mask && *dev->dma_mask)
diff --git a/include/linux/highmem.h b/include/linux/highmem.h
index 0690679832d4..d651b8a3209e 100644
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@ -40,6 +40,12 @@ extern unsigned long totalhigh_pages;
 
 void kmap_flush_unused(void);
 
+#ifdef CONFIG_ARCH_WANT_KMAP_ATOMIC_FLUSH
+void kmap_atomic_flush_unused(void);
+#else
+static inline void kmap_atomic_flush_unused(void) { }
+#endif
+
 struct page *kmap_to_page(void *addr);
 
 #else /* CONFIG_HIGHMEM */
@@ -81,6 +87,7 @@ static inline void __kunmap_atomic(void *addr)
 #define kmap_atomic_pfn(pfn)	kmap_atomic(pfn_to_page(pfn))
 
 #define kmap_flush_unused()	do {} while(0)
+#define kmap_atomic_flush_unused()	do {} while (0)
 #endif
 
 #endif /* CONFIG_HIGHMEM */
diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index eeceac3376fc..0d3dbefc9881 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -717,6 +717,7 @@ int arch_show_interrupts(struct seq_file *p, int prec);
 extern int early_irq_init(void);
 extern int arch_probe_nr_irqs(void);
 extern int arch_early_irq_init(void);
+extern void irq_set_pending(unsigned int irq);
 
 /*
  * We want to know which function is an entrypoint of a hardirq or a softirq.
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 201de12a9957..8b08f31d2046 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -618,6 +618,8 @@ extern int irq_chip_set_type_parent(struct irq_data *data, unsigned int type);
 /* Handling of unhandled and spurious interrupts: */
 extern void note_interrupt(struct irq_desc *desc, irqreturn_t action_ret);
 
+/* Resending of interrupts :*/
+void check_irq_resend(struct irq_desc *desc);
 
 /* Enable/disable irq debugging output: */
 extern int noirqdebug_setup(char *str);
diff --git a/include/linux/memblock.h b/include/linux/memblock.h
index ca59883c8364..9078bd6bd419 100644
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@ -336,6 +336,7 @@ void memblock_mem_limit_remove_map(phys_addr_t limit);
 bool memblock_is_memory(phys_addr_t addr);
 bool memblock_is_map_memory(phys_addr_t addr);
 bool memblock_is_region_memory(phys_addr_t base, phys_addr_t size);
+int memblock_overlaps_memory(phys_addr_t base, phys_addr_t size);
 bool memblock_is_reserved(phys_addr_t addr);
 bool memblock_is_region_reserved(phys_addr_t base, phys_addr_t size);
 
diff --git a/include/linux/regulator/consumer.h b/include/linux/regulator/consumer.h
index 25602afd4844..f4737a8945c3 100644
--- a/include/linux/regulator/consumer.h
+++ b/include/linux/regulator/consumer.h
@@ -244,6 +244,7 @@ void regulator_bulk_free(int num_consumers,
 
 int regulator_count_voltages(struct regulator *regulator);
 int regulator_list_voltage(struct regulator *regulator, unsigned selector);
+int regulator_list_corner_voltage(struct regulator *regulator, int corner);
 int regulator_is_supported_voltage(struct regulator *regulator,
 				   int min_uV, int max_uV);
 unsigned int regulator_get_linear_step(struct regulator *regulator);
diff --git a/include/linux/regulator/driver.h b/include/linux/regulator/driver.h
index fc2dc8df476f..9c9fbaee42b9 100644
--- a/include/linux/regulator/driver.h
+++ b/include/linux/regulator/driver.h
@@ -93,6 +93,10 @@ struct regulator_linear_range {
  *	if the selector indicates a voltage that is unusable on this system;
  *	or negative errno.  Selectors range from zero to one less than
  *	regulator_desc.n_voltages.  Voltages may be reported in any order.
+ * @list_corner_voltage: Return the maximum voltage in microvolts that
+ *	that can be physically configured for the regulator when operating at
+ *	the specified voltage corner or a negative errno if the corner value
+ *	can't be used on this system.
  *
  * @set_current_limit: Configure a limit for a current-limited regulator.
  *                     The driver should select the current closest to max_uA.
@@ -149,6 +153,7 @@ struct regulator_ops {
 
 	/* enumerate supported voltages */
 	int (*list_voltage) (struct regulator_dev *, unsigned selector);
+	int (*list_corner_voltage)(struct regulator_dev *, int corner);
 
 	/* get/set regulator voltage */
 	int (*set_voltage) (struct regulator_dev *, int min_uV, int max_uV,
diff --git a/include/linux/thermal.h b/include/linux/thermal.h
index 5f4705f46c2f..89fdee74a44f 100644
--- a/include/linux/thermal.h
+++ b/include/linux/thermal.h
@@ -68,6 +68,9 @@ enum thermal_trip_type {
 	THERMAL_TRIP_PASSIVE,
 	THERMAL_TRIP_HOT,
 	THERMAL_TRIP_CRITICAL,
+	THERMAL_TRIP_CONFIGURABLE_HI,
+	THERMAL_TRIP_CONFIGURABLE_LOW,
+	THERMAL_TRIP_CRITICAL_LOW,
 };
 
 enum thermal_trend {
@@ -146,6 +149,15 @@ struct thermal_attr {
 	char name[THERMAL_NAME_LENGTH];
 };
 
+struct sensor_threshold {
+	long temp;
+	enum thermal_trip_type trip;
+	int (*notify)(enum thermal_trip_type type, int temp, void *data);
+	void *data;
+	uint8_t active;
+	struct list_head list;
+};
+
 /**
  * struct thermal_zone_device - structure for a thermal zone
  * @id:		unique id number for each thermal zone
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 9a8b7ba9aa88..b558deb2b68b 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -1909,6 +1909,19 @@ int request_any_context_irq(unsigned int irq, irq_handler_t handler,
 }
 EXPORT_SYMBOL_GPL(request_any_context_irq);
 
+void irq_set_pending(unsigned int irq)
+{
+	struct irq_desc *desc = irq_to_desc(irq);
+	unsigned long flags;
+
+	if (desc) {
+		raw_spin_lock_irqsave(&desc->lock, flags);
+		desc->istate |= IRQS_PENDING;
+		raw_spin_unlock_irqrestore(&desc->lock, flags);
+	}
+}
+EXPORT_SYMBOL_GPL(irq_set_pending);
+
 void enable_percpu_irq(unsigned int irq, unsigned int type)
 {
 	unsigned int cpu = smp_processor_id();
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index 5b33e2f5c0ed..807034fe6a5a 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -28,6 +28,7 @@
 #include <linux/posix-timers.h>
 #include <linux/context_tracking.h>
 #include <linux/mm.h>
+#include <linux/rq_stats.h>
 
 #include <asm/irq_regs.h>
 
@@ -35,6 +36,10 @@
 
 #include <trace/events/timer.h>
 
+struct rq_data rq_info;
+struct workqueue_struct *rq_wq;
+spinlock_t rq_lock;
+
 /*
  * Per-CPU nohz control structure
  */
@@ -1253,6 +1258,51 @@ void tick_irq_enter(void)
  * High resolution timer specific code
  */
 #ifdef CONFIG_HIGH_RES_TIMERS
+static void update_rq_stats(void)
+{
+	unsigned long jiffy_gap = 0;
+	s64 rq_avg = 0;
+	unsigned long flags = 0;
+
+	jiffy_gap = jiffies - rq_info.rq_poll_last_jiffy;
+
+	if (jiffy_gap >= rq_info.rq_poll_jiffies) {
+
+		spin_lock_irqsave(&rq_lock, flags);
+
+		if (!rq_info.rq_avg)
+			rq_info.rq_poll_total_jiffies = 0;
+
+		rq_avg = (u64) (nr_running() * 10);
+
+		if (rq_info.rq_poll_total_jiffies) {
+			rq_avg = (u64)  ((rq_avg * jiffy_gap) +
+				(rq_info.rq_avg *
+				 rq_info.rq_poll_total_jiffies));
+			do_div(rq_avg,
+			       (unsigned long) (rq_info.rq_poll_total_jiffies + jiffy_gap));
+		}
+
+		rq_info.rq_avg =  rq_avg;
+		rq_info.rq_poll_total_jiffies += jiffy_gap;
+		rq_info.rq_poll_last_jiffy = jiffies;
+
+		spin_unlock_irqrestore(&rq_lock, flags);
+	}
+}
+
+static void wakeup_user(void)
+{
+	unsigned long jiffy_gap;
+
+	jiffy_gap = jiffies - rq_info.def_timer_last_jiffy;
+
+	if (jiffy_gap >= rq_info.def_timer_jiffies) {
+		rq_info.def_timer_last_jiffy = jiffies;
+		queue_work(rq_wq, &rq_info.def_timer_work);
+	}
+}
+
 /*
  * We rearm the timer until we get disabled by the idle code.
  * Called with interrupts disabled.
@@ -1270,9 +1320,22 @@ static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
 	 * Do not call, when we are not in irq context and have
 	 * no valid regs pointer
 	 */
-	if (regs)
+	if (regs){
 		tick_sched_handle(ts, regs);
-	else
+
+		if (rq_info.init == 1 &&
+				tick_do_timer_cpu == smp_processor_id()) {
+			/*
+			 * update run queue statistics
+			 */
+			update_rq_stats();
+
+			/*
+			 * wakeup user if needed
+			 */
+			wakeup_user();
+		}
+	}else
 		ts->next_tick = 0;
 
 	/* No need to reprogram if we are in idle or full dynticks mode */
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index dcc0166d1997..56abbb1f00ac 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -82,6 +82,23 @@ config RING_BUFFER_ALLOW_SWAP
 	 Allow the use of ring_buffer_swap_cpu.
 	 Adds a very slight overhead to tracing when enabled.
 
+config MSM_RTB
+	bool "Register tracing"
+	help
+	  Add support for logging different events to a small uncached
+	  region. This is designed to aid in debugging reset cases where the
+	  caches may not be flushed before the target resets.
+
+config MSM_RTB_SEPARATE_CPUS
+	bool "Separate entries for each cpu"
+	depends on MSM_RTB
+	depends on SMP
+	help
+	  Under some circumstances, it may be beneficial to give dedicated space
+	  for each cpu to log accesses. Selecting this option will log each cpu
+	  separately. This will guarantee that the last acesses for each cpu
+	  will be logged but there will be fewer entries per cpu
+
 # All tracer options should select GENERIC_TRACER. For those options that are
 # enabled by all tracers (context switch and event tracer) they select TRACING.
 # This allows those options to appear when no other tracer is selected. But the
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index e2538c7638d4..c9579bf68cfd 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -72,4 +72,6 @@ obj-$(CONFIG_UPROBE_EVENTS) += trace_uprobe.o
 
 obj-$(CONFIG_TRACEPOINT_BENCHMARK) += trace_benchmark.o
 
+obj-$(CONFIG_MSM_RTB) += msm_rtb.o
+
 libftrace-y := ftrace.o
diff --git a/mm/memblock.c b/mm/memblock.c
index 4b5d245fafc1..696e7449cbb5 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -1683,6 +1683,13 @@ bool __init_memblock memblock_is_region_memory(phys_addr_t base, phys_addr_t siz
 		 memblock.memory.regions[idx].size) >= end;
 }
 
+int __init_memblock memblock_overlaps_memory(phys_addr_t base, phys_addr_t size)
+{
+	memblock_cap_size(base, &size);
+
+	return memblock_overlaps_region(&memblock.memory, base, size) >= 0;
+}
+
 /**
  * memblock_is_region_reserved - check if a region intersects reserved memory
  * @base: base of region to check
